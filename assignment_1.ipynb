{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c531158",
   "metadata": {},
   "source": [
    "# IDS Assignment Part 1 - <font color=\"red\"><h7>Deadline: 18/12/2022 23:59</h7></font>\n",
    "This is the first part of the assignments in IDS 2022/2023. \n",
    "Please use this Jupyter notebook to work on the questions posed in the assignment. When you are done, upload the notebook in Moodle at the designated activity. This is the _only_ file that is required. A separate report is _not_ needed and will not be considered for grading. \n",
    "\n",
    "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
    "\n",
    "<font color=\"red\"> *Please make sure to include the names and matriculation numbers of all group members in the slot provided below.* </font> If a name or a student id is missing, the student will not receive any points.\n",
    "\n",
    "Hint 1: While working on the assignment, you will get a better understanding of the dataset. Feel free to generate additional results and visualizations to support your answers. For example, this might be useful regarding data modification, data simplification, or output interpretation. <font color=\"red\">Ensure that all your claims are supported.</font>\n",
    "\n",
    "Hint 2: <font color=\"red\">Plan your time wisely. </font> A few parts of this assignment may take some time to run. It might be necessary to consider time management when you plan your group work. Also, do not attempt to upload your assignment at the last minute before the deadline. This often does not work, and you will miss the deadline. Late submissions will not be considered.\n",
    "\n",
    "Hint 3: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e826e43",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and IDs:\n",
    "    \n",
    "    1. \n",
    "    \n",
    "    2. \n",
    "    \n",
    "    3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c05f0",
   "metadata": {},
   "source": [
    "For those using Jupyter Notebook in your local environments, please install the following library in your environment:\n",
    "```pip install umap-learn==0.5.3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ab570761-1a8a-4c62-8de2-534f88604bb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab570761-1a8a-4c62-8de2-534f88604bb8",
    "outputId": "a8278aa6-aa87-4cea-c7f7-10ab381b364f"
   },
   "outputs": [],
   "source": [
    "#your imports (only use libraries provided in the environment)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "le9qm43wNPGY",
   "metadata": {
    "id": "le9qm43wNPGY"
   },
   "source": [
    "# Pricing your AirBnB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa8631-4eba-4472-b981-510d243e1b2b",
   "metadata": {
    "id": "8dfa8631-4eba-4472-b981-510d243e1b2b"
   },
   "source": [
    "You finally made it! Working as a data scientist at an established process mining company in New York, you earned so much money that you can buy a flat for yourself. You decide to buy a 2-room apartment in Manhattan. Congratulations!\n",
    "\n",
    "Currently, you are still living in your rental apartment and are tied to a long-running rental contract. You can only use the apartment in 5 years from now. Therefore, you want to use the apartment as an AirBnB in the meantime, generating some consistent income. \n",
    "\n",
    "\n",
    "You have no clue how you should price your AirBnB per night (you are living in New York, so you never needed to book an AirBnB). You decide to leverage your unique skill set and perform a data-driven, informed decision. You collect all the data you can get your hands on by traversing all New York apartments. The retrieved data set **AirBnB_original.csv** is your starting point for investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21b607-828f-47b4-a57a-18b6ccab6322",
   "metadata": {
    "id": "7e21b607-828f-47b4-a57a-18b6ccab6322"
   },
   "source": [
    "## Question 1: Exploring the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79e889",
   "metadata": {
    "id": "3a79e889"
   },
   "source": [
    "As with every data set, you first want to know what you are dealing with. You perform some exploratory data analysis and clean the data along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8L8J85EfPRoZ",
   "metadata": {
    "id": "8L8J85EfPRoZ"
   },
   "source": [
    "Show a few data points contained in the data set. Include all attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ly3THPi7PdLJ",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9_9aEzu5PI1h",
   "metadata": {
    "id": "9_9aEzu5PI1h"
   },
   "source": [
    "### Basic Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38360b13",
   "metadata": {
    "id": "38360b13"
   },
   "source": [
    "First, you have to remove NaN entries across the data set such that we only keep rows without any NaN value. How many rows did you remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b96e7f",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee5fbc",
   "metadata": {
    "id": "1dee5fbc"
   },
   "source": [
    "Since duplicated entries are misleading, we want to remove duplicate flats (identified by their id), such that we only keep them once and create a new dataset. How many duplicates are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403479a7",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yty_b-ftfTPc",
   "metadata": {
    "id": "Yty_b-ftfTPc"
   },
   "source": [
    "From now on, you work with the dataset **AirBnB_cleaned.csv**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g-rLfHOoEGuz",
   "metadata": {
    "id": "g-rLfHOoEGuz"
   },
   "source": [
    "To get a feeling for nightly prices in New York, you take a look at the price attribute. Calculate the average price, the first, the third percentile, and the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb6f5a",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqDy9wQIElfF",
   "metadata": {
    "id": "eqDy9wQIElfF"
   },
   "source": [
    "You want to get a more visual picture of the price range. Use a histogram and boxplot to show the distribution of prices on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0714e",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3RkQxNBpQ7mC",
   "metadata": {
    "id": "3RkQxNBpQ7mC"
   },
   "source": [
    "### Exploring Price Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lKi9SkO1RF56",
   "metadata": {
    "id": "lKi9SkO1RF56"
   },
   "source": [
    "To inform our pricing decision, we want to investigate some differentiating factors for pricing apartments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nClcY_FV_AKv",
   "metadata": {
    "id": "nClcY_FV_AKv"
   },
   "source": [
    "Since you have an overview of the prices and their distribution, you want to investigate which features of your dataset you might use to make your pricing decision. To do so, discuss each feature of the dataset and already rule out features that will definitely not impact the pricing decision. Below you can find a brief description of each feature: \n",
    "\n",
    "- id: Unique identifier for an apartment\n",
    "- host id: Identifier of the host offering a certain apartment\n",
    "- neighborhood group: Name of a neighborhood group in which an apartment is located\n",
    "- neighborhood: More fine-grained view on neighborhoods than neighborhood group\n",
    "- lat: Latitude coordinate of an apartment\n",
    "- long: Longitude coordinate of an apartment\n",
    "- country: The country in which an apartment is located\n",
    "- country code: Code of the country in which an apartment is located\n",
    "- instant bookable: Boolean values if an apartment is instantly bookable\n",
    "- cancellation policy: Policy for cancellation of booking\n",
    "- room type: Type of apartment/room\n",
    "- construction year: Year of construction of apartment\n",
    "- price: Price for renting an apartment\n",
    "- service fee: Fee of service\n",
    "- minimum nights: Number of minimum nights to rent an apartment\n",
    "- number of reviews: Number of reviews for an apartment\n",
    "- review rate number:  Review rate for an apartment (1 to 5)\n",
    "- calculated host listings count: Number of listings a host has\n",
    "- number rooms: Number of rooms in an apartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146a915",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac44c1",
   "metadata": {},
   "source": [
    "_Answer:_ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QlBJgLSjh2l5",
   "metadata": {
    "id": "QlBJgLSjh2l5"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X0phX-tc_txX",
   "metadata": {
    "id": "X0phX-tc_txX"
   },
   "source": [
    "In the following, you want to analyze which features influence the price. You do this univariately, i.e., look at the relationship between one feature and the price individually. No matter which features you concluded to be useful in the last task, **you will consider the following seven features: service fee, minimum nights, neighborhood group, room type, cancellation policy, construction year, and the number of rooms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ymD1OuKhE58B",
   "metadata": {
    "id": "ymD1OuKhE58B"
   },
   "source": [
    "First, we look closely at the relationship between price and service fee. To do so, create a joint plot for these two attributes. What is your finding? Is this a helpful feature for our pricing decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c1da7",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7262980",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2rcPfXMcGk1M",
   "metadata": {
    "id": "2rcPfXMcGk1M"
   },
   "source": [
    "Next, we are interested in the relationship between the number of minimum nights and the price. To analyze it, create a joint plot with a regression. Describe your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028e83e",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd05bf",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6MIVIHji_tG1",
   "metadata": {
    "id": "6MIVIHji_tG1"
   },
   "source": [
    "To further investigate the pricing decision, you look at the remaining five features: neighborhood group, room type, cancellation policy, construction year, and the number of rooms.\n",
    "To get an overview, you decide to create box plots (including the mean) for each of these features to visualize their influence on the price. Describe your findings. Which values of the features influence the price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f080d",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16206993",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Bs51U6ch9Ka",
   "metadata": {
    "id": "1Bs51U6ch9Ka"
   },
   "source": [
    "### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LZJDkd6xiJuy",
   "metadata": {
    "id": "LZJDkd6xiJuy"
   },
   "source": [
    "As the univariate analysis of price relationships provided only limited insights, you decide to visualize some multivariate investigations of the price. **No matter what you concluded in the last section, you will use the features construction year, number of rooms, cancellation policy, room type, and neighborhood group in the multivariate analysis.**\n",
    "For each pair of features, create a grouped boxplot (using the hue argument) and investigate whether the two features together allow for an explanation of price differences. In the end, conclude which combination of features should be used to make your pricing decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af70ad",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis for Neighborhood Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4a749",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b80c4f",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbd25d",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis for Room Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809668e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72041142",
   "metadata": {},
   "source": [
    "_Answer:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11db5c3",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis for Number of Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95f5b1",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419042f2",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis for Construction Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43469f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ae423",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2kZI5CkIbahY",
   "metadata": {
    "id": "2kZI5CkIbahY"
   },
   "source": [
    "### Selection of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UUkY4fg4emGQ",
   "metadata": {
    "id": "UUkY4fg4emGQ"
   },
   "source": [
    "Please argue: Which combinations of variables would you consider for pricing your AirBnB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5808a47-1108-43c8-b371-aeb1f893e511",
   "metadata": {},
   "source": [
    "_Answer:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5625b-7666-447e-848b-6a7d239a6d0b",
   "metadata": {
    "id": "9fb5625b-7666-447e-848b-6a7d239a6d0b",
    "tags": []
   },
   "source": [
    "## Question 2: Setting the Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UVRItjgwQSNb",
   "metadata": {
    "id": "UVRItjgwQSNb"
   },
   "source": [
    "No matter what you have deducted from your analysis, from now on, you will continue only with three features: **The number of rooms, the neighborhood group, and the room type.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7128fc0-c9cc-4ecb-bc37-147ea4bfbf07",
   "metadata": {
    "id": "c7128fc0-c9cc-4ecb-bc37-147ea4bfbf07"
   },
   "source": [
    "**Enter Toby**: Toby is your long-term friend back from studying at RWTH. While you studied computer science, Toby studied business administration to become a big shot at Wall Street. He currently works in the back office of some swiss credit institute just across the corner from your office. You decide to drop by and get some of his input for your pricing decision. \n",
    "\n",
    "Toby proposes to first use comparables pricing to price your apartment. Comparables pricing is an established concept in economics and means that you price your apartment close to similar apartments. \n",
    "You will use different techniques to find similar apartments and set a price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139f2fd-5b7e-4086-8a1f-6098df4f6d25",
   "metadata": {
    "id": "b139f2fd-5b7e-4086-8a1f-6098df4f6d25"
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tD6hGWmni8eA",
   "metadata": {
    "id": "tD6hGWmni8eA"
   },
   "source": [
    "First, you want to set a baseline to compare against. The baseline should always use the average price of all apartments as a recommended price. Evaluate on a test set of 5%. The same test set should also be used later (you can use the random_state argument in scikit learn's train_test_split, but you need to stay consistent for different questions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dd20f",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1e5ce",
   "metadata": {
    "id": "ddc1e5ce"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8y3FUe5paWrY",
   "metadata": {
    "id": "8y3FUe5paWrY"
   },
   "source": [
    "Using a decision tree, you try to price your apartment according to similar apartments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06ecc8",
   "metadata": {
    "id": "0a06ecc8"
   },
   "source": [
    "In the following, we want to know the price ranges for renting AirBnBs. To use decision trees, you need to discretize prices. We use the following price bins:\n",
    "\n",
    "\n",
    "- [50, 200]\n",
    "- (200, 400]\n",
    "- (400, 600]\n",
    "- (600, 800]\n",
    "- (800, 1000]\n",
    "- (1000, 1200]\n",
    "\n",
    "We add the attribute \"Price_Bins\" using the discretization mentioned above for each apartment. For example, if an apartment costs 404, it is put into the bin (400, 600]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ec0f0",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15a3f0",
   "metadata": {
    "id": "9a15a3f0"
   },
   "source": [
    "Consider the extended dataset from the previous task. Use 'number rooms,' 'room type,' and 'neighborhood group' as descriptive features and \"Price_Bins\" as the target feature. Create a test and training set like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc234682",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WJuVU61pfuAU",
   "metadata": {
    "id": "WJuVU61pfuAU"
   },
   "source": [
    "Generate a decision tree in which the minimum number of samples for splitting is ten and use entropy. What is the system's entropy?\n",
    "\n",
    "Note: For this task, you must use the p_decision_tree library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a779d60",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5527b",
   "metadata": {
    "id": "ddc5527b"
   },
   "source": [
    "What is the first feature the decision tree splits on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb90c3",
   "metadata": {
    "id": "dedb90c3"
   },
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f3229",
   "metadata": {
    "id": "604f3229"
   },
   "source": [
    "Since it doesn't make sense to set a price range for renting a room, we need to fix a price. For simplicity, we use the means of the bins, i.e., 125, 300, 500, 700, 900, and 1100. For example, if a room were categorized in the bin (200, 400], it would have the price of 300.\n",
    "\n",
    "What prices are possible given your decision tree for:\n",
    "\n",
    "- Manhattan, 1-room, Private room\n",
    "- Staten Island, 1-room, Shared room"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc9049",
   "metadata": {
    "id": "dedb90c3"
   },
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gXRYGEa5QwCC",
   "metadata": {
    "id": "gXRYGEa5QwCC"
   },
   "source": [
    "What is the proposed price of the decision tree for our apartment? Store it in a variable for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a17248",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b71ded",
   "metadata": {
    "id": "d7b71ded"
   },
   "source": [
    "To evaluate how good our decision tree is for determining price categories, we want to measure the error of the predictions on the test set. To do so, the IDS team provided you helper functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ywjosLTaa8sY",
   "metadata": {
    "id": "ywjosLTaa8sY"
   },
   "outputs": [],
   "source": [
    "def get_name_value_dict(graph):\n",
    "    id_label_dict = {}\n",
    "    root = decisionTree.root\n",
    "    visited = set() # Set to keep track of visited nodes of graph.\n",
    "\n",
    "    def dfs(visited, graph, node):#function for dfs \n",
    "        id_label_dict[node.name] = node.value\n",
    "        if node not in visited:\n",
    "            if node.next:\n",
    "                id_label_dict[node.next.name] = node.next.value\n",
    "            visited.add(node)\n",
    "            if node.childs:\n",
    "                for child in node.childs:\n",
    "                    if child is not None:\n",
    "                        dfs(visited, graph, child)\n",
    "            elif node.next:\n",
    "                if node.next.childs:\n",
    "                    for child in node.next.childs:\n",
    "                        if child is not None:\n",
    "                            dfs(visited, graph, child)\n",
    "    dfs(visited, decisionTree, root)\n",
    "    return id_label_dict\n",
    "\n",
    "def get_predictor(tree):\n",
    "  id_label_dict = get_name_value_dict(tree)\n",
    "  dotplus = pydotplus.graph_from_dot_data(tree.source)\n",
    "  G = networkx.nx_pydot.from_pydot(dotplus)\n",
    "  root = [n for n,d in G.in_degree() if d==0][0]\n",
    "  paths = []\n",
    "  for node in G:\n",
    "      if G.out_degree(node)==0: #it's a leaf\n",
    "          paths.append(networkx.shortest_path(G, root, node))\n",
    "  new_paths = []\n",
    "  for path in paths:\n",
    "      for name, value in id_label_dict.items():\n",
    "          path = ([p.replace(name, value) for p in path])\n",
    "      new_paths.append(path)\n",
    "  predictor = {}\n",
    "  for path in new_paths.copy():\n",
    "    target = path[-1]\n",
    "    classification = tuple(path[:-1])\n",
    "    predictor[classification] = target\n",
    "  return predictor\n",
    "\n",
    "def get_prediction_features(predictor):\n",
    "    features = set()\n",
    "    for key in predictor:\n",
    "        i = 0\n",
    "        while i <= len(key)-2:\n",
    "            features.add(key[i])\n",
    "            i +=2\n",
    "    return features\n",
    "            \n",
    "def get_predicted_bin(instance, features, predictor):\n",
    "  feature_values = {}\n",
    "  for feature in features:\n",
    "      feature_values[feature] = instance[feature]\n",
    "  possible_paths = predictor.keys()\n",
    "  i=0\n",
    "  while i<=len(feature_values):\n",
    "    attribute = [path[i] for path in possible_paths][0]\n",
    "    if attribute in predictor.values():\n",
    "      break\n",
    "    feature_value = feature_values[attribute]\n",
    "    candidates = []\n",
    "    for el in possible_paths:\n",
    "      if str(el[i+1]) == str(feature_value):\n",
    "        candidates.append(el)\n",
    "    possible_paths = candidates.copy()\n",
    "    i+=2\n",
    "  return predictor[possible_paths[0]]\n",
    "\n",
    "\n",
    "\n",
    "### Dot is your visual decision tree. Adjust the naming if needed\n",
    "predictor = get_predictor(dot)\n",
    "features = get_prediction_features(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8fad0",
   "metadata": {},
   "source": [
    "To predict the price bin for each instance of your test set, apply the function get_predicted_bin using the variables *predictor* and *features* provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tErBbNh62NdR",
   "metadata": {
    "id": "tErBbNh62NdR"
   },
   "source": [
    "You predicted a price range for AirBnBs in your test set. However, since you cannot enter a price range for renting an apartment, we have to map the range of bins to one value. To do so, we use the mean of bins, as explained before, to predict the price. Given that, compute the mean absolute error between the prediction from the decision tree and the real price provided in the dataset. What is the mean absolute error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a988f",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d4040-8285-41d8-9da7-547127f999ce",
   "metadata": {
    "id": "4e0d4040-8285-41d8-9da7-547127f999ce"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae6185",
   "metadata": {
    "id": "8bae6185"
   },
   "source": [
    "Another way to do comparables pricing is to find similar apartments using clustering. Each calculated cluster represents one or more similar apartments in the dataset. For each cluster, you calculate the mean price as a representative price. To predict the price for new apartments, you assign them to an existing cluster and give the mean price as prediction.\n",
    "\n",
    "Use the same features you used in the decision tree for the clustering as well (**number rooms, neighborhood group, room type**), and remember that your dataset is still split into a training and test set.\n",
    "\n",
    "In this task, you will use the k-means algorithm.\n",
    "*Note: To keep results comparable, please use 100 for random state and n_init when initializing the clustering algorithm. Also, remember to use the same test and training set as in the previous tasks to compare performances.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bHmkuRpS-pjY",
   "metadata": {
    "id": "bHmkuRpS-pjY"
   },
   "source": [
    "### Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TjuIWkKM1ylm",
   "metadata": {
    "id": "TjuIWkKM1ylm"
   },
   "source": [
    "K-means tries to calculate the distance between different data points. Since some of the features you want to use are not numeric, apply one-hot encoding to be able to use distance measures. Use the preprocessing module of sklearn to prepare your dataset for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6285240",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LYtN1tb8-0Yf",
   "metadata": {
    "id": "LYtN1tb8-0Yf"
   },
   "source": [
    "### Choosing number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f00cf",
   "metadata": {
    "id": "a58f00cf"
   },
   "source": [
    "In k-means, the number of wanted clusters is a parameter one can decide on. Since this is information you do not have at this time, you choose to use the data to decide.\n",
    "\n",
    "You cluster on the training dataset by using `k` number of clusters where `k` is between 4 and 15. Since you are interested in the price, for each clustering, you calculate the **standard deviation of the mean prices for the clusters**. Therefore, for each clustering:\n",
    "1. Calculate the mean price for each of the resulting clusters.\n",
    "2. Calculate the standard deviation of the mean prices calculated in 1.\n",
    "\n",
    "Plot a diagram where on the `x-axis` you will have the *number of clusters used* and on the `y-axis` the *standard deviation of the means*.\n",
    "\n",
    "In your opinion, which amount of clusters makes more sense? Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32d53e",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a62a5b",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047129d",
   "metadata": {
    "id": "8047129d"
   },
   "source": [
    "**Note:** No matter the conclusion made in the previous task, from now on, you use the clustering model with **9 clusters** for predicting the price for your apartment and evaluating the clustering technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39a5ea",
   "metadata": {},
   "source": [
    "### Pricing your apartment\n",
    "\n",
    "To predict the price for your apartment, calculate in which cluster your apartment would be and use the mean value of the cluster as a possible price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a772aa",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n",
    "# cluster for 9 clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_MyMuKLPVihY",
   "metadata": {
    "id": "_MyMuKLPVihY"
   },
   "source": [
    "### Evaluating clustering\n",
    "\n",
    "You want to evaluate how well your clustering worked. In your case, two things can be evaluated:\n",
    "\n",
    "1.   Whether the clustering technique can group similar apartments together\n",
    "2.   Whether other people determine the price of their apartment as you did, meaning whether similar apartments get a similar price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3VJSoR9VseI",
   "metadata": {
    "id": "O3VJSoR9VseI"
   },
   "source": [
    "You are going to evaluate the clustering technique visually. The IDS team provides you with the following fancy code that reduces your input to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IaMFg-ogVvpZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaMFg-ogVvpZ",
    "outputId": "36e6a0f2-75fb-4331-8337-c3c9fa893e02"
   },
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "#Preprocessing numerical\n",
    "full_data = x_train # use the train dataset here\n",
    "numerical = full_data.select_dtypes(exclude='object')\n",
    "\n",
    "for c in numerical.columns:\n",
    "    pt = sklearn.preprocessing.PowerTransformer()\n",
    "    numerical.loc[:, c] = pt.fit_transform(np.array(numerical[c]).reshape(-1, 1))\n",
    "    \n",
    "##preprocessing categorical\n",
    "categorical = full_data.select_dtypes(include='object')\n",
    "categorical = pd.get_dummies(categorical)\n",
    "\n",
    "#Percentage of columns which are categorical is used as weight parameter in embeddings later\n",
    "categorical_weight = len(full_data.select_dtypes(include='object').columns) / full_data.shape[1]\n",
    "\n",
    "#Embedding numerical & categorical\n",
    "fit1 = umap.UMAP(metric='l2').fit(numerical)\n",
    "fit2 = umap.UMAP(metric='dice').fit(categorical)\n",
    "\n",
    "#Augmenting the numerical embedding with categorical\n",
    "intersection = umap.general_simplicial_set_intersection(fit1.graph_, fit2.graph_, weight=categorical_weight)\n",
    "intersection = umap.reset_local_connectivity(intersection)\n",
    "embedding = umap.simplicial_set_embedding(fit1._raw_data, intersection, fit1.n_components,\n",
    "                                                fit1._initial_alpha, fit1._a, fit1._b,\n",
    "                                                fit1.repulsion_strength, fit1.negative_sample_rate,\n",
    "                                                200, 'random', np.random, fit1.metric,\n",
    "                                                fit1._metric_kwds, False, {}, False)\n",
    "\n",
    "# Extracting the resulting dimensions\n",
    "dimension1 = embedding[0].T[0]\n",
    "dimension2 = embedding[0].T[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea842d4",
   "metadata": {},
   "source": [
    "Create a scatter plot from the two dimensions and use the cluster labels for coloring the data points. Use the train set for the visualization. Discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319f3d2",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c822564",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EFNtFLuyV7yQ",
   "metadata": {
    "id": "EFNtFLuyV7yQ"
   },
   "source": [
    "We next evaluate whether similar apartments (considering the attributes we chose) get similar prices. To do this, we use our test set. Assign each apartment of the test set to the nearest centroid. Predict the price of the apartments in the test set by taking the mean price of the cluster they was assigned to. Calculate the mean absolute error between the predicted and real prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ae627",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qxrtlduTBkRQ",
   "metadata": {
    "id": "qxrtlduTBkRQ"
   },
   "source": [
    "### Final comments on the price prediction\n",
    "\n",
    "Please compare the three approaches used for determining the price. Do they provide a good way of determining the price?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dedc80",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6797ffe-8983-42d2-891d-7df7d8f9fc85",
   "metadata": {
    "id": "a6797ffe-8983-42d2-891d-7df7d8f9fc85"
   },
   "source": [
    "## Question 3: Investigating Price Elasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca941f-cdf0-45fa-b930-b55a21c71e86",
   "metadata": {
    "id": "88ca941f-cdf0-45fa-b930-b55a21c71e86"
   },
   "source": [
    "You show your results to Toby, excited about the price suggestions you got from the decision tree and clustering. You decide to take the higher price, which naturally means more income.\n",
    "\n",
    "However, he points you to one central flaw in your thinking: The number of bookings is not independent of the price. Therefore, you can not just take any price and assume that this will lead to more income than a lower price, as the demand (utilization) for your apartment might decrease. This is called **price elasticity**.\n",
    "\n",
    "You want to verify what Toby has been telling you. You extracted the utilization column of your dataset by grabbing the available data from AirBnB's calendar (smart!). The utilization values are contained in dataset **AirBnB_utilization.csv**. **Show several graphs investigating price elasticity.** For example, you might plot the utilization in relation to the price and color according to different variable values. \n",
    "After you have verified Toby's statements, you want to learn price elasticity from the dataset (the utilization of an apartment given the price, the number of rooms, the room type, and the neighborhood group, i.e., the same features as before). With the learned price elasticity, you want to evaluate which of the proposed prices (decision tree or clustering) is the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e26b23",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014ad7a-a391-4485-9395-b69c3f081ee8",
   "metadata": {
    "id": "3014ad7a-a391-4485-9395-b69c3f081ee8"
   },
   "source": [
    "You will investigate the relationship between price and utilization in this subtask. First, you will train different models and choose the one that best predicts utilization for an apartment. Second, you will evaluate which of the proposed prices of decision tree and clustering prices were better, assuming that your model provides the correct utilization. Third, you will conclude your investigation and provide a pricing decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64c4de-fa5c-41fe-8bbe-3bb3c9008534",
   "metadata": {
    "id": "dc64c4de-fa5c-41fe-8bbe-3bb3c9008534"
   },
   "source": [
    "### Learning price elasticity\n",
    "You want to use different supervised learning techniques to learn price elasticity. You decide on linear regression, SVM, and multilayer perceptrons. However, you have to preprocess the data. Use the features of the neighborhood group, the room type, the number of rooms, and the price to predict the utilization. Split into training and testing data (test size 5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UjOJNxt_UrQr",
   "metadata": {
    "id": "UjOJNxt_UrQr"
   },
   "source": [
    "Prepare the data set by splitting, one-hot encoding, and normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8be5553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading utilization dataset\n",
    "\n",
    "utilization = pd.read_csv('AirBnB_utilization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cd6a1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data, testing_data = train_test_split(utilization, test_size=0.05, random_state=414463)\n",
    "\n",
    "#descriptive feature of training data\n",
    "df_training_data=training_data[['number rooms','room type','neighborhood group','price']]\n",
    "#target feature of training data\n",
    "tf_training_data=training_data[['utilization']]\n",
    "\n",
    "#descriptive feature of test data\n",
    "df_testing_data=testing_data[['number rooms','room type','neighborhood group','price']]\n",
    "#target feature of test data\n",
    "tf_testing_data=testing_data[['utilization']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4586aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "for_OHE_utilization=utilization[['number rooms','room type','neighborhood group','price']]\n",
    "\n",
    "#apply the one hot encoder\n",
    "\n",
    "encoder=make_column_transformer(\n",
    "    (OneHotEncoder(), ['room type', 'neighborhood group']),\n",
    "    remainder='passthrough')\n",
    "encoder.fit(for_OHE_utilization)\n",
    "\n",
    "#transform and convert lists to pandas dataframe\n",
    "training_data_OneHot = pd.DataFrame(encoder.transform(df_training_data).tolist())\n",
    "training_data_OneHot.columns = encoder.get_feature_names_out(input_features=df_training_data.columns)\n",
    "\n",
    "#transform and convert lists to pandas dataframe\n",
    "testing_data_OneHot = pd.DataFrame(encoder.transform(df_testing_data).tolist())\n",
    "testing_data_OneHot.columns = encoder.get_feature_names_out(input_features=df_testing_data.columns)\n",
    "\n",
    "\n",
    "# encode labels with value between 0 and n_classes-1\n",
    "#encoder = LabelEncoder()\n",
    "#df_training_data_LE = training_data_OneHot.apply(encoder.fit_transform)\n",
    "#df_testing_data_LE = testing_data_OneHot.apply(encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d750df9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset before OHE\n",
      "(67299, 4)\n",
      "Shape of training dataset after OHE\n",
      "(67299, 11)\n",
      "-------------------------------------------\n",
      "Shape of testing dataset before OHE\n",
      "(3543, 4)\n",
      "Shape of testing dataset after OHE\n",
      "(3543, 11)\n"
     ]
    }
   ],
   "source": [
    "#remaining \"number rooms\",\"id\" columns and type to int. Also setting index\n",
    "\n",
    "training_data_OneHot.rename({ 'remainder__number rooms': 'number rooms','remainder__price': 'price'}, axis=1, inplace=True)\n",
    "testing_data_OneHot.rename({'remainder__number rooms': 'number rooms','remainder__price': 'price'}, axis=1, inplace=True)\n",
    "\n",
    "training_data_OneHot[[\"number rooms\"]]=training_data_OneHot[[\"number rooms\"]].astype(int)\n",
    "testing_data_OneHot[[\"number rooms\"]]=testing_data_OneHot[[\"number rooms\"]].astype(int)\n",
    "\n",
    "#training_data_OneHot.set_index('id')\n",
    "#testing_data_OneHot.set_index('id')\n",
    "\n",
    "print(\"Shape of training dataset before OHE\")\n",
    "print(df_training_data.shape)\n",
    "print(\"Shape of training dataset after OHE\")\n",
    "print(training_data_OneHot.shape)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Shape of testing dataset before OHE\")\n",
    "print(df_testing_data.shape)\n",
    "print(\"Shape of testing dataset after OHE\")\n",
    "print(testing_data_OneHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15c80466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehotencoder__room type_Entire home/apt</th>\n",
       "      <th>onehotencoder__room type_Hotel room</th>\n",
       "      <th>onehotencoder__room type_Private room</th>\n",
       "      <th>onehotencoder__room type_Shared room</th>\n",
       "      <th>onehotencoder__neighborhood group_Bronx</th>\n",
       "      <th>onehotencoder__neighborhood group_Brooklyn</th>\n",
       "      <th>onehotencoder__neighborhood group_Manhattan</th>\n",
       "      <th>onehotencoder__neighborhood group_Queens</th>\n",
       "      <th>onehotencoder__neighborhood group_Staten Island</th>\n",
       "      <th>number rooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.846087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.788696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.752174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.239130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67295</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.061739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67296</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.466087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67298</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.924348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67299 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       onehotencoder__room type_Entire home/apt  \\\n",
       "0                                           0.0   \n",
       "1                                           1.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           1.0   \n",
       "...                                         ...   \n",
       "67294                                       0.0   \n",
       "67295                                       1.0   \n",
       "67296                                       1.0   \n",
       "67297                                       1.0   \n",
       "67298                                       1.0   \n",
       "\n",
       "       onehotencoder__room type_Hotel room  \\\n",
       "0                                      0.0   \n",
       "1                                      0.0   \n",
       "2                                      0.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "...                                    ...   \n",
       "67294                                  0.0   \n",
       "67295                                  0.0   \n",
       "67296                                  0.0   \n",
       "67297                                  0.0   \n",
       "67298                                  0.0   \n",
       "\n",
       "       onehotencoder__room type_Private room  \\\n",
       "0                                        1.0   \n",
       "1                                        0.0   \n",
       "2                                        1.0   \n",
       "3                                        1.0   \n",
       "4                                        0.0   \n",
       "...                                      ...   \n",
       "67294                                    1.0   \n",
       "67295                                    0.0   \n",
       "67296                                    0.0   \n",
       "67297                                    0.0   \n",
       "67298                                    0.0   \n",
       "\n",
       "       onehotencoder__room type_Shared room  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "...                                     ...   \n",
       "67294                                   0.0   \n",
       "67295                                   0.0   \n",
       "67296                                   0.0   \n",
       "67297                                   0.0   \n",
       "67298                                   0.0   \n",
       "\n",
       "       onehotencoder__neighborhood group_Bronx  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "...                                        ...   \n",
       "67294                                      0.0   \n",
       "67295                                      0.0   \n",
       "67296                                      0.0   \n",
       "67297                                      0.0   \n",
       "67298                                      0.0   \n",
       "\n",
       "       onehotencoder__neighborhood group_Brooklyn  \\\n",
       "0                                             0.0   \n",
       "1                                             1.0   \n",
       "2                                             1.0   \n",
       "3                                             1.0   \n",
       "4                                             0.0   \n",
       "...                                           ...   \n",
       "67294                                         1.0   \n",
       "67295                                         0.0   \n",
       "67296                                         0.0   \n",
       "67297                                         0.0   \n",
       "67298                                         0.0   \n",
       "\n",
       "       onehotencoder__neighborhood group_Manhattan  \\\n",
       "0                                              1.0   \n",
       "1                                              0.0   \n",
       "2                                              0.0   \n",
       "3                                              0.0   \n",
       "4                                              0.0   \n",
       "...                                            ...   \n",
       "67294                                          0.0   \n",
       "67295                                          1.0   \n",
       "67296                                          1.0   \n",
       "67297                                          1.0   \n",
       "67298                                          1.0   \n",
       "\n",
       "       onehotencoder__neighborhood group_Queens  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           1.0   \n",
       "...                                         ...   \n",
       "67294                                       0.0   \n",
       "67295                                       0.0   \n",
       "67296                                       0.0   \n",
       "67297                                       0.0   \n",
       "67298                                       0.0   \n",
       "\n",
       "       onehotencoder__neighborhood group_Staten Island  number rooms     price  \n",
       "0                                                  0.0             1  0.544348  \n",
       "1                                                  0.0             3  0.846087  \n",
       "2                                                  0.0             1  0.788696  \n",
       "3                                                  0.0             1  0.873913  \n",
       "4                                                  0.0             2  0.752174  \n",
       "...                                                ...           ...       ...  \n",
       "67294                                              0.0             1  0.239130  \n",
       "67295                                              0.0             1  0.061739  \n",
       "67296                                              0.0             1  0.336522  \n",
       "67297                                              0.0             2  0.466087  \n",
       "67298                                              0.0             2  0.924348  \n",
       "\n",
       "[67299 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# copy the data\n",
    "df_training_scaled = training_data_OneHot.copy()\n",
    "  \n",
    "# apply normalization techniques by Column 1\n",
    "column = 'price'\n",
    "df_training_scaled[column] = (df_training_scaled[column] - df_training_scaled[column].min()) / (df_training_scaled[column].max() - df_training_scaled[column].min())    \n",
    "  \n",
    "# view normalized data\n",
    "display(df_training_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "677af2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehotencoder__room type_Entire home/apt</th>\n",
       "      <th>onehotencoder__room type_Hotel room</th>\n",
       "      <th>onehotencoder__room type_Private room</th>\n",
       "      <th>onehotencoder__room type_Shared room</th>\n",
       "      <th>onehotencoder__neighborhood group_Bronx</th>\n",
       "      <th>onehotencoder__neighborhood group_Brooklyn</th>\n",
       "      <th>onehotencoder__neighborhood group_Manhattan</th>\n",
       "      <th>onehotencoder__neighborhood group_Queens</th>\n",
       "      <th>onehotencoder__neighborhood group_Staten Island</th>\n",
       "      <th>number rooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.686087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.502609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.145217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.714783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3538</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.355652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3539</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.673913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3541</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.213913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3542</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.606087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3543 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      onehotencoder__room type_Entire home/apt  \\\n",
       "0                                          1.0   \n",
       "1                                          1.0   \n",
       "2                                          0.0   \n",
       "3                                          1.0   \n",
       "4                                          1.0   \n",
       "...                                        ...   \n",
       "3538                                       1.0   \n",
       "3539                                       1.0   \n",
       "3540                                       0.0   \n",
       "3541                                       0.0   \n",
       "3542                                       1.0   \n",
       "\n",
       "      onehotencoder__room type_Hotel room  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "3538                                  0.0   \n",
       "3539                                  0.0   \n",
       "3540                                  0.0   \n",
       "3541                                  0.0   \n",
       "3542                                  0.0   \n",
       "\n",
       "      onehotencoder__room type_Private room  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       1.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "...                                     ...   \n",
       "3538                                    0.0   \n",
       "3539                                    0.0   \n",
       "3540                                    1.0   \n",
       "3541                                    1.0   \n",
       "3542                                    0.0   \n",
       "\n",
       "      onehotencoder__room type_Shared room  \\\n",
       "0                                      0.0   \n",
       "1                                      0.0   \n",
       "2                                      0.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "...                                    ...   \n",
       "3538                                   0.0   \n",
       "3539                                   0.0   \n",
       "3540                                   0.0   \n",
       "3541                                   0.0   \n",
       "3542                                   0.0   \n",
       "\n",
       "      onehotencoder__neighborhood group_Bronx  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         1.0   \n",
       "...                                       ...   \n",
       "3538                                      0.0   \n",
       "3539                                      0.0   \n",
       "3540                                      0.0   \n",
       "3541                                      0.0   \n",
       "3542                                      0.0   \n",
       "\n",
       "      onehotencoder__neighborhood group_Brooklyn  \\\n",
       "0                                            1.0   \n",
       "1                                            0.0   \n",
       "2                                            1.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "...                                          ...   \n",
       "3538                                         0.0   \n",
       "3539                                         0.0   \n",
       "3540                                         1.0   \n",
       "3541                                         1.0   \n",
       "3542                                         1.0   \n",
       "\n",
       "      onehotencoder__neighborhood group_Manhattan  \\\n",
       "0                                             0.0   \n",
       "1                                             0.0   \n",
       "2                                             0.0   \n",
       "3                                             1.0   \n",
       "4                                             0.0   \n",
       "...                                           ...   \n",
       "3538                                          1.0   \n",
       "3539                                          1.0   \n",
       "3540                                          0.0   \n",
       "3541                                          0.0   \n",
       "3542                                          0.0   \n",
       "\n",
       "      onehotencoder__neighborhood group_Queens  \\\n",
       "0                                          0.0   \n",
       "1                                          1.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "...                                        ...   \n",
       "3538                                       0.0   \n",
       "3539                                       0.0   \n",
       "3540                                       0.0   \n",
       "3541                                       0.0   \n",
       "3542                                       0.0   \n",
       "\n",
       "      onehotencoder__neighborhood group_Staten Island  number rooms     price  \n",
       "0                                                 0.0             3  0.686087  \n",
       "1                                                 0.0             1  0.502609  \n",
       "2                                                 0.0             1  0.145217  \n",
       "3                                                 0.0             2  0.714783  \n",
       "4                                                 0.0             1  0.042609  \n",
       "...                                               ...           ...       ...  \n",
       "3538                                              0.0             1  0.355652  \n",
       "3539                                              0.0             1  0.188696  \n",
       "3540                                              0.0             1  0.673913  \n",
       "3541                                              0.0             1  0.213913  \n",
       "3542                                              0.0             2  0.606087  \n",
       "\n",
       "[3543 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# copy the data\n",
    "df_testing_scaled = testing_data_OneHot.copy()\n",
    "  \n",
    "# apply normalization techniques by Column 1\n",
    "column = 'price'\n",
    "df_testing_scaled[column] = (df_testing_scaled[column] - df_testing_scaled[column].min()) / (df_testing_scaled[column].max() - df_testing_scaled[column].min())    \n",
    "  \n",
    "# view normalized data\n",
    "display(df_testing_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JEKOoD10VF9y",
   "metadata": {
    "id": "JEKOoD10VF9y"
   },
   "source": [
    "For each technique, use 5-cross-fold validation and assess the performance on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24bef91-6a16-4b09-a0e7-3069a687ed07",
   "metadata": {
    "id": "c24bef91-6a16-4b09-a0e7-3069a687ed07"
   },
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jK-Rpzm2kBEu",
   "metadata": {
    "id": "jK-Rpzm2kBEu"
   },
   "source": [
    "Learn a linear regression to predict the utilization of an AirBnB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37d648e1",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END fit_intercept=True, normalize=True, positive=True;, score=0.086 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=True, normalize=True, positive=True;, score=0.083 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=True, normalize=True, positive=True;, score=0.079 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=True, normalize=True, positive=True;, score=0.073 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=True, normalize=True, positive=True;, score=0.077 total time=   0.0s\n",
      "[CV 1/5] END fit_intercept=True, normalize=True, positive=False;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=True, normalize=True, positive=False;, score=0.661 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=True, normalize=True, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=True, normalize=True, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=True, normalize=True, positive=False;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END fit_intercept=True, normalize=False, positive=True;, score=0.086 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=True, normalize=False, positive=True;, score=0.083 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=True, normalize=False, positive=True;, score=0.079 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=True, normalize=False, positive=True;, score=0.073 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=True, normalize=False, positive=True;, score=0.077 total time=   0.0s\n",
      "[CV 1/5] END fit_intercept=True, normalize=False, positive=False;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=True, normalize=False, positive=False;, score=0.661 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=True, normalize=False, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=True, normalize=False, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=True, normalize=False, positive=False;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END fit_intercept=False, normalize=True, positive=True;, score=0.086 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=False, normalize=True, positive=True;, score=0.083 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=False, normalize=True, positive=True;, score=0.079 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=False, normalize=True, positive=True;, score=0.073 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=False, normalize=True, positive=True;, score=0.077 total time=   0.0s\n",
      "[CV 1/5] END fit_intercept=False, normalize=True, positive=False;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=False, normalize=True, positive=False;, score=0.661 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=False, normalize=True, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=False, normalize=True, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=False, normalize=True, positive=False;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END fit_intercept=False, normalize=False, positive=True;, score=0.086 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=False, normalize=False, positive=True;, score=0.083 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=False, normalize=False, positive=True;, score=0.079 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=False, normalize=False, positive=True;, score=0.073 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=False, normalize=False, positive=True;, score=0.077 total time=   0.0s\n",
      "[CV 1/5] END fit_intercept=False, normalize=False, positive=False;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END fit_intercept=False, normalize=False, positive=False;, score=0.661 total time=   0.0s\n",
      "[CV 3/5] END fit_intercept=False, normalize=False, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 4/5] END fit_intercept=False, normalize=False, positive=False;, score=0.656 total time=   0.0s\n",
      "[CV 5/5] END fit_intercept=False, normalize=False, positive=False;, score=0.661 total time=   0.0s\n",
      "{'fit_intercept': True, 'normalize': False, 'positive': False}\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"fit_intercept\": [True, False],\n",
    "              'normalize':[True,False],\n",
    "              'positive':[True,False]}  \n",
    "grid = GridSearchCV(LinearRegression(), param_grid, cv = 5, verbose = 3) \n",
    "grid.fit(df_training_scaled, tf_training_data.values.ravel())\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b8e753a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_data_pred=grid.predict(df_testing_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KXaCxzVRkQT9",
   "metadata": {
    "id": "KXaCxzVRkQT9"
   },
   "source": [
    "What is the mean absolute error achieved on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f11f7bf2",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for test data:35.009816\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "MAE=mean_absolute_error(tf_testing_data,df_testing_data_pred)\n",
    "print(\"MAE for test data:%f\"%(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qod_TrT-kW8H",
   "metadata": {
    "id": "Qod_TrT-kW8H"
   },
   "source": [
    "Show (draw!) the price elasticity for your apartment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ee85945",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77f7d2-4752-49f8-b5e8-5bffda6e64d2",
   "metadata": {
    "id": "ae77f7d2-4752-49f8-b5e8-5bffda6e64d2"
   },
   "source": [
    "#### Linear SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s0NzpYCzs8y9",
   "metadata": {
    "id": "s0NzpYCzs8y9"
   },
   "source": [
    "Train and evaluate a Linear SVR (Support Vector Regression) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "385ccf8e",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END C=1.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.608 total time=   0.0s\n",
      "[CV 2/5] END C=1.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.607 total time=   0.0s\n",
      "[CV 3/5] END C=1.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.603 total time=   0.0s\n",
      "[CV 4/5] END C=1.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.604 total time=   0.0s\n",
      "[CV 5/5] END C=1.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.607 total time=   0.0s\n",
      "[CV 1/5] END C=1.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.659 total time=   1.5s\n",
      "[CV 2/5] END C=1.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.661 total time=   1.5s\n",
      "[CV 3/5] END C=1.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.656 total time=   1.5s\n",
      "[CV 4/5] END C=1.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.656 total time=   1.5s\n",
      "[CV 5/5] END C=1.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.661 total time=   1.5s\n",
      "[CV 1/5] END C=1.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.607 total time=   0.0s\n",
      "[CV 2/5] END C=1.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.605 total time=   0.0s\n",
      "[CV 3/5] END C=1.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.601 total time=   0.0s\n",
      "[CV 4/5] END C=1.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.601 total time=   0.0s\n",
      "[CV 5/5] END C=1.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.606 total time=   0.0s\n",
      "[CV 1/5] END C=1.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.659 total time=   1.2s\n",
      "[CV 2/5] END C=1.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.661 total time=   1.2s\n",
      "[CV 3/5] END C=1.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.656 total time=   1.2s\n",
      "[CV 4/5] END C=1.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.656 total time=   1.2s\n",
      "[CV 5/5] END C=1.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.661 total time=   1.2s\n",
      "[CV 1/5] END C=2.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.612 total time=   0.0s\n",
      "[CV 2/5] END C=2.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.610 total time=   0.0s\n",
      "[CV 3/5] END C=2.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.605 total time=   0.0s\n",
      "[CV 4/5] END C=2.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.606 total time=   0.0s\n",
      "[CV 5/5] END C=2.0, fit_intercept=True, loss=epsilon_insensitive;, score=0.609 total time=   0.0s\n",
      "[CV 1/5] END C=2.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.659 total time=   2.9s\n",
      "[CV 2/5] END C=2.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.661 total time=   3.0s\n",
      "[CV 3/5] END C=2.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.656 total time=   3.1s\n",
      "[CV 4/5] END C=2.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.656 total time=   2.9s\n",
      "[CV 5/5] END C=2.0, fit_intercept=True, loss=squared_epsilon_insensitive;, score=0.661 total time=   3.5s\n",
      "[CV 1/5] END C=2.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.611 total time=   0.0s\n",
      "[CV 2/5] END C=2.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.608 total time=   0.0s\n",
      "[CV 3/5] END C=2.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.605 total time=   0.0s\n",
      "[CV 4/5] END C=2.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.605 total time=   0.0s\n",
      "[CV 5/5] END C=2.0, fit_intercept=False, loss=epsilon_insensitive;, score=0.609 total time=   0.0s\n",
      "[CV 1/5] END C=2.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.659 total time=   2.5s\n",
      "[CV 2/5] END C=2.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.661 total time=   2.3s\n",
      "[CV 3/5] END C=2.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.656 total time=   2.5s\n",
      "[CV 4/5] END C=2.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.656 total time=   2.5s\n",
      "[CV 5/5] END C=2.0, fit_intercept=False, loss=squared_epsilon_insensitive;, score=0.661 total time=   2.3s\n",
      "{'C': 1.0, 'fit_intercept': True, 'loss': 'squared_epsilon_insensitive'}\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "from sklearn.svm import LinearSVR\n",
    "param_grid = {\"fit_intercept\": [True, False],\n",
    "              'loss':['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "             'C':[1.0,2.0]}  \n",
    "grid = GridSearchCV(LinearSVR(), param_grid, cv = 5, verbose = 3) \n",
    "grid.fit(df_training_scaled, tf_training_data)\n",
    "print(grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YCccX3ZOdiiQ",
   "metadata": {
    "id": "YCccX3ZOdiiQ"
   },
   "source": [
    "What is the mean absolute error achieved on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bbfe034",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for test data:35.009422\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "df_testing_data_pred=grid.predict(df_testing_scaled)\n",
    "MAE=mean_absolute_error(tf_testing_data,df_testing_data_pred)\n",
    "print(\"MAE for test data:%f\"%(MAE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "akKwnMMgdrQX",
   "metadata": {
    "id": "akKwnMMgdrQX"
   },
   "source": [
    "Show (draw!) the price elasticity for your apartment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2acb0",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n",
    "#Without histogram, with rug plot\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=df_testing_data_LE[['price']]\n",
    "y=pd.DataFrame({\"utilization\": df_testing_data_pred})\n",
    "frames=[x,y]\n",
    "pt_data=pd.concat(frames)\n",
    "\n",
    "sns.displot(data=pt_data, x='utilization', kind='kde', rug=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78fa88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9737709-05dc-43e3-ad50-34d0bd89606c",
   "metadata": {},
   "source": [
    "#### Learning Non-Linear Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c7b78-3bf4-40cd-a8ca-1b6ca399df9f",
   "metadata": {},
   "source": [
    "You realize that learning linear relationships only provides limited performance. You want to learn non-linear relationships. First, you will learn non-linear relationships using linear regression and kernels. Second, you will use non-linear kernels in the Support Vector Regression. Third, you will use neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6422fa-115f-441e-a3bc-abdea38f4b25",
   "metadata": {},
   "source": [
    "#### Kernels in Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016dd91-0b6d-47d6-bf86-7485b3cd623c",
   "metadata": {},
   "source": [
    "Use a transformation of the price and a regression to incorporate non-linear relationships, as learned in the lecture. You can try different kernel functions, but at least you should use one quadratic kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb46b78",
   "metadata": {},
   "source": [
    "# Your answer goes here\n",
    "#from sklearn.linear_model import PR\n",
    "\n",
    "poly_grid = {'PolynomialFeatures': [2,3]#,\n",
    "             #'fit_intercept':[True,False]\n",
    "             }  \n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv = 5, verbose = 3) \n",
    "grid.fit(df_training_scaled, tf_training_data)\n",
    "print(grid.best_params_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1952d86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "x = df_training_scaled[\"price\"].values.reshape(-1, 1)\n",
    "x_test=df_testing_scaled[\"price\"].values.reshape(-1, 1)\n",
    "y = tf_training_data[\"utilization\"]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "x_ = poly.fit_transform(x)\n",
    "x_test_=poly.fit_transform(x_test)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c4070af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm.predict(x_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28b4fb-b9f2-4408-983a-1053034003bc",
   "metadata": {},
   "source": [
    "What is the mean absolute error achieved on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f0e3ca24-6c5e-40c2-ad2d-e9d09670f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for test data:34.368327\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "#df_testing_data_pred=grid.predict(df_testing_scaled)\n",
    "MAE=mean_absolute_error(tf_testing_data,y_pred)\n",
    "print(\"MAE for test data:%f\"%(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca155df-d2e2-4d0e-b8d0-db8e780828aa",
   "metadata": {},
   "source": [
    "Show (draw!) the price elasticity for your apartment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462859b-ae69-4799-bbb3-42ba65609b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad54ac-0056-4204-bae4-f4098e5f1175",
   "metadata": {
    "id": "ae77f7d2-4752-49f8-b5e8-5bffda6e64d2"
   },
   "source": [
    "#### Kernels in SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e205a3-ed09-4dfd-9d38-95cf7ef048e5",
   "metadata": {
    "id": "s0NzpYCzs8y9"
   },
   "source": [
    "Train and evaluate an SVR (Support Vector Regression) model with different, non-linear kernels. You can limit the size of your training set to 25000 samples to make the problem feasible in computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8fc03286-feeb-413e-8389-fede8fe8d11a",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END .......................kernel=poly;, score=0.537 total time=  18.2s\n",
      "[CV 2/5] END .......................kernel=poly;, score=0.527 total time=  18.6s\n",
      "[CV 3/5] END .......................kernel=poly;, score=0.539 total time=  19.7s\n",
      "[CV 4/5] END .......................kernel=poly;, score=0.527 total time=  21.0s\n",
      "[CV 5/5] END .......................kernel=poly;, score=0.531 total time=  21.0s\n",
      "[CV 1/5] END ..................kernel=sigmoid;, score=-10.262 total time=  35.9s\n",
      "[CV 2/5] END ..................kernel=sigmoid;, score=-10.574 total time=  35.7s\n",
      "[CV 3/5] END ...................kernel=sigmoid;, score=-9.650 total time=  35.7s\n",
      "[CV 4/5] END ...................kernel=sigmoid;, score=-9.608 total time=  34.2s\n",
      "[CV 5/5] END ...................kernel=sigmoid;, score=-9.992 total time=  33.8s\n",
      "{'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = {'kernel': ['poly','sigmoid']}  \n",
    "grid = GridSearchCV(SVR(), param_grid, cv = 5, verbose = 3) \n",
    "grid.fit(df_training_scaled.head(25000), tf_training_data.head(25000))\n",
    "print(grid.best_params_) \n",
    "\n",
    "#c parameter is important.use in the parameters and train. Also consider logrithmic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb862b-97c2-42fa-9f44-f58cdbb95cba",
   "metadata": {
    "id": "YCccX3ZOdiiQ"
   },
   "source": [
    "What is the mean absolute error achieved on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "05343b84-1597-41b7-b7b6-4fa7bc5e7e82",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for test data:33.845762\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "df_testing_data_pred=grid.predict(df_testing_scaled)\n",
    "MAE=mean_absolute_error(tf_testing_data,df_testing_data_pred)\n",
    "print(\"MAE for test data:%f\"%(MAE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f3d21-40ab-4aa5-9b0b-e40e7fcc00e8",
   "metadata": {
    "id": "akKwnMMgdrQX"
   },
   "source": [
    "Show (draw!) the price elasticity for your apartment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2da55-efb6-410a-8c37-56bf4abaf043",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43359b3-a2a4-4cda-90a8-0f66b04aa3fb",
   "metadata": {},
   "source": [
    "What is your conclusion on using kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01dafcb-ce55-453c-a158-a46ab8bbfd1e",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea35b3-b236-429d-81cb-17fd6877aed4",
   "metadata": {
    "id": "39ea35b3-b236-429d-81cb-17fd6877aed4"
   },
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79IHOFdbkmN-",
   "metadata": {
    "id": "79IHOFdbkmN-"
   },
   "source": [
    "Learn a neural network (multilayer perceptron) to predict the utilization of an AirBnB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d963fd7e",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 28373.35948722\n",
      "Iteration 2, loss = 8430.71563997\n",
      "Iteration 3, loss = 8430.69304280\n",
      "Iteration 4, loss = 8430.29295443\n",
      "Iteration 5, loss = 8430.02827106\n",
      "Iteration 6, loss = 8430.11654493\n",
      "Iteration 7, loss = 8429.63107795\n",
      "Iteration 8, loss = 8429.69211428\n",
      "Iteration 9, loss = 8429.46427821\n",
      "Iteration 10, loss = 8429.26137230\n",
      "Iteration 11, loss = 8429.40720172\n",
      "Iteration 12, loss = 8429.81604737\n",
      "Iteration 13, loss = 8428.74651773\n",
      "Iteration 14, loss = 8428.71850557\n",
      "Iteration 15, loss = 8428.77962953\n",
      "Iteration 16, loss = 8428.44265494\n",
      "Iteration 17, loss = 8428.84429735\n",
      "Iteration 18, loss = 8428.05475050\n",
      "Iteration 19, loss = 8428.25562500\n",
      "Iteration 20, loss = 8428.18821137\n",
      "Iteration 21, loss = 8427.79732142\n",
      "Iteration 22, loss = 8427.32878484\n",
      "Iteration 23, loss = 8427.69886561\n",
      "Iteration 24, loss = 8426.59049927\n",
      "Iteration 25, loss = 8427.46373249\n",
      "Iteration 26, loss = 8426.77597378\n",
      "Iteration 27, loss = 8427.57973120\n",
      "Iteration 28, loss = 8426.42399067\n",
      "Iteration 29, loss = 8427.03641093\n",
      "Iteration 30, loss = 8426.85845470\n",
      "Iteration 31, loss = 8426.30006223\n",
      "Iteration 32, loss = 8426.18039790\n",
      "Iteration 33, loss = 8425.63513036\n",
      "Iteration 34, loss = 8425.81775850\n",
      "Iteration 35, loss = 8425.79246507\n",
      "Iteration 36, loss = 8425.65248394\n",
      "Iteration 37, loss = 8425.54739585\n",
      "Iteration 38, loss = 8425.07368587\n",
      "Iteration 39, loss = 8425.68512680\n",
      "Iteration 40, loss = 8424.99937046\n",
      "Iteration 41, loss = 8425.14733491\n",
      "Iteration 42, loss = 8425.46012182\n",
      "Iteration 43, loss = 8424.98874240\n",
      "Iteration 44, loss = 8424.69452925\n",
      "Iteration 45, loss = 8424.44073197\n",
      "Iteration 46, loss = 8424.56211245\n",
      "Iteration 47, loss = 8424.61377210\n",
      "Iteration 48, loss = 8423.84426516\n",
      "Iteration 49, loss = 8423.62368936\n",
      "Iteration 50, loss = 8423.32830000\n",
      "Iteration 51, loss = 8423.80215665\n",
      "Iteration 52, loss = 8423.62737856\n",
      "Iteration 53, loss = 8423.61107907\n",
      "Iteration 54, loss = 8423.15531482\n",
      "Iteration 55, loss = 8423.27080227\n",
      "Iteration 56, loss = 8422.45250584\n",
      "Iteration 57, loss = 8423.05283900\n",
      "Iteration 58, loss = 8422.77818247\n",
      "Iteration 59, loss = 8423.19080981\n",
      "Iteration 60, loss = 8422.40638952\n",
      "Iteration 61, loss = 8422.34688922\n",
      "Iteration 62, loss = 8421.76686015\n",
      "Iteration 63, loss = 8421.84317510\n",
      "Iteration 64, loss = 8421.90860404\n",
      "Iteration 65, loss = 8421.55274855\n",
      "Iteration 66, loss = 8421.72571184\n",
      "Iteration 67, loss = 8421.29207413\n",
      "Iteration 68, loss = 8421.19490499\n",
      "Iteration 69, loss = 8421.08800659\n",
      "Iteration 70, loss = 8421.02237632\n",
      "Iteration 71, loss = 8420.82722009\n",
      "Iteration 72, loss = 8420.41447567\n",
      "Iteration 73, loss = 8420.66537491\n",
      "Iteration 74, loss = 8420.38916758\n",
      "Iteration 75, loss = 8420.37505492\n",
      "Iteration 76, loss = 8420.17496773\n",
      "Iteration 77, loss = 8419.79296476\n",
      "Iteration 78, loss = 8419.92993649\n",
      "Iteration 79, loss = 8419.43887457\n",
      "Iteration 80, loss = 8419.32313120\n",
      "Iteration 81, loss = 8419.75444189\n",
      "Iteration 82, loss = 8418.78995215\n",
      "Iteration 83, loss = 8419.09228642\n",
      "Iteration 84, loss = 8418.86462541\n",
      "Iteration 85, loss = 8419.36902541\n",
      "Iteration 86, loss = 8418.89178302\n",
      "Iteration 87, loss = 8418.38865067\n",
      "Iteration 88, loss = 8418.39391396\n",
      "Iteration 89, loss = 8418.08742685\n",
      "Iteration 90, loss = 8418.43012633\n",
      "Iteration 91, loss = 8418.02329404\n",
      "Iteration 92, loss = 8417.60403903\n",
      "Iteration 93, loss = 8417.93670318\n",
      "Iteration 94, loss = 8417.45836925\n",
      "Iteration 95, loss = 8417.72893163\n",
      "Iteration 96, loss = 8417.15403100\n",
      "Iteration 97, loss = 8417.37357206\n",
      "Iteration 98, loss = 8417.05614804\n",
      "Iteration 99, loss = 8416.53383125\n",
      "Iteration 100, loss = 8416.54785790\n",
      "Iteration 101, loss = 8416.68450519\n",
      "Iteration 102, loss = 8416.60907406\n",
      "Iteration 103, loss = 8416.27395998\n",
      "Iteration 104, loss = 8416.33928094\n",
      "Iteration 105, loss = 8416.52363860\n",
      "Iteration 106, loss = 8415.82476409\n",
      "Iteration 107, loss = 8415.92657087\n",
      "Iteration 108, loss = 8415.46795107\n",
      "Iteration 109, loss = 8415.35836920\n",
      "Iteration 110, loss = 8415.83852360\n",
      "Iteration 111, loss = 8415.34388397\n",
      "Iteration 112, loss = 8415.05979880\n",
      "Iteration 113, loss = 8414.72209454\n",
      "Iteration 114, loss = 8414.57683108\n",
      "Iteration 115, loss = 8414.47922738\n",
      "Iteration 116, loss = 8414.16121446\n",
      "Iteration 117, loss = 8414.33557862\n",
      "Iteration 118, loss = 8414.18362154\n",
      "Iteration 119, loss = 8414.39856036\n",
      "Iteration 120, loss = 8413.37510675\n",
      "Iteration 121, loss = 8413.98706516\n",
      "Iteration 122, loss = 8413.62600168\n",
      "Iteration 123, loss = 8413.17462063\n",
      "Iteration 124, loss = 8413.64357188\n",
      "Iteration 125, loss = 8413.12379334\n",
      "Iteration 126, loss = 8413.39800410\n",
      "Iteration 127, loss = 8412.79866291\n",
      "Iteration 128, loss = 8412.44005327\n",
      "Iteration 129, loss = 8412.76079750\n",
      "Iteration 130, loss = 8412.21244201\n",
      "Iteration 131, loss = 8412.17168117\n",
      "Iteration 132, loss = 8412.01699487\n",
      "Iteration 133, loss = 8411.65512683\n",
      "Iteration 134, loss = 8412.52143540\n",
      "Iteration 135, loss = 8411.70173340\n",
      "Iteration 136, loss = 8411.74228058\n",
      "Iteration 137, loss = 8411.51951398\n",
      "Iteration 138, loss = 8411.04962317\n",
      "Iteration 139, loss = 8411.07805725\n",
      "Iteration 140, loss = 8410.83545848\n",
      "Iteration 141, loss = 8411.08222612\n",
      "Iteration 142, loss = 8410.34621466\n",
      "Iteration 143, loss = 8410.43436621\n",
      "Iteration 144, loss = 8410.42918461\n",
      "Iteration 145, loss = 8410.43592869\n",
      "Iteration 146, loss = 8410.55721897\n",
      "Iteration 147, loss = 8409.93276787\n",
      "Iteration 148, loss = 8409.89447518\n",
      "Iteration 149, loss = 8409.81209550\n",
      "Iteration 150, loss = 8409.61189837\n",
      "Iteration 151, loss = 8409.73317981\n",
      "Iteration 152, loss = 8408.46042518\n",
      "Iteration 153, loss = 8409.10957388\n",
      "Iteration 154, loss = 8409.05372353\n",
      "Iteration 155, loss = 8409.15666593\n",
      "Iteration 156, loss = 8408.41024078\n",
      "Iteration 157, loss = 8408.32992708\n",
      "Iteration 158, loss = 8408.51035700\n",
      "Iteration 159, loss = 8408.58309220\n",
      "Iteration 160, loss = 8408.48943748\n",
      "Iteration 161, loss = 8408.08862973\n",
      "Iteration 162, loss = 8408.16252688\n",
      "Iteration 163, loss = 8407.82129016\n",
      "Iteration 164, loss = 8407.60295055\n",
      "Iteration 165, loss = 8407.86554376\n",
      "Iteration 166, loss = 8406.92861803\n",
      "Iteration 167, loss = 8407.19722195\n",
      "Iteration 168, loss = 8407.21563187\n",
      "Iteration 169, loss = 8406.83774366\n",
      "Iteration 170, loss = 8406.78340378\n",
      "Iteration 171, loss = 8406.63628850\n",
      "Iteration 172, loss = 8406.38980001\n",
      "Iteration 173, loss = 8406.52512986\n",
      "Iteration 174, loss = 8406.13571126\n",
      "Iteration 175, loss = 8406.14574668\n",
      "Iteration 176, loss = 8405.79943735\n",
      "Iteration 177, loss = 8406.24261900\n",
      "Iteration 178, loss = 8405.83317401\n",
      "Iteration 179, loss = 8405.29911807\n",
      "Iteration 180, loss = 8405.84568455\n",
      "Iteration 181, loss = 8405.44366196\n",
      "Iteration 182, loss = 8404.78640838\n",
      "Iteration 183, loss = 8405.28642834\n",
      "Iteration 184, loss = 8404.73454247\n",
      "Iteration 185, loss = 8404.67547804\n",
      "Iteration 186, loss = 8404.84372727\n",
      "Iteration 187, loss = 8404.19545154\n",
      "Iteration 188, loss = 8404.28689535\n",
      "Iteration 189, loss = 8404.29276163\n",
      "Iteration 190, loss = 8403.75308561\n",
      "Iteration 191, loss = 8403.64649637\n",
      "Iteration 192, loss = 8404.52644803\n",
      "Iteration 193, loss = 8403.74569097\n",
      "Iteration 194, loss = 8403.28433100\n",
      "Iteration 195, loss = 8402.78744833\n",
      "Iteration 196, loss = 8404.35303814\n",
      "Iteration 197, loss = 8402.78070019\n",
      "Iteration 198, loss = 8402.85103495\n",
      "Iteration 199, loss = 8402.55936963\n",
      "Iteration 200, loss = 8402.35318267\n",
      "Iteration 1, loss = 13849.71058699\n",
      "Iteration 2, loss = 4588.99743067\n",
      "Iteration 3, loss = 4589.02813192\n",
      "Iteration 4, loss = 4588.73360760\n",
      "Iteration 5, loss = 4588.71533659\n",
      "Iteration 6, loss = 4589.17525228\n",
      "Iteration 7, loss = 4588.79590113\n",
      "Iteration 8, loss = 4588.91781256\n",
      "Iteration 9, loss = 4588.65432646\n",
      "Iteration 10, loss = 4588.77749087\n",
      "Iteration 11, loss = 4589.07937831\n",
      "Iteration 12, loss = 4588.51667203\n",
      "Iteration 13, loss = 4588.51591680\n",
      "Iteration 14, loss = 4588.43856255\n",
      "Iteration 15, loss = 4588.54207254\n",
      "Iteration 16, loss = 4588.56560495\n",
      "Iteration 17, loss = 4588.60104816\n",
      "Iteration 18, loss = 4588.08154767\n",
      "Iteration 19, loss = 4588.60053853\n",
      "Iteration 20, loss = 4588.71091495\n",
      "Iteration 21, loss = 4588.07790303\n",
      "Iteration 22, loss = 4588.09481700\n",
      "Iteration 23, loss = 4588.52437297\n",
      "Iteration 24, loss = 4587.95843933\n",
      "Iteration 25, loss = 4588.16756121\n",
      "Iteration 26, loss = 4588.14219366\n",
      "Iteration 27, loss = 4588.06925674\n",
      "Iteration 28, loss = 4588.01696710\n",
      "Iteration 29, loss = 4588.09684193\n",
      "Iteration 30, loss = 4587.72421876\n",
      "Iteration 31, loss = 4587.91442606\n",
      "Iteration 32, loss = 4587.94987409\n",
      "Iteration 33, loss = 4587.89177117\n",
      "Iteration 34, loss = 4587.50515714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 4587.57858826\n",
      "Iteration 36, loss = 4587.85012487\n",
      "Iteration 37, loss = 4588.02522710\n",
      "Iteration 38, loss = 4587.72171723\n",
      "Iteration 39, loss = 4587.77274629\n",
      "Iteration 40, loss = 4587.35133806\n",
      "Iteration 41, loss = 4587.53785076\n",
      "Iteration 42, loss = 4587.85660687\n",
      "Iteration 43, loss = 4587.74882236\n",
      "Iteration 44, loss = 4587.46532069\n",
      "Iteration 45, loss = 4587.21881885\n",
      "Iteration 46, loss = 4587.58771766\n",
      "Iteration 47, loss = 4587.31661346\n",
      "Iteration 48, loss = 4587.01055652\n",
      "Iteration 49, loss = 4587.01893770\n",
      "Iteration 50, loss = 4586.74700171\n",
      "Iteration 51, loss = 4587.28830575\n",
      "Iteration 52, loss = 4587.03431378\n",
      "Iteration 53, loss = 4588.00674707\n",
      "Iteration 54, loss = 4586.74310894\n",
      "Iteration 55, loss = 4587.11505168\n",
      "Iteration 56, loss = 4586.78648870\n",
      "Iteration 57, loss = 4586.93793204\n",
      "Iteration 58, loss = 4587.13446840\n",
      "Iteration 59, loss = 4587.26341807\n",
      "Iteration 60, loss = 4586.96366987\n",
      "Iteration 61, loss = 4587.08289022\n",
      "Iteration 62, loss = 4586.66637598\n",
      "Iteration 63, loss = 4587.05450465\n",
      "Iteration 64, loss = 4586.65473077\n",
      "Iteration 65, loss = 4586.58216302\n",
      "Iteration 66, loss = 4586.61479071\n",
      "Iteration 67, loss = 4586.30846134\n",
      "Iteration 68, loss = 4587.00146525\n",
      "Iteration 69, loss = 4586.82175548\n",
      "Iteration 70, loss = 4586.27313381\n",
      "Iteration 71, loss = 4586.94878039\n",
      "Iteration 72, loss = 4586.51352124\n",
      "Iteration 73, loss = 4586.30384682\n",
      "Iteration 74, loss = 4586.58628132\n",
      "Iteration 75, loss = 4586.78814917\n",
      "Iteration 76, loss = 4586.44782830\n",
      "Iteration 77, loss = 4586.26815500\n",
      "Iteration 78, loss = 4586.05941094\n",
      "Iteration 79, loss = 4586.14817976\n",
      "Iteration 80, loss = 4585.57897832\n",
      "Iteration 81, loss = 4586.44134110\n",
      "Iteration 82, loss = 4585.92642525\n",
      "Iteration 83, loss = 4586.11783965\n",
      "Iteration 84, loss = 4586.07497096\n",
      "Iteration 85, loss = 4586.13956677\n",
      "Iteration 86, loss = 4586.22491059\n",
      "Iteration 87, loss = 4585.95738326\n",
      "Iteration 88, loss = 4585.43843454\n",
      "Iteration 89, loss = 4586.08929694\n",
      "Iteration 90, loss = 4585.67787770\n",
      "Iteration 91, loss = 4586.01632615\n",
      "Iteration 92, loss = 4585.38145091\n",
      "Iteration 93, loss = 4585.64466240\n",
      "Iteration 94, loss = 4585.69015740\n",
      "Iteration 95, loss = 4585.89953904\n",
      "Iteration 96, loss = 4585.50444782\n",
      "Iteration 97, loss = 4585.82778841\n",
      "Iteration 98, loss = 4585.72599928\n",
      "Iteration 99, loss = 4584.86670980\n",
      "Iteration 100, loss = 4585.63586133\n",
      "Iteration 101, loss = 4585.35946020\n",
      "Iteration 102, loss = 4585.41984511\n",
      "Iteration 103, loss = 4585.72468297\n",
      "Iteration 104, loss = 4585.55528304\n",
      "Iteration 105, loss = 4585.33185900\n",
      "Iteration 106, loss = 4585.32413233\n",
      "Iteration 107, loss = 4585.19323111\n",
      "Iteration 108, loss = 4585.27723414\n",
      "Iteration 109, loss = 4585.34916159\n",
      "Iteration 110, loss = 4585.62910735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 12730.83692570\n",
      "Iteration 2, loss = 4476.77939894\n",
      "Iteration 3, loss = 4476.06404109\n",
      "Iteration 4, loss = 4476.00515859\n",
      "Iteration 5, loss = 4476.02739087\n",
      "Iteration 6, loss = 4476.64438411\n",
      "Iteration 7, loss = 4476.08295594\n",
      "Iteration 8, loss = 4475.79410010\n",
      "Iteration 9, loss = 4476.06823603\n",
      "Iteration 10, loss = 4475.92649798\n",
      "Iteration 11, loss = 4476.39930037\n",
      "Iteration 12, loss = 4475.38166320\n",
      "Iteration 13, loss = 4475.49106738\n",
      "Iteration 14, loss = 4475.71352160\n",
      "Iteration 15, loss = 4475.39180173\n",
      "Iteration 16, loss = 4476.12043568\n",
      "Iteration 17, loss = 4475.73796565\n",
      "Iteration 18, loss = 4475.54075349\n",
      "Iteration 19, loss = 4476.18386298\n",
      "Iteration 20, loss = 4475.65739975\n",
      "Iteration 21, loss = 4475.43425760\n",
      "Iteration 22, loss = 4475.32756625\n",
      "Iteration 23, loss = 4475.26713795\n",
      "Iteration 24, loss = 4474.93464028\n",
      "Iteration 25, loss = 4475.20937251\n",
      "Iteration 26, loss = 4475.47581602\n",
      "Iteration 27, loss = 4475.59291842\n",
      "Iteration 28, loss = 4475.52150570\n",
      "Iteration 29, loss = 4475.53247055\n",
      "Iteration 30, loss = 4475.05253626\n",
      "Iteration 31, loss = 4475.29602951\n",
      "Iteration 32, loss = 4475.05383385\n",
      "Iteration 33, loss = 4475.31348809\n",
      "Iteration 34, loss = 4475.43042333\n",
      "Iteration 35, loss = 4474.80515645\n",
      "Iteration 36, loss = 4475.11601555\n",
      "Iteration 37, loss = 4475.30854496\n",
      "Iteration 38, loss = 4475.25480284\n",
      "Iteration 39, loss = 4474.83356068\n",
      "Iteration 40, loss = 4474.70632636\n",
      "Iteration 41, loss = 4475.05147756\n",
      "Iteration 42, loss = 4475.02986162\n",
      "Iteration 43, loss = 4474.62504207\n",
      "Iteration 44, loss = 4474.82040452\n",
      "Iteration 45, loss = 4474.58464630\n",
      "Iteration 46, loss = 4475.08992277\n",
      "Iteration 47, loss = 4474.66849214\n",
      "Iteration 48, loss = 4474.20778470\n",
      "Iteration 49, loss = 4474.74642105\n",
      "Iteration 50, loss = 4473.89643051\n",
      "Iteration 51, loss = 4474.61723371\n",
      "Iteration 52, loss = 4474.62993928\n",
      "Iteration 53, loss = 4475.38703638\n",
      "Iteration 54, loss = 4474.00546317\n",
      "Iteration 55, loss = 4474.40137602\n",
      "Iteration 56, loss = 4474.25750586\n",
      "Iteration 57, loss = 4474.40309477\n",
      "Iteration 58, loss = 4474.40657867\n",
      "Iteration 59, loss = 4474.41821633\n",
      "Iteration 60, loss = 4474.57730586\n",
      "Iteration 61, loss = 4474.54122119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 9115.28607635\n",
      "Iteration 2, loss = 3861.80746768\n",
      "Iteration 3, loss = 3861.34765616\n",
      "Iteration 4, loss = 3861.53735272\n",
      "Iteration 5, loss = 3861.11099215\n",
      "Iteration 6, loss = 3862.20416147\n",
      "Iteration 7, loss = 3861.26927129\n",
      "Iteration 8, loss = 3860.86465057\n",
      "Iteration 9, loss = 3861.46126938\n",
      "Iteration 10, loss = 3860.94872865\n",
      "Iteration 11, loss = 3861.53149193\n",
      "Iteration 12, loss = 3860.77572493\n",
      "Iteration 13, loss = 3860.82910748\n",
      "Iteration 14, loss = 3861.24688809\n",
      "Iteration 15, loss = 3860.72922194\n",
      "Iteration 16, loss = 3861.51842575\n",
      "Iteration 17, loss = 3860.73737989\n",
      "Iteration 18, loss = 3860.99725831\n",
      "Iteration 19, loss = 3861.08272866\n",
      "Iteration 20, loss = 3860.91381897\n",
      "Iteration 21, loss = 3860.73546890\n",
      "Iteration 22, loss = 3860.90952116\n",
      "Iteration 23, loss = 3860.76147431\n",
      "Iteration 24, loss = 3860.96344229\n",
      "Iteration 25, loss = 3860.46724877\n",
      "Iteration 26, loss = 3861.09187962\n",
      "Iteration 27, loss = 3861.21463513\n",
      "Iteration 28, loss = 3861.05365466\n",
      "Iteration 29, loss = 3860.87501989\n",
      "Iteration 30, loss = 3860.83340308\n",
      "Iteration 31, loss = 3861.04244600\n",
      "Iteration 32, loss = 3860.62278637\n",
      "Iteration 33, loss = 3860.99276029\n",
      "Iteration 34, loss = 3861.37401524\n",
      "Iteration 35, loss = 3860.95749163\n",
      "Iteration 36, loss = 3860.45531912\n",
      "Iteration 37, loss = 3860.73613753\n",
      "Iteration 38, loss = 3860.48011144\n",
      "Iteration 39, loss = 3860.62714688\n",
      "Iteration 40, loss = 3860.21665107\n",
      "Iteration 41, loss = 3860.82953696\n",
      "Iteration 42, loss = 3860.62126325\n",
      "Iteration 43, loss = 3860.35427284\n",
      "Iteration 44, loss = 3860.58291501\n",
      "Iteration 45, loss = 3860.45271525\n",
      "Iteration 46, loss = 3860.58461644\n",
      "Iteration 47, loss = 3860.12430511\n",
      "Iteration 48, loss = 3860.25780241\n",
      "Iteration 49, loss = 3860.57860263\n",
      "Iteration 50, loss = 3860.29213115\n",
      "Iteration 51, loss = 3860.76924550\n",
      "Iteration 52, loss = 3860.54774498\n",
      "Iteration 53, loss = 3860.94854720\n",
      "Iteration 54, loss = 3860.25504434\n",
      "Iteration 55, loss = 3860.77732586\n",
      "Iteration 56, loss = 3860.42215518\n",
      "Iteration 57, loss = 3860.65037694\n",
      "Iteration 58, loss = 3860.44946328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10143.29496425\n",
      "Iteration 2, loss = 3999.55826371\n",
      "Iteration 3, loss = 3999.59203929\n",
      "Iteration 4, loss = 3999.39764846\n",
      "Iteration 5, loss = 3999.14256753\n",
      "Iteration 6, loss = 3999.74611328\n",
      "Iteration 7, loss = 3999.52284726\n",
      "Iteration 8, loss = 3999.65457380\n",
      "Iteration 9, loss = 3999.69841600\n",
      "Iteration 10, loss = 3999.02969113\n",
      "Iteration 11, loss = 3999.19333282\n",
      "Iteration 12, loss = 3999.48378690\n",
      "Iteration 13, loss = 3999.48824456\n",
      "Iteration 14, loss = 3999.17453975\n",
      "Iteration 15, loss = 3999.81591163\n",
      "Iteration 16, loss = 3998.91874219\n",
      "Iteration 17, loss = 3999.17685400\n",
      "Iteration 18, loss = 3998.86333763\n",
      "Iteration 19, loss = 3999.17239776\n",
      "Iteration 20, loss = 3999.20171851\n",
      "Iteration 21, loss = 3999.13066610\n",
      "Iteration 22, loss = 3999.42812949\n",
      "Iteration 23, loss = 3999.01944952\n",
      "Iteration 24, loss = 3999.37725661\n",
      "Iteration 25, loss = 3999.37148230\n",
      "Iteration 26, loss = 3999.35448177\n",
      "Iteration 27, loss = 3998.59282906\n",
      "Iteration 28, loss = 3998.55300436\n",
      "Iteration 29, loss = 3999.29756172\n",
      "Iteration 30, loss = 3998.91258453\n",
      "Iteration 31, loss = 3999.36096335\n",
      "Iteration 32, loss = 3999.19289970\n",
      "Iteration 33, loss = 3998.57144183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 3998.93180668\n",
      "Iteration 35, loss = 3999.16065483\n",
      "Iteration 36, loss = 3998.97651425\n",
      "Iteration 37, loss = 3998.82716955\n",
      "Iteration 38, loss = 3998.80192797\n",
      "Iteration 39, loss = 3999.17846524\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4580.76234027\n",
      "Iteration 2, loss = 1121.89571908\n",
      "Iteration 3, loss = 603.47532016\n",
      "Iteration 4, loss = 406.75973306\n",
      "Iteration 5, loss = 319.93459389\n",
      "Iteration 6, loss = 304.89710754\n",
      "Iteration 7, loss = 299.62537757\n",
      "Iteration 8, loss = 297.65448428\n",
      "Iteration 9, loss = 297.46749878\n",
      "Iteration 10, loss = 297.16003393\n",
      "Iteration 11, loss = 297.27261785\n",
      "Iteration 12, loss = 296.86761314\n",
      "Iteration 13, loss = 296.67976115\n",
      "Iteration 14, loss = 297.08345733\n",
      "Iteration 15, loss = 296.85576312\n",
      "Iteration 16, loss = 296.70376953\n",
      "Iteration 17, loss = 296.57657794\n",
      "Iteration 18, loss = 297.38336728\n",
      "Iteration 19, loss = 296.87167453\n",
      "Iteration 20, loss = 297.00743033\n",
      "Iteration 21, loss = 296.71810828\n",
      "Iteration 22, loss = 297.45664822\n",
      "Iteration 23, loss = 296.59566643\n",
      "Iteration 24, loss = 296.48148475\n",
      "Iteration 25, loss = 296.89469289\n",
      "Iteration 26, loss = 296.68048393\n",
      "Iteration 27, loss = 296.90401873\n",
      "Iteration 28, loss = 296.99489354\n",
      "Iteration 29, loss = 296.29440613\n",
      "Iteration 30, loss = 295.97861672\n",
      "Iteration 31, loss = 296.15976842\n",
      "Iteration 32, loss = 296.26165210\n",
      "Iteration 33, loss = 296.64049768\n",
      "Iteration 34, loss = 296.37423094\n",
      "Iteration 35, loss = 296.82116588\n",
      "Iteration 36, loss = 296.23496776\n",
      "Iteration 37, loss = 296.26813775\n",
      "Iteration 38, loss = 295.79291677\n",
      "Iteration 39, loss = 295.84812222\n",
      "Iteration 40, loss = 296.26484710\n",
      "Iteration 41, loss = 296.30822208\n",
      "Iteration 42, loss = 296.31623815\n",
      "Iteration 43, loss = 295.89969763\n",
      "Iteration 44, loss = 296.62592996\n",
      "Iteration 45, loss = 296.98145814\n",
      "Iteration 46, loss = 296.44375286\n",
      "Iteration 47, loss = 297.22106500\n",
      "Iteration 48, loss = 296.83643655\n",
      "Iteration 49, loss = 296.60327860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4558.04685862\n",
      "Iteration 2, loss = 1110.92863892\n",
      "Iteration 3, loss = 613.77950704\n",
      "Iteration 4, loss = 433.11987126\n",
      "Iteration 5, loss = 336.15333569\n",
      "Iteration 6, loss = 306.98570377\n",
      "Iteration 7, loss = 301.45335032\n",
      "Iteration 8, loss = 299.14528826\n",
      "Iteration 9, loss = 299.25900184\n",
      "Iteration 10, loss = 298.03347861\n",
      "Iteration 11, loss = 298.50827172\n",
      "Iteration 12, loss = 296.00860592\n",
      "Iteration 13, loss = 293.87401915\n",
      "Iteration 14, loss = 292.85350798\n",
      "Iteration 15, loss = 293.04351330\n",
      "Iteration 16, loss = 292.43326337\n",
      "Iteration 17, loss = 292.88926194\n",
      "Iteration 18, loss = 292.76714219\n",
      "Iteration 19, loss = 292.60168256\n",
      "Iteration 20, loss = 292.57788052\n",
      "Iteration 21, loss = 292.40372887\n",
      "Iteration 22, loss = 292.64906364\n",
      "Iteration 23, loss = 292.15731296\n",
      "Iteration 24, loss = 292.14382137\n",
      "Iteration 25, loss = 292.82141667\n",
      "Iteration 26, loss = 292.60686328\n",
      "Iteration 27, loss = 293.33504075\n",
      "Iteration 28, loss = 292.31990073\n",
      "Iteration 29, loss = 292.43023331\n",
      "Iteration 30, loss = 292.13651389\n",
      "Iteration 31, loss = 292.69683395\n",
      "Iteration 32, loss = 292.24749588\n",
      "Iteration 33, loss = 292.41021832\n",
      "Iteration 34, loss = 291.61578345\n",
      "Iteration 35, loss = 292.62333208\n",
      "Iteration 36, loss = 292.01029724\n",
      "Iteration 37, loss = 291.69999405\n",
      "Iteration 38, loss = 292.36300126\n",
      "Iteration 39, loss = 292.36779651\n",
      "Iteration 40, loss = 292.27603570\n",
      "Iteration 41, loss = 292.19190746\n",
      "Iteration 42, loss = 292.28458265\n",
      "Iteration 43, loss = 292.55741295\n",
      "Iteration 44, loss = 292.70169150\n",
      "Iteration 45, loss = 292.44814437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4560.07279640\n",
      "Iteration 2, loss = 1103.86113740\n",
      "Iteration 3, loss = 609.62922133\n",
      "Iteration 4, loss = 432.24270214\n",
      "Iteration 5, loss = 339.57907973\n",
      "Iteration 6, loss = 305.20590421\n",
      "Iteration 7, loss = 299.51798334\n",
      "Iteration 8, loss = 297.76695495\n",
      "Iteration 9, loss = 297.11717680\n",
      "Iteration 10, loss = 296.76944383\n",
      "Iteration 11, loss = 296.82218812\n",
      "Iteration 12, loss = 296.25423615\n",
      "Iteration 13, loss = 292.49173700\n",
      "Iteration 14, loss = 291.79711352\n",
      "Iteration 15, loss = 292.04057936\n",
      "Iteration 16, loss = 291.10346399\n",
      "Iteration 17, loss = 291.79579323\n",
      "Iteration 18, loss = 291.58183460\n",
      "Iteration 19, loss = 291.35576084\n",
      "Iteration 20, loss = 291.27919914\n",
      "Iteration 21, loss = 290.91058346\n",
      "Iteration 22, loss = 290.63524808\n",
      "Iteration 23, loss = 290.65906914\n",
      "Iteration 24, loss = 290.64402444\n",
      "Iteration 25, loss = 291.11549368\n",
      "Iteration 26, loss = 291.33044102\n",
      "Iteration 27, loss = 291.44797625\n",
      "Iteration 28, loss = 290.46904987\n",
      "Iteration 29, loss = 290.92871509\n",
      "Iteration 30, loss = 291.01678012\n",
      "Iteration 31, loss = 291.59365611\n",
      "Iteration 32, loss = 290.48427655\n",
      "Iteration 33, loss = 291.56820547\n",
      "Iteration 34, loss = 290.47806177\n",
      "Iteration 35, loss = 291.84375058\n",
      "Iteration 36, loss = 290.81988464\n",
      "Iteration 37, loss = 290.28823193\n",
      "Iteration 38, loss = 290.80589611\n",
      "Iteration 39, loss = 290.88434189\n",
      "Iteration 40, loss = 290.77583986\n",
      "Iteration 41, loss = 291.05581772\n",
      "Iteration 42, loss = 290.53142750\n",
      "Iteration 43, loss = 291.69295702\n",
      "Iteration 44, loss = 290.92240976\n",
      "Iteration 45, loss = 290.66768987\n",
      "Iteration 46, loss = 290.45548897\n",
      "Iteration 47, loss = 291.05562678\n",
      "Iteration 48, loss = 291.26153340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4564.10581904\n",
      "Iteration 2, loss = 1090.09916583\n",
      "Iteration 3, loss = 600.84548721\n",
      "Iteration 4, loss = 432.45193347\n",
      "Iteration 5, loss = 353.14425469\n",
      "Iteration 6, loss = 309.50737076\n",
      "Iteration 7, loss = 302.19380453\n",
      "Iteration 8, loss = 300.54002233\n",
      "Iteration 9, loss = 299.88613650\n",
      "Iteration 10, loss = 300.27267672\n",
      "Iteration 11, loss = 297.23779167\n",
      "Iteration 12, loss = 295.03038218\n",
      "Iteration 13, loss = 293.91286535\n",
      "Iteration 14, loss = 294.93533983\n",
      "Iteration 15, loss = 294.47889792\n",
      "Iteration 16, loss = 294.35747088\n",
      "Iteration 17, loss = 293.78858576\n",
      "Iteration 18, loss = 293.57534579\n",
      "Iteration 19, loss = 293.18991864\n",
      "Iteration 20, loss = 293.75444373\n",
      "Iteration 21, loss = 293.65363367\n",
      "Iteration 22, loss = 293.20715886\n",
      "Iteration 23, loss = 292.84614992\n",
      "Iteration 24, loss = 293.25664040\n",
      "Iteration 25, loss = 293.36653937\n",
      "Iteration 26, loss = 294.75547469\n",
      "Iteration 27, loss = 293.56629913\n",
      "Iteration 28, loss = 293.61469307\n",
      "Iteration 29, loss = 293.29056008\n",
      "Iteration 30, loss = 293.27227637\n",
      "Iteration 31, loss = 294.05204756\n",
      "Iteration 32, loss = 293.14555725\n",
      "Iteration 33, loss = 294.01116082\n",
      "Iteration 34, loss = 293.34964840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4574.88279390\n",
      "Iteration 2, loss = 1089.06340149\n",
      "Iteration 3, loss = 598.48816903\n",
      "Iteration 4, loss = 426.48764207\n",
      "Iteration 5, loss = 348.90978382\n",
      "Iteration 6, loss = 307.40572291\n",
      "Iteration 7, loss = 300.74019141\n",
      "Iteration 8, loss = 298.70425167\n",
      "Iteration 9, loss = 297.54530367\n",
      "Iteration 10, loss = 296.61716752\n",
      "Iteration 11, loss = 292.53148927\n",
      "Iteration 12, loss = 292.43505581\n",
      "Iteration 13, loss = 292.90932717\n",
      "Iteration 14, loss = 292.63969633\n",
      "Iteration 15, loss = 292.58402027\n",
      "Iteration 16, loss = 293.18265704\n",
      "Iteration 17, loss = 292.38956632\n",
      "Iteration 18, loss = 292.10957447\n",
      "Iteration 19, loss = 293.11415629\n",
      "Iteration 20, loss = 291.92626216\n",
      "Iteration 21, loss = 291.93601005\n",
      "Iteration 22, loss = 292.01570276\n",
      "Iteration 23, loss = 293.38931024\n",
      "Iteration 24, loss = 291.79829130\n",
      "Iteration 25, loss = 292.09287439\n",
      "Iteration 26, loss = 292.48170755\n",
      "Iteration 27, loss = 292.37763371\n",
      "Iteration 28, loss = 292.20769798\n",
      "Iteration 29, loss = 291.55653970\n",
      "Iteration 30, loss = 292.14047527\n",
      "Iteration 31, loss = 292.23551819\n",
      "Iteration 32, loss = 292.13382146\n",
      "Iteration 33, loss = 292.33234723\n",
      "Iteration 34, loss = 291.81975312\n",
      "Iteration 35, loss = 291.84216822\n",
      "Iteration 36, loss = 292.40546204\n",
      "Iteration 37, loss = 292.20924520\n",
      "Iteration 38, loss = 292.18880185\n",
      "Iteration 39, loss = 291.64404836\n",
      "Iteration 40, loss = 291.81810718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 28373.35948722\n",
      "Iteration 2, loss = 8430.71563997\n",
      "Iteration 3, loss = 8430.69304280\n",
      "Iteration 4, loss = 8430.29295443\n",
      "Iteration 5, loss = 8430.02827106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 8430.11654493\n",
      "Iteration 7, loss = 8429.63107795\n",
      "Iteration 8, loss = 8429.69211428\n",
      "Iteration 9, loss = 8429.46427821\n",
      "Iteration 10, loss = 8429.26137230\n",
      "Iteration 11, loss = 8429.40720172\n",
      "Iteration 12, loss = 8429.81604737\n",
      "Iteration 13, loss = 8428.74651773\n",
      "Iteration 14, loss = 8428.71850557\n",
      "Iteration 15, loss = 8428.77962953\n",
      "Iteration 16, loss = 8428.44265494\n",
      "Iteration 17, loss = 8428.84429735\n",
      "Iteration 18, loss = 8428.05475050\n",
      "Iteration 19, loss = 8428.25562500\n",
      "Iteration 20, loss = 8428.18821137\n",
      "Iteration 21, loss = 8427.79732142\n",
      "Iteration 22, loss = 8427.32878484\n",
      "Iteration 23, loss = 8427.69886561\n",
      "Iteration 24, loss = 8426.59049927\n",
      "Iteration 25, loss = 8427.46373249\n",
      "Iteration 26, loss = 8426.77597378\n",
      "Iteration 27, loss = 8427.57973120\n",
      "Iteration 28, loss = 8426.42399067\n",
      "Iteration 29, loss = 8427.03641093\n",
      "Iteration 30, loss = 8426.85845470\n",
      "Iteration 31, loss = 8426.30006223\n",
      "Iteration 32, loss = 8426.18039790\n",
      "Iteration 33, loss = 8425.63513036\n",
      "Iteration 34, loss = 8425.81775850\n",
      "Iteration 35, loss = 8425.79246507\n",
      "Iteration 36, loss = 8425.65248394\n",
      "Iteration 37, loss = 8425.54739585\n",
      "Iteration 38, loss = 8425.07368587\n",
      "Iteration 39, loss = 8425.68512680\n",
      "Iteration 40, loss = 8424.99937046\n",
      "Iteration 41, loss = 8425.14733491\n",
      "Iteration 42, loss = 8425.46012182\n",
      "Iteration 43, loss = 8424.98874240\n",
      "Iteration 44, loss = 8424.69452925\n",
      "Iteration 45, loss = 8424.44073197\n",
      "Iteration 46, loss = 8424.56211245\n",
      "Iteration 47, loss = 8424.61377210\n",
      "Iteration 48, loss = 8423.84426516\n",
      "Iteration 49, loss = 8423.62368936\n",
      "Iteration 50, loss = 8423.32830000\n",
      "Iteration 51, loss = 8423.80215665\n",
      "Iteration 52, loss = 8423.62737856\n",
      "Iteration 53, loss = 8423.61107907\n",
      "Iteration 54, loss = 8423.15531482\n",
      "Iteration 55, loss = 8423.27080227\n",
      "Iteration 56, loss = 8422.45250584\n",
      "Iteration 57, loss = 8423.05283900\n",
      "Iteration 58, loss = 8422.77818247\n",
      "Iteration 59, loss = 8423.19080981\n",
      "Iteration 60, loss = 8422.40638952\n",
      "Iteration 61, loss = 8422.34688922\n",
      "Iteration 62, loss = 8421.76686015\n",
      "Iteration 63, loss = 8421.84317510\n",
      "Iteration 64, loss = 8421.90860404\n",
      "Iteration 65, loss = 8421.55274855\n",
      "Iteration 66, loss = 8421.72571184\n",
      "Iteration 67, loss = 8421.29207413\n",
      "Iteration 68, loss = 8421.19490499\n",
      "Iteration 69, loss = 8421.08800659\n",
      "Iteration 70, loss = 8421.02237632\n",
      "Iteration 71, loss = 8420.82722009\n",
      "Iteration 72, loss = 8420.41447567\n",
      "Iteration 73, loss = 8420.66537491\n",
      "Iteration 74, loss = 8420.38916758\n",
      "Iteration 75, loss = 8420.37505492\n",
      "Iteration 76, loss = 8420.17496773\n",
      "Iteration 77, loss = 8419.79296476\n",
      "Iteration 78, loss = 8419.92993649\n",
      "Iteration 79, loss = 8419.43887457\n",
      "Iteration 80, loss = 8419.32313120\n",
      "Iteration 81, loss = 8419.75444189\n",
      "Iteration 82, loss = 8418.78995215\n",
      "Iteration 83, loss = 8419.09228642\n",
      "Iteration 84, loss = 8418.86462541\n",
      "Iteration 85, loss = 8419.36902541\n",
      "Iteration 86, loss = 8418.89178302\n",
      "Iteration 87, loss = 8418.38865067\n",
      "Iteration 88, loss = 8418.39391396\n",
      "Iteration 89, loss = 8418.08742685\n",
      "Iteration 90, loss = 8418.43012633\n",
      "Iteration 91, loss = 8418.02329404\n",
      "Iteration 92, loss = 8417.60403903\n",
      "Iteration 93, loss = 8417.93670318\n",
      "Iteration 94, loss = 8417.45836925\n",
      "Iteration 95, loss = 8417.72893163\n",
      "Iteration 96, loss = 8417.15403100\n",
      "Iteration 97, loss = 8417.37357206\n",
      "Iteration 98, loss = 8417.05614804\n",
      "Iteration 99, loss = 8416.53383125\n",
      "Iteration 100, loss = 8416.54785790\n",
      "Iteration 101, loss = 8416.68450519\n",
      "Iteration 102, loss = 8416.60907406\n",
      "Iteration 103, loss = 8416.27395998\n",
      "Iteration 104, loss = 8416.33928094\n",
      "Iteration 105, loss = 8416.52363860\n",
      "Iteration 106, loss = 8415.82476409\n",
      "Iteration 107, loss = 8415.92657087\n",
      "Iteration 108, loss = 8415.46795107\n",
      "Iteration 109, loss = 8415.35836920\n",
      "Iteration 110, loss = 8415.83852360\n",
      "Iteration 111, loss = 8415.34388397\n",
      "Iteration 112, loss = 8415.05979880\n",
      "Iteration 113, loss = 8414.72209454\n",
      "Iteration 114, loss = 8414.57683108\n",
      "Iteration 115, loss = 8414.47922738\n",
      "Iteration 116, loss = 8414.16121446\n",
      "Iteration 117, loss = 8414.33557862\n",
      "Iteration 118, loss = 8414.18362154\n",
      "Iteration 119, loss = 8414.39856036\n",
      "Iteration 120, loss = 8413.37510675\n",
      "Iteration 121, loss = 8413.98706516\n",
      "Iteration 122, loss = 8413.62600168\n",
      "Iteration 123, loss = 8413.17462063\n",
      "Iteration 124, loss = 8413.64357188\n",
      "Iteration 125, loss = 8413.12379334\n",
      "Iteration 126, loss = 8413.39800410\n",
      "Iteration 127, loss = 8412.79866291\n",
      "Iteration 128, loss = 8412.44005327\n",
      "Iteration 129, loss = 8412.76079750\n",
      "Iteration 130, loss = 8412.21244201\n",
      "Iteration 131, loss = 8412.17168117\n",
      "Iteration 132, loss = 8412.01699487\n",
      "Iteration 133, loss = 8411.65512683\n",
      "Iteration 134, loss = 8412.52143540\n",
      "Iteration 135, loss = 8411.70173340\n",
      "Iteration 136, loss = 8411.74228058\n",
      "Iteration 137, loss = 8411.51951398\n",
      "Iteration 138, loss = 8411.04962317\n",
      "Iteration 139, loss = 8411.07805725\n",
      "Iteration 140, loss = 8410.83545848\n",
      "Iteration 141, loss = 8411.08222612\n",
      "Iteration 142, loss = 8410.34621466\n",
      "Iteration 143, loss = 8410.43436621\n",
      "Iteration 144, loss = 8410.42918461\n",
      "Iteration 145, loss = 8410.43592869\n",
      "Iteration 146, loss = 8410.55721897\n",
      "Iteration 147, loss = 8409.93276787\n",
      "Iteration 148, loss = 8409.89447518\n",
      "Iteration 149, loss = 8409.81209550\n",
      "Iteration 150, loss = 8409.61189837\n",
      "Iteration 151, loss = 8409.73317981\n",
      "Iteration 152, loss = 8408.46042518\n",
      "Iteration 153, loss = 8409.10957388\n",
      "Iteration 154, loss = 8409.05372353\n",
      "Iteration 155, loss = 8409.15666593\n",
      "Iteration 156, loss = 8408.41024078\n",
      "Iteration 157, loss = 8408.32992708\n",
      "Iteration 158, loss = 8408.51035700\n",
      "Iteration 159, loss = 8408.58309220\n",
      "Iteration 160, loss = 8408.48943748\n",
      "Iteration 161, loss = 8408.08862973\n",
      "Iteration 162, loss = 8408.16252688\n",
      "Iteration 163, loss = 8407.82129016\n",
      "Iteration 164, loss = 8407.60295055\n",
      "Iteration 165, loss = 8407.86554376\n",
      "Iteration 166, loss = 8406.92861803\n",
      "Iteration 167, loss = 8407.19722195\n",
      "Iteration 168, loss = 8407.21563187\n",
      "Iteration 169, loss = 8406.83774366\n",
      "Iteration 170, loss = 8406.78340378\n",
      "Iteration 171, loss = 8406.63628850\n",
      "Iteration 172, loss = 8406.38980001\n",
      "Iteration 173, loss = 8406.52512986\n",
      "Iteration 174, loss = 8406.13571126\n",
      "Iteration 175, loss = 8406.14574668\n",
      "Iteration 176, loss = 8405.79943735\n",
      "Iteration 177, loss = 8406.24261900\n",
      "Iteration 178, loss = 8405.83317401\n",
      "Iteration 179, loss = 8405.29911807\n",
      "Iteration 180, loss = 8405.84568455\n",
      "Iteration 181, loss = 8405.44366196\n",
      "Iteration 182, loss = 8404.78640838\n",
      "Iteration 183, loss = 8405.28642834\n",
      "Iteration 184, loss = 8404.73454247\n",
      "Iteration 185, loss = 8404.67547804\n",
      "Iteration 186, loss = 8404.84372727\n",
      "Iteration 187, loss = 8404.19545154\n",
      "Iteration 188, loss = 8404.28689535\n",
      "Iteration 189, loss = 8404.29276163\n",
      "Iteration 190, loss = 8403.75308561\n",
      "Iteration 191, loss = 8403.64649637\n",
      "Iteration 192, loss = 8404.52644803\n",
      "Iteration 193, loss = 8403.74569097\n",
      "Iteration 194, loss = 8403.28433100\n",
      "Iteration 195, loss = 8402.78744833\n",
      "Iteration 196, loss = 8404.35303814\n",
      "Iteration 197, loss = 8402.78070019\n",
      "Iteration 198, loss = 8402.85103495\n",
      "Iteration 199, loss = 8402.55936963\n",
      "Iteration 200, loss = 8402.35318267\n",
      "Iteration 1, loss = 13849.71058699\n",
      "Iteration 2, loss = 4588.99743067\n",
      "Iteration 3, loss = 4589.02813192\n",
      "Iteration 4, loss = 4588.73360760\n",
      "Iteration 5, loss = 4588.71533659\n",
      "Iteration 6, loss = 4589.17525228\n",
      "Iteration 7, loss = 4588.79590113\n",
      "Iteration 8, loss = 4588.91781256\n",
      "Iteration 9, loss = 4588.65432646\n",
      "Iteration 10, loss = 4588.77749087\n",
      "Iteration 11, loss = 4589.07937831\n",
      "Iteration 12, loss = 4588.51667203\n",
      "Iteration 13, loss = 4588.51591680\n",
      "Iteration 14, loss = 4588.43856255\n",
      "Iteration 15, loss = 4588.54207254\n",
      "Iteration 16, loss = 4588.56560495\n",
      "Iteration 17, loss = 4588.60104816\n",
      "Iteration 18, loss = 4588.08154767\n",
      "Iteration 19, loss = 4588.60053853\n",
      "Iteration 20, loss = 4588.71091495\n",
      "Iteration 21, loss = 4588.07790303\n",
      "Iteration 22, loss = 4588.09481700\n",
      "Iteration 23, loss = 4588.52437297\n",
      "Iteration 24, loss = 4587.95843933\n",
      "Iteration 25, loss = 4588.16756121\n",
      "Iteration 26, loss = 4588.14219366\n",
      "Iteration 27, loss = 4588.06925674\n",
      "Iteration 28, loss = 4588.01696710\n",
      "Iteration 29, loss = 4588.09684193\n",
      "Iteration 30, loss = 4587.72421876\n",
      "Iteration 31, loss = 4587.91442606\n",
      "Iteration 32, loss = 4587.94987409\n",
      "Iteration 33, loss = 4587.89177117\n",
      "Iteration 34, loss = 4587.50515714\n",
      "Iteration 35, loss = 4587.57858826\n",
      "Iteration 36, loss = 4587.85012487\n",
      "Iteration 37, loss = 4588.02522710\n",
      "Iteration 38, loss = 4587.72171723\n",
      "Iteration 39, loss = 4587.77274629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 4587.35133806\n",
      "Iteration 41, loss = 4587.53785076\n",
      "Iteration 42, loss = 4587.85660687\n",
      "Iteration 43, loss = 4587.74882236\n",
      "Iteration 44, loss = 4587.46532069\n",
      "Iteration 45, loss = 4587.21881885\n",
      "Iteration 46, loss = 4587.58771766\n",
      "Iteration 47, loss = 4587.31661346\n",
      "Iteration 48, loss = 4587.01055652\n",
      "Iteration 49, loss = 4587.01893770\n",
      "Iteration 50, loss = 4586.74700171\n",
      "Iteration 51, loss = 4587.28830575\n",
      "Iteration 52, loss = 4587.03431378\n",
      "Iteration 53, loss = 4588.00674707\n",
      "Iteration 54, loss = 4586.74310894\n",
      "Iteration 55, loss = 4587.11505168\n",
      "Iteration 56, loss = 4586.78648870\n",
      "Iteration 57, loss = 4586.93793204\n",
      "Iteration 58, loss = 4587.13446840\n",
      "Iteration 59, loss = 4587.26341807\n",
      "Iteration 60, loss = 4586.96366987\n",
      "Iteration 61, loss = 4587.08289022\n",
      "Iteration 62, loss = 4586.66637598\n",
      "Iteration 63, loss = 4587.05450465\n",
      "Iteration 64, loss = 4586.65473077\n",
      "Iteration 65, loss = 4586.58216302\n",
      "Iteration 66, loss = 4586.61479071\n",
      "Iteration 67, loss = 4586.30846134\n",
      "Iteration 68, loss = 4587.00146525\n",
      "Iteration 69, loss = 4586.82175548\n",
      "Iteration 70, loss = 4586.27313381\n",
      "Iteration 71, loss = 4586.94878039\n",
      "Iteration 72, loss = 4586.51352124\n",
      "Iteration 73, loss = 4586.30384682\n",
      "Iteration 74, loss = 4586.58628132\n",
      "Iteration 75, loss = 4586.78814917\n",
      "Iteration 76, loss = 4586.44782830\n",
      "Iteration 77, loss = 4586.26815500\n",
      "Iteration 78, loss = 4586.05941094\n",
      "Iteration 79, loss = 4586.14817976\n",
      "Iteration 80, loss = 4585.57897832\n",
      "Iteration 81, loss = 4586.44134110\n",
      "Iteration 82, loss = 4585.92642525\n",
      "Iteration 83, loss = 4586.11783965\n",
      "Iteration 84, loss = 4586.07497096\n",
      "Iteration 85, loss = 4586.13956677\n",
      "Iteration 86, loss = 4586.22491059\n",
      "Iteration 87, loss = 4585.95738326\n",
      "Iteration 88, loss = 4585.43843454\n",
      "Iteration 89, loss = 4586.08929694\n",
      "Iteration 90, loss = 4585.67787770\n",
      "Iteration 91, loss = 4586.01632615\n",
      "Iteration 92, loss = 4585.38145091\n",
      "Iteration 93, loss = 4585.64466240\n",
      "Iteration 94, loss = 4585.69015740\n",
      "Iteration 95, loss = 4585.89953904\n",
      "Iteration 96, loss = 4585.50444782\n",
      "Iteration 97, loss = 4585.82778841\n",
      "Iteration 98, loss = 4585.72599928\n",
      "Iteration 99, loss = 4584.86670980\n",
      "Iteration 100, loss = 4585.63586133\n",
      "Iteration 101, loss = 4585.35946020\n",
      "Iteration 102, loss = 4585.41984511\n",
      "Iteration 103, loss = 4585.72468297\n",
      "Iteration 104, loss = 4585.55528304\n",
      "Iteration 105, loss = 4585.33185900\n",
      "Iteration 106, loss = 4585.32413233\n",
      "Iteration 107, loss = 4585.19323111\n",
      "Iteration 108, loss = 4585.27723414\n",
      "Iteration 109, loss = 4585.34916159\n",
      "Iteration 110, loss = 4585.62910735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 111, loss = 4585.67668253\n",
      "Iteration 112, loss = 4584.32170861\n",
      "Iteration 113, loss = 4584.40769646\n",
      "Iteration 114, loss = 4584.39710502\n",
      "Iteration 115, loss = 4584.38860827\n",
      "Iteration 116, loss = 4584.24102393\n",
      "Iteration 117, loss = 4584.35258144\n",
      "Iteration 118, loss = 4584.44606110\n",
      "Iteration 119, loss = 4584.35035897\n",
      "Iteration 120, loss = 4584.22347164\n",
      "Iteration 121, loss = 4584.39914372\n",
      "Iteration 122, loss = 4584.36334974\n",
      "Iteration 123, loss = 4584.42884981\n",
      "Iteration 124, loss = 4584.37585939\n",
      "Iteration 125, loss = 4584.18381550\n",
      "Iteration 126, loss = 4584.42813484\n",
      "Iteration 127, loss = 4584.27827106\n",
      "Iteration 128, loss = 4584.30592033\n",
      "Iteration 129, loss = 4584.33739712\n",
      "Iteration 130, loss = 4584.26874203\n",
      "Iteration 131, loss = 4584.29202545\n",
      "Iteration 132, loss = 4584.22644212\n",
      "Iteration 133, loss = 4584.04410885\n",
      "Iteration 134, loss = 4584.36618250\n",
      "Iteration 135, loss = 4584.24928241\n",
      "Iteration 136, loss = 4584.18105860\n",
      "Iteration 137, loss = 4584.22193335\n",
      "Iteration 138, loss = 4584.20082142\n",
      "Iteration 139, loss = 4584.27998574\n",
      "Iteration 140, loss = 4584.25137099\n",
      "Iteration 141, loss = 4584.17445273\n",
      "Iteration 142, loss = 4584.23108988\n",
      "Iteration 143, loss = 4584.02458128\n",
      "Iteration 144, loss = 4584.29884145\n",
      "Iteration 145, loss = 4584.21907700\n",
      "Iteration 146, loss = 4584.28899596\n",
      "Iteration 147, loss = 4584.16333459\n",
      "Iteration 148, loss = 4584.28388491\n",
      "Iteration 149, loss = 4584.10033072\n",
      "Iteration 150, loss = 4584.10366036\n",
      "Iteration 151, loss = 4584.13507301\n",
      "Iteration 152, loss = 4583.89755704\n",
      "Iteration 153, loss = 4584.16996253\n",
      "Iteration 154, loss = 4584.05068852\n",
      "Iteration 155, loss = 4584.13730832\n",
      "Iteration 156, loss = 4583.94551273\n",
      "Iteration 157, loss = 4583.98968464\n",
      "Iteration 158, loss = 4584.01691403\n",
      "Iteration 159, loss = 4584.10378000\n",
      "Iteration 160, loss = 4584.09623385\n",
      "Iteration 161, loss = 4584.04438067\n",
      "Iteration 162, loss = 4584.07630387\n",
      "Iteration 163, loss = 4584.08638784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 164, loss = 4584.06177753\n",
      "Iteration 165, loss = 4583.90618650\n",
      "Iteration 166, loss = 4583.86749026\n",
      "Iteration 167, loss = 4583.88596857\n",
      "Iteration 168, loss = 4583.87126844\n",
      "Iteration 169, loss = 4583.88284540\n",
      "Iteration 170, loss = 4583.88785266\n",
      "Iteration 171, loss = 4583.85510921\n",
      "Iteration 172, loss = 4583.87213721\n",
      "Iteration 173, loss = 4583.87360398\n",
      "Iteration 174, loss = 4583.87408855\n",
      "Iteration 175, loss = 4583.85767288\n",
      "Iteration 176, loss = 4583.85329107\n",
      "Iteration 177, loss = 4583.86490752\n",
      "Iteration 178, loss = 4583.87119026\n",
      "Iteration 179, loss = 4583.86677412\n",
      "Iteration 180, loss = 4583.85063025\n",
      "Iteration 181, loss = 4583.84932680\n",
      "Iteration 182, loss = 4583.84225425\n",
      "Iteration 183, loss = 4583.84888742\n",
      "Iteration 184, loss = 4583.85448626\n",
      "Iteration 185, loss = 4583.87330796\n",
      "Iteration 186, loss = 4583.86261282\n",
      "Iteration 187, loss = 4583.86722283\n",
      "Iteration 188, loss = 4583.83597936\n",
      "Iteration 189, loss = 4583.84465530\n",
      "Iteration 190, loss = 4583.84173828\n",
      "Iteration 191, loss = 4583.83357800\n",
      "Iteration 192, loss = 4583.85909188\n",
      "Iteration 193, loss = 4583.84560202\n",
      "Iteration 194, loss = 4583.83072979\n",
      "Iteration 195, loss = 4583.81959959\n",
      "Iteration 196, loss = 4583.86563082\n",
      "Iteration 197, loss = 4583.83210405\n",
      "Iteration 198, loss = 4583.84435603\n",
      "Iteration 199, loss = 4583.86044145\n",
      "Iteration 200, loss = 4583.83733248\n",
      "Iteration 1, loss = 12730.83692570\n",
      "Iteration 2, loss = 4476.77939894\n",
      "Iteration 3, loss = 4476.06404109\n",
      "Iteration 4, loss = 4476.00515859\n",
      "Iteration 5, loss = 4476.02739087\n",
      "Iteration 6, loss = 4476.64438411\n",
      "Iteration 7, loss = 4476.08295594\n",
      "Iteration 8, loss = 4475.79410010\n",
      "Iteration 9, loss = 4476.06823603\n",
      "Iteration 10, loss = 4475.92649798\n",
      "Iteration 11, loss = 4476.39930037\n",
      "Iteration 12, loss = 4475.38166320\n",
      "Iteration 13, loss = 4475.49106738\n",
      "Iteration 14, loss = 4475.71352160\n",
      "Iteration 15, loss = 4475.39180173\n",
      "Iteration 16, loss = 4476.12043568\n",
      "Iteration 17, loss = 4475.73796565\n",
      "Iteration 18, loss = 4475.54075349\n",
      "Iteration 19, loss = 4476.18386298\n",
      "Iteration 20, loss = 4475.65739975\n",
      "Iteration 21, loss = 4475.43425760\n",
      "Iteration 22, loss = 4475.32756625\n",
      "Iteration 23, loss = 4475.26713795\n",
      "Iteration 24, loss = 4474.93464028\n",
      "Iteration 25, loss = 4475.20937251\n",
      "Iteration 26, loss = 4475.47581602\n",
      "Iteration 27, loss = 4475.59291842\n",
      "Iteration 28, loss = 4475.52150570\n",
      "Iteration 29, loss = 4475.53247055\n",
      "Iteration 30, loss = 4475.05253626\n",
      "Iteration 31, loss = 4475.29602951\n",
      "Iteration 32, loss = 4475.05383385\n",
      "Iteration 33, loss = 4475.31348809\n",
      "Iteration 34, loss = 4475.43042333\n",
      "Iteration 35, loss = 4474.80515645\n",
      "Iteration 36, loss = 4475.11601555\n",
      "Iteration 37, loss = 4475.30854496\n",
      "Iteration 38, loss = 4475.25480284\n",
      "Iteration 39, loss = 4474.83356068\n",
      "Iteration 40, loss = 4474.70632636\n",
      "Iteration 41, loss = 4475.05147756\n",
      "Iteration 42, loss = 4475.02986162\n",
      "Iteration 43, loss = 4474.62504207\n",
      "Iteration 44, loss = 4474.82040452\n",
      "Iteration 45, loss = 4474.58464630\n",
      "Iteration 46, loss = 4475.08992277\n",
      "Iteration 47, loss = 4474.66849214\n",
      "Iteration 48, loss = 4474.20778470\n",
      "Iteration 49, loss = 4474.74642105\n",
      "Iteration 50, loss = 4473.89643051\n",
      "Iteration 51, loss = 4474.61723371\n",
      "Iteration 52, loss = 4474.62993928\n",
      "Iteration 53, loss = 4475.38703638\n",
      "Iteration 54, loss = 4474.00546317\n",
      "Iteration 55, loss = 4474.40137602\n",
      "Iteration 56, loss = 4474.25750586\n",
      "Iteration 57, loss = 4474.40309477\n",
      "Iteration 58, loss = 4474.40657867\n",
      "Iteration 59, loss = 4474.41821633\n",
      "Iteration 60, loss = 4474.57730586\n",
      "Iteration 61, loss = 4474.54122119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62, loss = 4473.66524148\n",
      "Iteration 63, loss = 4473.58330805\n",
      "Iteration 64, loss = 4473.63088196\n",
      "Iteration 65, loss = 4473.50932517\n",
      "Iteration 66, loss = 4473.54587957\n",
      "Iteration 67, loss = 4473.59522749\n",
      "Iteration 68, loss = 4473.62885027\n",
      "Iteration 69, loss = 4473.58457814\n",
      "Iteration 70, loss = 4473.52220309\n",
      "Iteration 71, loss = 4473.55814700\n",
      "Iteration 72, loss = 4473.44710373\n",
      "Iteration 73, loss = 4473.53650323\n",
      "Iteration 74, loss = 4473.52573866\n",
      "Iteration 75, loss = 4473.63428263\n",
      "Iteration 76, loss = 4473.53041202\n",
      "Iteration 77, loss = 4473.69445085\n",
      "Iteration 78, loss = 4473.55201747\n",
      "Iteration 79, loss = 4473.48734423\n",
      "Iteration 80, loss = 4473.46330647\n",
      "Iteration 81, loss = 4473.49997392\n",
      "Iteration 82, loss = 4473.42359085\n",
      "Iteration 83, loss = 4473.53219267\n",
      "Iteration 84, loss = 4473.50273276\n",
      "Iteration 85, loss = 4473.49680854\n",
      "Iteration 86, loss = 4473.68277269\n",
      "Iteration 87, loss = 4473.60443168\n",
      "Iteration 88, loss = 4473.41897468\n",
      "Iteration 89, loss = 4473.50777105\n",
      "Iteration 90, loss = 4473.40224723\n",
      "Iteration 91, loss = 4473.35839317\n",
      "Iteration 92, loss = 4473.50823839\n",
      "Iteration 93, loss = 4473.43061881\n",
      "Iteration 94, loss = 4473.34472049\n",
      "Iteration 95, loss = 4473.39877994\n",
      "Iteration 96, loss = 4473.36692827\n",
      "Iteration 97, loss = 4473.45369211\n",
      "Iteration 98, loss = 4473.53112278\n",
      "Iteration 99, loss = 4473.44198763\n",
      "Iteration 100, loss = 4473.24322655\n",
      "Iteration 101, loss = 4473.23632433\n",
      "Iteration 102, loss = 4473.33113121\n",
      "Iteration 103, loss = 4473.11446674\n",
      "Iteration 104, loss = 4473.34691617\n",
      "Iteration 105, loss = 4473.36043785\n",
      "Iteration 106, loss = 4473.37888748\n",
      "Iteration 107, loss = 4473.35365819\n",
      "Iteration 108, loss = 4473.43889399\n",
      "Iteration 109, loss = 4473.37201853\n",
      "Iteration 110, loss = 4473.41754767\n",
      "Iteration 111, loss = 4473.35895254\n",
      "Iteration 112, loss = 4473.26811046\n",
      "Iteration 113, loss = 4473.26390094\n",
      "Iteration 114, loss = 4473.28233650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 115, loss = 4473.15344743\n",
      "Iteration 116, loss = 4473.10509298\n",
      "Iteration 117, loss = 4473.12272279\n",
      "Iteration 118, loss = 4473.12902353\n",
      "Iteration 119, loss = 4473.13558427\n",
      "Iteration 120, loss = 4473.10386833\n",
      "Iteration 121, loss = 4473.11047916\n",
      "Iteration 122, loss = 4473.13169912\n",
      "Iteration 123, loss = 4473.10276558\n",
      "Iteration 124, loss = 4473.12990182\n",
      "Iteration 125, loss = 4473.10438438\n",
      "Iteration 126, loss = 4473.12481548\n",
      "Iteration 127, loss = 4473.09940974\n",
      "Iteration 128, loss = 4473.09944938\n",
      "Iteration 129, loss = 4473.11933746\n",
      "Iteration 130, loss = 4473.08901561\n",
      "Iteration 131, loss = 4473.09944887\n",
      "Iteration 132, loss = 4473.08891395\n",
      "Iteration 133, loss = 4473.09025861\n",
      "Iteration 134, loss = 4473.09371868\n",
      "Iteration 135, loss = 4473.10154547\n",
      "Iteration 136, loss = 4473.08918805\n",
      "Iteration 137, loss = 4473.09550490\n",
      "Iteration 138, loss = 4473.12835635\n",
      "Iteration 139, loss = 4473.09868481\n",
      "Iteration 140, loss = 4473.09502487\n",
      "Iteration 141, loss = 4473.08871952\n",
      "Iteration 142, loss = 4473.09824233\n",
      "Iteration 143, loss = 4473.08838914\n",
      "Iteration 144, loss = 4473.10135319\n",
      "Iteration 145, loss = 4473.09513123\n",
      "Iteration 146, loss = 4473.11460382\n",
      "Iteration 147, loss = 4473.09361539\n",
      "Iteration 148, loss = 4473.09211510\n",
      "Iteration 149, loss = 4473.06789574\n",
      "Iteration 150, loss = 4473.06737839\n",
      "Iteration 151, loss = 4473.07063173\n",
      "Iteration 152, loss = 4473.06595614\n",
      "Iteration 153, loss = 4473.09097272\n",
      "Iteration 154, loss = 4473.07836374\n",
      "Iteration 155, loss = 4473.07696628\n",
      "Iteration 156, loss = 4473.09265976\n",
      "Iteration 157, loss = 4473.08626276\n",
      "Iteration 158, loss = 4473.07579099\n",
      "Iteration 159, loss = 4473.05560194\n",
      "Iteration 160, loss = 4473.06777622\n",
      "Iteration 161, loss = 4473.05466604\n",
      "Iteration 162, loss = 4473.06122871\n",
      "Iteration 163, loss = 4473.06752088\n",
      "Iteration 164, loss = 4473.06884526\n",
      "Iteration 165, loss = 4473.08169841\n",
      "Iteration 166, loss = 4473.04293946\n",
      "Iteration 167, loss = 4473.06598708\n",
      "Iteration 168, loss = 4473.06303301\n",
      "Iteration 169, loss = 4473.05545562\n",
      "Iteration 170, loss = 4473.06730943\n",
      "Iteration 171, loss = 4473.04817499\n",
      "Iteration 172, loss = 4473.04403127\n",
      "Iteration 173, loss = 4473.05733389\n",
      "Iteration 174, loss = 4473.06060616\n",
      "Iteration 175, loss = 4473.02136809\n",
      "Iteration 176, loss = 4473.03885381\n",
      "Iteration 177, loss = 4473.03950597\n",
      "Iteration 178, loss = 4473.04340148\n",
      "Iteration 179, loss = 4473.05234102\n",
      "Iteration 180, loss = 4473.03537195\n",
      "Iteration 181, loss = 4473.02240855\n",
      "Iteration 182, loss = 4473.01618894\n",
      "Iteration 183, loss = 4473.03270473\n",
      "Iteration 184, loss = 4473.04085668\n",
      "Iteration 185, loss = 4473.04412029\n",
      "Iteration 186, loss = 4473.03994514\n",
      "Iteration 187, loss = 4473.04043787\n",
      "Iteration 188, loss = 4473.03084022\n",
      "Iteration 189, loss = 4473.00719865\n",
      "Iteration 190, loss = 4473.01929046\n",
      "Iteration 191, loss = 4473.00804560\n",
      "Iteration 192, loss = 4473.03055614\n",
      "Iteration 193, loss = 4473.03253451\n",
      "Iteration 194, loss = 4473.02202698\n",
      "Iteration 195, loss = 4473.00803834\n",
      "Iteration 196, loss = 4473.03137771\n",
      "Iteration 197, loss = 4473.00192666\n",
      "Iteration 198, loss = 4473.02776506\n",
      "Iteration 199, loss = 4473.03574253\n",
      "Iteration 200, loss = 4473.02617457\n",
      "Iteration 1, loss = 9115.28607635\n",
      "Iteration 2, loss = 3861.80746768\n",
      "Iteration 3, loss = 3861.34765616\n",
      "Iteration 4, loss = 3861.53735272\n",
      "Iteration 5, loss = 3861.11099215\n",
      "Iteration 6, loss = 3862.20416147\n",
      "Iteration 7, loss = 3861.26927129\n",
      "Iteration 8, loss = 3860.86465057\n",
      "Iteration 9, loss = 3861.46126938\n",
      "Iteration 10, loss = 3860.94872865\n",
      "Iteration 11, loss = 3861.53149193\n",
      "Iteration 12, loss = 3860.77572493\n",
      "Iteration 13, loss = 3860.82910748\n",
      "Iteration 14, loss = 3861.24688809\n",
      "Iteration 15, loss = 3860.72922194\n",
      "Iteration 16, loss = 3861.51842575\n",
      "Iteration 17, loss = 3860.73737989\n",
      "Iteration 18, loss = 3860.99725831\n",
      "Iteration 19, loss = 3861.08272866\n",
      "Iteration 20, loss = 3860.91381897\n",
      "Iteration 21, loss = 3860.73546890\n",
      "Iteration 22, loss = 3860.90952116\n",
      "Iteration 23, loss = 3860.76147431\n",
      "Iteration 24, loss = 3860.96344229\n",
      "Iteration 25, loss = 3860.46724877\n",
      "Iteration 26, loss = 3861.09187962\n",
      "Iteration 27, loss = 3861.21463513\n",
      "Iteration 28, loss = 3861.05365466\n",
      "Iteration 29, loss = 3860.87501989\n",
      "Iteration 30, loss = 3860.83340308\n",
      "Iteration 31, loss = 3861.04244600\n",
      "Iteration 32, loss = 3860.62278637\n",
      "Iteration 33, loss = 3860.99276029\n",
      "Iteration 34, loss = 3861.37401524\n",
      "Iteration 35, loss = 3860.95749163\n",
      "Iteration 36, loss = 3860.45531912\n",
      "Iteration 37, loss = 3860.73613753\n",
      "Iteration 38, loss = 3860.48011144\n",
      "Iteration 39, loss = 3860.62714688\n",
      "Iteration 40, loss = 3860.21665107\n",
      "Iteration 41, loss = 3860.82953696\n",
      "Iteration 42, loss = 3860.62126325\n",
      "Iteration 43, loss = 3860.35427284\n",
      "Iteration 44, loss = 3860.58291501\n",
      "Iteration 45, loss = 3860.45271525\n",
      "Iteration 46, loss = 3860.58461644\n",
      "Iteration 47, loss = 3860.12430511\n",
      "Iteration 48, loss = 3860.25780241\n",
      "Iteration 49, loss = 3860.57860263\n",
      "Iteration 50, loss = 3860.29213115\n",
      "Iteration 51, loss = 3860.76924550\n",
      "Iteration 52, loss = 3860.54774498\n",
      "Iteration 53, loss = 3860.94854720\n",
      "Iteration 54, loss = 3860.25504434\n",
      "Iteration 55, loss = 3860.77732586\n",
      "Iteration 56, loss = 3860.42215518\n",
      "Iteration 57, loss = 3860.65037694\n",
      "Iteration 58, loss = 3860.44946328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 59, loss = 3859.71512832\n",
      "Iteration 60, loss = 3859.72142725\n",
      "Iteration 61, loss = 3859.84940786\n",
      "Iteration 62, loss = 3859.73451265\n",
      "Iteration 63, loss = 3859.61445901\n",
      "Iteration 64, loss = 3859.79806787\n",
      "Iteration 65, loss = 3859.62715723\n",
      "Iteration 66, loss = 3859.80610125\n",
      "Iteration 67, loss = 3859.69025554\n",
      "Iteration 68, loss = 3859.69914230\n",
      "Iteration 69, loss = 3859.73270769\n",
      "Iteration 70, loss = 3859.57038722\n",
      "Iteration 71, loss = 3859.76300899\n",
      "Iteration 72, loss = 3859.66566459\n",
      "Iteration 73, loss = 3859.70870538\n",
      "Iteration 74, loss = 3859.73432598\n",
      "Iteration 75, loss = 3859.75994058\n",
      "Iteration 76, loss = 3859.78823477\n",
      "Iteration 77, loss = 3859.81559429\n",
      "Iteration 78, loss = 3859.66546370\n",
      "Iteration 79, loss = 3859.60703040\n",
      "Iteration 80, loss = 3859.60105459\n",
      "Iteration 81, loss = 3859.65649666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 82, loss = 3859.79775845\n",
      "Iteration 83, loss = 3859.59849299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 84, loss = 3859.54804602\n",
      "Iteration 85, loss = 3859.53162062\n",
      "Iteration 86, loss = 3859.54750247\n",
      "Iteration 87, loss = 3859.54464644\n",
      "Iteration 88, loss = 3859.51827494\n",
      "Iteration 89, loss = 3859.51948463\n",
      "Iteration 90, loss = 3859.49027491\n",
      "Iteration 91, loss = 3859.50394357\n",
      "Iteration 92, loss = 3859.51199227\n",
      "Iteration 93, loss = 3859.51393497\n",
      "Iteration 94, loss = 3859.48455867\n",
      "Iteration 95, loss = 3859.50505446\n",
      "Iteration 96, loss = 3859.52459356\n",
      "Iteration 97, loss = 3859.53120827\n",
      "Iteration 98, loss = 3859.53183405\n",
      "Iteration 99, loss = 3859.50616059\n",
      "Iteration 100, loss = 3859.53502706\n",
      "Iteration 101, loss = 3859.50221425\n",
      "Iteration 102, loss = 3859.52702284\n",
      "Iteration 103, loss = 3859.49757787\n",
      "Iteration 104, loss = 3859.49321175\n",
      "Iteration 105, loss = 3859.49100157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 106, loss = 3859.46606760\n",
      "Iteration 107, loss = 3859.46414068\n",
      "Iteration 108, loss = 3859.46419018\n",
      "Iteration 109, loss = 3859.46760093\n",
      "Iteration 110, loss = 3859.46291124\n",
      "Iteration 111, loss = 3859.46180958\n",
      "Iteration 112, loss = 3859.46127857\n",
      "Iteration 113, loss = 3859.46118804\n",
      "Iteration 114, loss = 3859.46431799\n",
      "Iteration 115, loss = 3859.46145400\n",
      "Iteration 116, loss = 3859.46059937\n",
      "Iteration 117, loss = 3859.45800156\n",
      "Iteration 118, loss = 3859.46258172\n",
      "Iteration 119, loss = 3859.46335854\n",
      "Iteration 120, loss = 3859.45803716\n",
      "Iteration 121, loss = 3859.45857113\n",
      "Iteration 122, loss = 3859.45997552\n",
      "Iteration 123, loss = 3859.46264937\n",
      "Iteration 124, loss = 3859.46050866\n",
      "Iteration 125, loss = 3859.46629281\n",
      "Iteration 126, loss = 3859.45554239\n",
      "Iteration 127, loss = 3859.45864612\n",
      "Iteration 128, loss = 3859.45582156\n",
      "Iteration 129, loss = 3859.46113893\n",
      "Iteration 130, loss = 3859.45667132\n",
      "Iteration 131, loss = 3859.46030841\n",
      "Iteration 132, loss = 3859.45530514\n",
      "Iteration 133, loss = 3859.45855289\n",
      "Iteration 134, loss = 3859.45777006\n",
      "Iteration 135, loss = 3859.45433299\n",
      "Iteration 136, loss = 3859.45843837\n",
      "Iteration 137, loss = 3859.46293505\n",
      "Iteration 138, loss = 3859.46157300\n",
      "Iteration 139, loss = 3859.45690738\n",
      "Iteration 140, loss = 3859.45396339\n",
      "Iteration 141, loss = 3859.45498860\n",
      "Iteration 142, loss = 3859.46642457\n",
      "Iteration 143, loss = 3859.45670840\n",
      "Iteration 144, loss = 3859.46087994\n",
      "Iteration 145, loss = 3859.45334261\n",
      "Iteration 146, loss = 3859.46420395\n",
      "Iteration 147, loss = 3859.45717265\n",
      "Iteration 148, loss = 3859.45862614\n",
      "Iteration 149, loss = 3859.45577838\n",
      "Iteration 150, loss = 3859.45234978\n",
      "Iteration 151, loss = 3859.45606632\n",
      "Iteration 152, loss = 3859.46209814\n",
      "Iteration 153, loss = 3859.45932540\n",
      "Iteration 154, loss = 3859.45431824\n",
      "Iteration 155, loss = 3859.45871382\n",
      "Iteration 156, loss = 3859.45975442\n",
      "Iteration 157, loss = 3859.45708032\n",
      "Iteration 158, loss = 3859.45437195\n",
      "Iteration 159, loss = 3859.45125185\n",
      "Iteration 160, loss = 3859.45095674\n",
      "Iteration 161, loss = 3859.45407832\n",
      "Iteration 162, loss = 3859.45275531\n",
      "Iteration 163, loss = 3859.45509601\n",
      "Iteration 164, loss = 3859.45573872\n",
      "Iteration 165, loss = 3859.45617391\n",
      "Iteration 166, loss = 3859.45745675\n",
      "Iteration 167, loss = 3859.45868625\n",
      "Iteration 168, loss = 3859.45492838\n",
      "Iteration 169, loss = 3859.45487951\n",
      "Iteration 170, loss = 3859.45937937\n",
      "Iteration 171, loss = 3859.45440930\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 172, loss = 3859.44551102\n",
      "Iteration 173, loss = 3859.44420737\n",
      "Iteration 174, loss = 3859.44419089\n",
      "Iteration 175, loss = 3859.44434245\n",
      "Iteration 176, loss = 3859.44399010\n",
      "Iteration 177, loss = 3859.44470011\n",
      "Iteration 178, loss = 3859.44419028\n",
      "Iteration 179, loss = 3859.44481297\n",
      "Iteration 180, loss = 3859.44532400\n",
      "Iteration 181, loss = 3859.44492793\n",
      "Iteration 182, loss = 3859.44371910\n",
      "Iteration 183, loss = 3859.44417604\n",
      "Iteration 184, loss = 3859.44423985\n",
      "Iteration 185, loss = 3859.44419884\n",
      "Iteration 186, loss = 3859.44365261\n",
      "Iteration 187, loss = 3859.44437479\n",
      "Iteration 188, loss = 3859.44367500\n",
      "Iteration 189, loss = 3859.44287407\n",
      "Iteration 190, loss = 3859.44415276\n",
      "Iteration 191, loss = 3859.44382828\n",
      "Iteration 192, loss = 3859.44380616\n",
      "Iteration 193, loss = 3859.44345669\n",
      "Iteration 194, loss = 3859.44382037\n",
      "Iteration 195, loss = 3859.44383315\n",
      "Iteration 196, loss = 3859.44382794\n",
      "Iteration 197, loss = 3859.44311170\n",
      "Iteration 198, loss = 3859.44387516\n",
      "Iteration 199, loss = 3859.44394701\n",
      "Iteration 200, loss = 3859.44437392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 1, loss = 10143.29496425\n",
      "Iteration 2, loss = 3999.55826371\n",
      "Iteration 3, loss = 3999.59203929\n",
      "Iteration 4, loss = 3999.39764846\n",
      "Iteration 5, loss = 3999.14256753\n",
      "Iteration 6, loss = 3999.74611328\n",
      "Iteration 7, loss = 3999.52284726\n",
      "Iteration 8, loss = 3999.65457380\n",
      "Iteration 9, loss = 3999.69841600\n",
      "Iteration 10, loss = 3999.02969113\n",
      "Iteration 11, loss = 3999.19333282\n",
      "Iteration 12, loss = 3999.48378690\n",
      "Iteration 13, loss = 3999.48824456\n",
      "Iteration 14, loss = 3999.17453975\n",
      "Iteration 15, loss = 3999.81591163\n",
      "Iteration 16, loss = 3998.91874219\n",
      "Iteration 17, loss = 3999.17685400\n",
      "Iteration 18, loss = 3998.86333763\n",
      "Iteration 19, loss = 3999.17239776\n",
      "Iteration 20, loss = 3999.20171851\n",
      "Iteration 21, loss = 3999.13066610\n",
      "Iteration 22, loss = 3999.42812949\n",
      "Iteration 23, loss = 3999.01944952\n",
      "Iteration 24, loss = 3999.37725661\n",
      "Iteration 25, loss = 3999.37148230\n",
      "Iteration 26, loss = 3999.35448177\n",
      "Iteration 27, loss = 3998.59282906\n",
      "Iteration 28, loss = 3998.55300436\n",
      "Iteration 29, loss = 3999.29756172\n",
      "Iteration 30, loss = 3998.91258453\n",
      "Iteration 31, loss = 3999.36096335\n",
      "Iteration 32, loss = 3999.19289970\n",
      "Iteration 33, loss = 3998.57144183\n",
      "Iteration 34, loss = 3998.93180668\n",
      "Iteration 35, loss = 3999.16065483\n",
      "Iteration 36, loss = 3998.97651425\n",
      "Iteration 37, loss = 3998.82716955\n",
      "Iteration 38, loss = 3998.80192797\n",
      "Iteration 39, loss = 3999.17846524\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 40, loss = 3998.30818245\n",
      "Iteration 41, loss = 3998.09145982\n",
      "Iteration 42, loss = 3998.19366636\n",
      "Iteration 43, loss = 3998.20903083\n",
      "Iteration 44, loss = 3998.20945717\n",
      "Iteration 45, loss = 3998.23457053\n",
      "Iteration 46, loss = 3998.13494459\n",
      "Iteration 47, loss = 3998.19632667\n",
      "Iteration 48, loss = 3998.10472832\n",
      "Iteration 49, loss = 3998.08357840\n",
      "Iteration 50, loss = 3998.22540229\n",
      "Iteration 51, loss = 3998.03491999\n",
      "Iteration 52, loss = 3998.21074010\n",
      "Iteration 53, loss = 3998.09885051\n",
      "Iteration 54, loss = 3998.12112501\n",
      "Iteration 55, loss = 3998.18296640\n",
      "Iteration 56, loss = 3998.15711753\n",
      "Iteration 57, loss = 3998.07199139\n",
      "Iteration 58, loss = 3998.12167259\n",
      "Iteration 59, loss = 3998.12892512\n",
      "Iteration 60, loss = 3998.21004813\n",
      "Iteration 61, loss = 3998.00150117\n",
      "Iteration 62, loss = 3998.17943528\n",
      "Iteration 63, loss = 3998.10367218\n",
      "Iteration 64, loss = 3998.02794078\n",
      "Iteration 65, loss = 3998.11685077\n",
      "Iteration 66, loss = 3998.01292199\n",
      "Iteration 67, loss = 3998.04657619\n",
      "Iteration 68, loss = 3998.13917646\n",
      "Iteration 69, loss = 3998.07972688\n",
      "Iteration 70, loss = 3998.06814731\n",
      "Iteration 71, loss = 3997.94799932\n",
      "Iteration 72, loss = 3997.92111597\n",
      "Iteration 73, loss = 3997.87804081\n",
      "Iteration 74, loss = 3998.03613938\n",
      "Iteration 75, loss = 3998.14788939\n",
      "Iteration 76, loss = 3997.97532073\n",
      "Iteration 77, loss = 3998.07148956\n",
      "Iteration 78, loss = 3997.90699861\n",
      "Iteration 79, loss = 3997.96805823\n",
      "Iteration 80, loss = 3998.13131546\n",
      "Iteration 81, loss = 3998.06326179\n",
      "Iteration 82, loss = 3998.13354275\n",
      "Iteration 83, loss = 3998.12954767\n",
      "Iteration 84, loss = 3997.98094943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 85, loss = 3998.09844765\n",
      "Iteration 86, loss = 3997.88687818\n",
      "Iteration 87, loss = 3997.85006445\n",
      "Iteration 88, loss = 3997.83669714\n",
      "Iteration 89, loss = 3997.85667363\n",
      "Iteration 90, loss = 3997.83898074\n",
      "Iteration 91, loss = 3997.83423045\n",
      "Iteration 92, loss = 3997.84975141\n",
      "Iteration 93, loss = 3997.84289539\n",
      "Iteration 94, loss = 3997.82525542\n",
      "Iteration 95, loss = 3997.82242522\n",
      "Iteration 96, loss = 3997.82350864\n",
      "Iteration 97, loss = 3997.83459666\n",
      "Iteration 98, loss = 3997.81639768\n",
      "Iteration 99, loss = 3997.83277778\n",
      "Iteration 100, loss = 3997.82355097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 101, loss = 3997.83513160\n",
      "Iteration 102, loss = 3997.82852357\n",
      "Iteration 103, loss = 3997.85796943\n",
      "Iteration 104, loss = 3997.83541225\n",
      "Iteration 105, loss = 3997.84210268\n",
      "Iteration 106, loss = 3997.83507862\n",
      "Iteration 107, loss = 3997.85351804\n",
      "Iteration 108, loss = 3997.83835927\n",
      "Iteration 109, loss = 3997.84050673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 110, loss = 3997.79524175\n",
      "Iteration 111, loss = 3997.79034295\n",
      "Iteration 112, loss = 3997.79241891\n",
      "Iteration 113, loss = 3997.78652106\n",
      "Iteration 114, loss = 3997.78400176\n",
      "Iteration 115, loss = 3997.78530033\n",
      "Iteration 116, loss = 3997.78466098\n",
      "Iteration 117, loss = 3997.78416634\n",
      "Iteration 118, loss = 3997.78475962\n",
      "Iteration 119, loss = 3997.78515255\n",
      "Iteration 120, loss = 3997.78731957\n",
      "Iteration 121, loss = 3997.78178965\n",
      "Iteration 122, loss = 3997.78937669\n",
      "Iteration 123, loss = 3997.78387061\n",
      "Iteration 124, loss = 3997.78240569\n",
      "Iteration 125, loss = 3997.78191383\n",
      "Iteration 126, loss = 3997.78338655\n",
      "Iteration 127, loss = 3997.77953961\n",
      "Iteration 128, loss = 3997.78766790\n",
      "Iteration 129, loss = 3997.78573424\n",
      "Iteration 130, loss = 3997.78109656\n",
      "Iteration 131, loss = 3997.78720484\n",
      "Iteration 132, loss = 3997.78118715\n",
      "Iteration 133, loss = 3997.77838281\n",
      "Iteration 134, loss = 3997.78351009\n",
      "Iteration 135, loss = 3997.78303995\n",
      "Iteration 136, loss = 3997.78647311\n",
      "Iteration 137, loss = 3997.78220463\n",
      "Iteration 138, loss = 3997.77755579\n",
      "Iteration 139, loss = 3997.78124786\n",
      "Iteration 140, loss = 3997.77657942\n",
      "Iteration 141, loss = 3997.78486666\n",
      "Iteration 142, loss = 3997.78556489\n",
      "Iteration 143, loss = 3997.77785550\n",
      "Iteration 144, loss = 3997.78166262\n",
      "Iteration 145, loss = 3997.78390394\n",
      "Iteration 146, loss = 3997.78176940\n",
      "Iteration 147, loss = 3997.78085574\n",
      "Iteration 148, loss = 3997.78007358\n",
      "Iteration 149, loss = 3997.78081855\n",
      "Iteration 150, loss = 3997.78213113\n",
      "Iteration 151, loss = 3997.78125937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 152, loss = 3997.77016636\n",
      "Iteration 153, loss = 3997.77074830\n",
      "Iteration 154, loss = 3997.77020653\n",
      "Iteration 155, loss = 3997.77131136\n",
      "Iteration 156, loss = 3997.77056505\n",
      "Iteration 157, loss = 3997.77065704\n",
      "Iteration 158, loss = 3997.77013000\n",
      "Iteration 159, loss = 3997.77088371\n",
      "Iteration 160, loss = 3997.76984070\n",
      "Iteration 161, loss = 3997.76957477\n",
      "Iteration 162, loss = 3997.76981170\n",
      "Iteration 163, loss = 3997.77038381\n",
      "Iteration 164, loss = 3997.76973441\n",
      "Iteration 165, loss = 3997.77122621\n",
      "Iteration 166, loss = 3997.77046139\n",
      "Iteration 167, loss = 3997.76918160\n",
      "Iteration 168, loss = 3997.76928380\n",
      "Iteration 169, loss = 3997.77104443\n",
      "Iteration 170, loss = 3997.77038234\n",
      "Iteration 171, loss = 3997.77111195\n",
      "Iteration 172, loss = 3997.77117516\n",
      "Iteration 173, loss = 3997.76987756\n",
      "Iteration 174, loss = 3997.77049188\n",
      "Iteration 175, loss = 3997.76964617\n",
      "Iteration 176, loss = 3997.76930028\n",
      "Iteration 177, loss = 3997.76926055\n",
      "Iteration 178, loss = 3997.76983029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 179, loss = 3997.76772456\n",
      "Iteration 180, loss = 3997.76791741\n",
      "Iteration 181, loss = 3997.76774023\n",
      "Iteration 182, loss = 3997.76772999\n",
      "Iteration 183, loss = 3997.76784002\n",
      "Iteration 184, loss = 3997.76777012\n",
      "Iteration 185, loss = 3997.76782089\n",
      "Iteration 186, loss = 3997.76830615\n",
      "Iteration 187, loss = 3997.76771512\n",
      "Iteration 188, loss = 3997.76766922\n",
      "Iteration 189, loss = 3997.76780036\n",
      "Iteration 190, loss = 3997.76779588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 191, loss = 3997.76737933\n",
      "Iteration 192, loss = 3997.76738802\n",
      "Iteration 193, loss = 3997.76739075\n",
      "Iteration 194, loss = 3997.76742705\n",
      "Iteration 195, loss = 3997.76736412\n",
      "Iteration 196, loss = 3997.76741877\n",
      "Iteration 197, loss = 3997.76737687\n",
      "Iteration 198, loss = 3997.76737952\n",
      "Iteration 199, loss = 3997.76739506\n",
      "Iteration 200, loss = 3997.76743145\n",
      "Iteration 1, loss = 4580.76234027\n",
      "Iteration 2, loss = 1121.89571908\n",
      "Iteration 3, loss = 603.47532016\n",
      "Iteration 4, loss = 406.75973306\n",
      "Iteration 5, loss = 319.93459389\n",
      "Iteration 6, loss = 304.89710754\n",
      "Iteration 7, loss = 299.62537757\n",
      "Iteration 8, loss = 297.65448428\n",
      "Iteration 9, loss = 297.46749878\n",
      "Iteration 10, loss = 297.16003393\n",
      "Iteration 11, loss = 297.27261785\n",
      "Iteration 12, loss = 296.86761314\n",
      "Iteration 13, loss = 296.67976115\n",
      "Iteration 14, loss = 297.08345733\n",
      "Iteration 15, loss = 296.85576312\n",
      "Iteration 16, loss = 296.70376953\n",
      "Iteration 17, loss = 296.57657794\n",
      "Iteration 18, loss = 297.38336728\n",
      "Iteration 19, loss = 296.87167453\n",
      "Iteration 20, loss = 297.00743033\n",
      "Iteration 21, loss = 296.71810828\n",
      "Iteration 22, loss = 297.45664822\n",
      "Iteration 23, loss = 296.59566643\n",
      "Iteration 24, loss = 296.48148475\n",
      "Iteration 25, loss = 296.89469289\n",
      "Iteration 26, loss = 296.68048393\n",
      "Iteration 27, loss = 296.90401873\n",
      "Iteration 28, loss = 296.99489354\n",
      "Iteration 29, loss = 296.29440613\n",
      "Iteration 30, loss = 295.97861672\n",
      "Iteration 31, loss = 296.15976842\n",
      "Iteration 32, loss = 296.26165210\n",
      "Iteration 33, loss = 296.64049768\n",
      "Iteration 34, loss = 296.37423094\n",
      "Iteration 35, loss = 296.82116588\n",
      "Iteration 36, loss = 296.23496776\n",
      "Iteration 37, loss = 296.26813775\n",
      "Iteration 38, loss = 295.79291677\n",
      "Iteration 39, loss = 295.84812222\n",
      "Iteration 40, loss = 296.26484710\n",
      "Iteration 41, loss = 296.30822208\n",
      "Iteration 42, loss = 296.31623815\n",
      "Iteration 43, loss = 295.89969763\n",
      "Iteration 44, loss = 296.62592996\n",
      "Iteration 45, loss = 296.98145814\n",
      "Iteration 46, loss = 296.44375286\n",
      "Iteration 47, loss = 297.22106500\n",
      "Iteration 48, loss = 296.83643655\n",
      "Iteration 49, loss = 296.60327860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4558.04685862\n",
      "Iteration 2, loss = 1110.92863892\n",
      "Iteration 3, loss = 613.77950704\n",
      "Iteration 4, loss = 433.11987126\n",
      "Iteration 5, loss = 336.15333569\n",
      "Iteration 6, loss = 306.98570377\n",
      "Iteration 7, loss = 301.45335032\n",
      "Iteration 8, loss = 299.14528826\n",
      "Iteration 9, loss = 299.25900184\n",
      "Iteration 10, loss = 298.03347861\n",
      "Iteration 11, loss = 298.50827172\n",
      "Iteration 12, loss = 296.00860592\n",
      "Iteration 13, loss = 293.87401915\n",
      "Iteration 14, loss = 292.85350798\n",
      "Iteration 15, loss = 293.04351330\n",
      "Iteration 16, loss = 292.43326337\n",
      "Iteration 17, loss = 292.88926194\n",
      "Iteration 18, loss = 292.76714219\n",
      "Iteration 19, loss = 292.60168256\n",
      "Iteration 20, loss = 292.57788052\n",
      "Iteration 21, loss = 292.40372887\n",
      "Iteration 22, loss = 292.64906364\n",
      "Iteration 23, loss = 292.15731296\n",
      "Iteration 24, loss = 292.14382137\n",
      "Iteration 25, loss = 292.82141667\n",
      "Iteration 26, loss = 292.60686328\n",
      "Iteration 27, loss = 293.33504075\n",
      "Iteration 28, loss = 292.31990073\n",
      "Iteration 29, loss = 292.43023331\n",
      "Iteration 30, loss = 292.13651389\n",
      "Iteration 31, loss = 292.69683395\n",
      "Iteration 32, loss = 292.24749588\n",
      "Iteration 33, loss = 292.41021832\n",
      "Iteration 34, loss = 291.61578345\n",
      "Iteration 35, loss = 292.62333208\n",
      "Iteration 36, loss = 292.01029724\n",
      "Iteration 37, loss = 291.69999405\n",
      "Iteration 38, loss = 292.36300126\n",
      "Iteration 39, loss = 292.36779651\n",
      "Iteration 40, loss = 292.27603570\n",
      "Iteration 41, loss = 292.19190746\n",
      "Iteration 42, loss = 292.28458265\n",
      "Iteration 43, loss = 292.55741295\n",
      "Iteration 44, loss = 292.70169150\n",
      "Iteration 45, loss = 292.44814437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4560.07279640\n",
      "Iteration 2, loss = 1103.86113740\n",
      "Iteration 3, loss = 609.62922133\n",
      "Iteration 4, loss = 432.24270214\n",
      "Iteration 5, loss = 339.57907973\n",
      "Iteration 6, loss = 305.20590421\n",
      "Iteration 7, loss = 299.51798334\n",
      "Iteration 8, loss = 297.76695495\n",
      "Iteration 9, loss = 297.11717680\n",
      "Iteration 10, loss = 296.76944383\n",
      "Iteration 11, loss = 296.82218812\n",
      "Iteration 12, loss = 296.25423615\n",
      "Iteration 13, loss = 292.49173700\n",
      "Iteration 14, loss = 291.79711352\n",
      "Iteration 15, loss = 292.04057936\n",
      "Iteration 16, loss = 291.10346399\n",
      "Iteration 17, loss = 291.79579323\n",
      "Iteration 18, loss = 291.58183460\n",
      "Iteration 19, loss = 291.35576084\n",
      "Iteration 20, loss = 291.27919914\n",
      "Iteration 21, loss = 290.91058346\n",
      "Iteration 22, loss = 290.63524808\n",
      "Iteration 23, loss = 290.65906914\n",
      "Iteration 24, loss = 290.64402444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 291.11549368\n",
      "Iteration 26, loss = 291.33044102\n",
      "Iteration 27, loss = 291.44797625\n",
      "Iteration 28, loss = 290.46904987\n",
      "Iteration 29, loss = 290.92871509\n",
      "Iteration 30, loss = 291.01678012\n",
      "Iteration 31, loss = 291.59365611\n",
      "Iteration 32, loss = 290.48427655\n",
      "Iteration 33, loss = 291.56820547\n",
      "Iteration 34, loss = 290.47806177\n",
      "Iteration 35, loss = 291.84375058\n",
      "Iteration 36, loss = 290.81988464\n",
      "Iteration 37, loss = 290.28823193\n",
      "Iteration 38, loss = 290.80589611\n",
      "Iteration 39, loss = 290.88434189\n",
      "Iteration 40, loss = 290.77583986\n",
      "Iteration 41, loss = 291.05581772\n",
      "Iteration 42, loss = 290.53142750\n",
      "Iteration 43, loss = 291.69295702\n",
      "Iteration 44, loss = 290.92240976\n",
      "Iteration 45, loss = 290.66768987\n",
      "Iteration 46, loss = 290.45548897\n",
      "Iteration 47, loss = 291.05562678\n",
      "Iteration 48, loss = 291.26153340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4564.10581904\n",
      "Iteration 2, loss = 1090.09916583\n",
      "Iteration 3, loss = 600.84548721\n",
      "Iteration 4, loss = 432.45193347\n",
      "Iteration 5, loss = 353.14425469\n",
      "Iteration 6, loss = 309.50737076\n",
      "Iteration 7, loss = 302.19380453\n",
      "Iteration 8, loss = 300.54002233\n",
      "Iteration 9, loss = 299.88613650\n",
      "Iteration 10, loss = 300.27267672\n",
      "Iteration 11, loss = 297.23779167\n",
      "Iteration 12, loss = 295.03038218\n",
      "Iteration 13, loss = 293.91286535\n",
      "Iteration 14, loss = 294.93533983\n",
      "Iteration 15, loss = 294.47889792\n",
      "Iteration 16, loss = 294.35747088\n",
      "Iteration 17, loss = 293.78858576\n",
      "Iteration 18, loss = 293.57534579\n",
      "Iteration 19, loss = 293.18991864\n",
      "Iteration 20, loss = 293.75444373\n",
      "Iteration 21, loss = 293.65363367\n",
      "Iteration 22, loss = 293.20715886\n",
      "Iteration 23, loss = 292.84614992\n",
      "Iteration 24, loss = 293.25664040\n",
      "Iteration 25, loss = 293.36653937\n",
      "Iteration 26, loss = 294.75547469\n",
      "Iteration 27, loss = 293.56629913\n",
      "Iteration 28, loss = 293.61469307\n",
      "Iteration 29, loss = 293.29056008\n",
      "Iteration 30, loss = 293.27227637\n",
      "Iteration 31, loss = 294.05204756\n",
      "Iteration 32, loss = 293.14555725\n",
      "Iteration 33, loss = 294.01116082\n",
      "Iteration 34, loss = 293.34964840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4574.88279390\n",
      "Iteration 2, loss = 1089.06340149\n",
      "Iteration 3, loss = 598.48816903\n",
      "Iteration 4, loss = 426.48764207\n",
      "Iteration 5, loss = 348.90978382\n",
      "Iteration 6, loss = 307.40572291\n",
      "Iteration 7, loss = 300.74019141\n",
      "Iteration 8, loss = 298.70425167\n",
      "Iteration 9, loss = 297.54530367\n",
      "Iteration 10, loss = 296.61716752\n",
      "Iteration 11, loss = 292.53148927\n",
      "Iteration 12, loss = 292.43505581\n",
      "Iteration 13, loss = 292.90932717\n",
      "Iteration 14, loss = 292.63969633\n",
      "Iteration 15, loss = 292.58402027\n",
      "Iteration 16, loss = 293.18265704\n",
      "Iteration 17, loss = 292.38956632\n",
      "Iteration 18, loss = 292.10957447\n",
      "Iteration 19, loss = 293.11415629\n",
      "Iteration 20, loss = 291.92626216\n",
      "Iteration 21, loss = 291.93601005\n",
      "Iteration 22, loss = 292.01570276\n",
      "Iteration 23, loss = 293.38931024\n",
      "Iteration 24, loss = 291.79829130\n",
      "Iteration 25, loss = 292.09287439\n",
      "Iteration 26, loss = 292.48170755\n",
      "Iteration 27, loss = 292.37763371\n",
      "Iteration 28, loss = 292.20769798\n",
      "Iteration 29, loss = 291.55653970\n",
      "Iteration 30, loss = 292.14047527\n",
      "Iteration 31, loss = 292.23551819\n",
      "Iteration 32, loss = 292.13382146\n",
      "Iteration 33, loss = 292.33234723\n",
      "Iteration 34, loss = 291.81975312\n",
      "Iteration 35, loss = 291.84216822\n",
      "Iteration 36, loss = 292.40546204\n",
      "Iteration 37, loss = 292.20924520\n",
      "Iteration 38, loss = 292.18880185\n",
      "Iteration 39, loss = 291.64404836\n",
      "Iteration 40, loss = 291.81810718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2434999.63605303\n",
      "Iteration 2, loss = 2540329.88766517\n",
      "Iteration 3, loss = 2505785.55562900\n",
      "Iteration 4, loss = 2471711.19876928\n",
      "Iteration 5, loss = 2438100.92572263\n",
      "Iteration 6, loss = 2404948.63698119\n",
      "Iteration 7, loss = 2372247.17479903\n",
      "Iteration 8, loss = 2339991.51321354\n",
      "Iteration 9, loss = 2308174.75432826\n",
      "Iteration 10, loss = 2276791.23197140\n",
      "Iteration 11, loss = 2245835.37139638\n",
      "Iteration 12, loss = 2215301.26864619\n",
      "Iteration 13, loss = 2185181.44331181\n",
      "Iteration 14, loss = 2155472.75441018\n",
      "Iteration 15, loss = 2126168.66576187\n",
      "Iteration 16, loss = 2097263.18236235\n",
      "Iteration 17, loss = 2068752.00815043\n",
      "Iteration 18, loss = 2040627.85451506\n",
      "Iteration 19, loss = 2012887.61722681\n",
      "Iteration 20, loss = 1985524.82370747\n",
      "Iteration 21, loss = 1958534.27589149\n",
      "Iteration 22, loss = 1931911.14675591\n",
      "Iteration 23, loss = 1905651.34878336\n",
      "Iteration 24, loss = 1879747.62925886\n",
      "Iteration 25, loss = 1854198.57978171\n",
      "Iteration 26, loss = 1828995.85560119\n",
      "Iteration 27, loss = 1804137.77245169\n",
      "Iteration 28, loss = 1779616.20704075\n",
      "Iteration 29, loss = 1755430.27836586\n",
      "Iteration 30, loss = 1731572.88197939\n",
      "Iteration 31, loss = 1708039.94380776\n",
      "Iteration 32, loss = 1684827.86005528\n",
      "Iteration 33, loss = 1661931.40365294\n",
      "Iteration 34, loss = 1639347.42476443\n",
      "Iteration 35, loss = 1617070.74283097\n",
      "Iteration 36, loss = 1595097.26414071\n",
      "Iteration 37, loss = 1573423.00833757\n",
      "Iteration 38, loss = 1552043.49820249\n",
      "Iteration 39, loss = 1530956.16927114\n",
      "Iteration 40, loss = 1510154.67567001\n",
      "Iteration 41, loss = 1489637.23875864\n",
      "Iteration 42, loss = 1469399.33331223\n",
      "Iteration 43, loss = 1449436.20652276\n",
      "Iteration 44, loss = 1429745.06769587\n",
      "Iteration 45, loss = 1410322.07914133\n",
      "Iteration 46, loss = 1391163.92507269\n",
      "Iteration 47, loss = 1372266.55974840\n",
      "Iteration 48, loss = 1353625.67990834\n",
      "Iteration 49, loss = 1335239.15219695\n",
      "Iteration 50, loss = 1317102.89711873\n",
      "Iteration 51, loss = 1299214.35003557\n",
      "Iteration 52, loss = 1281568.73078134\n",
      "Iteration 53, loss = 1264163.52996449\n",
      "Iteration 54, loss = 1246994.87829383\n",
      "Iteration 55, loss = 1230060.55968027\n",
      "Iteration 56, loss = 1213355.88621836\n",
      "Iteration 57, loss = 1196880.07078932\n",
      "Iteration 58, loss = 1180627.72298902\n",
      "Iteration 59, loss = 1164597.35047158\n",
      "Iteration 60, loss = 1148784.05587741\n",
      "Iteration 61, loss = 1133186.78917097\n",
      "Iteration 62, loss = 1117801.37336677\n",
      "Iteration 63, loss = 1102626.09371627\n",
      "Iteration 64, loss = 1087657.43074007\n",
      "Iteration 65, loss = 1072892.16062620\n",
      "Iteration 66, loss = 1058328.45837141\n",
      "Iteration 67, loss = 1043962.45123491\n",
      "Iteration 68, loss = 1029792.38223631\n",
      "Iteration 69, loss = 1015815.24189128\n",
      "Iteration 70, loss = 1002028.45417373\n",
      "Iteration 71, loss = 988429.25703524\n",
      "Iteration 72, loss = 975015.00644146\n",
      "Iteration 73, loss = 961784.06245389\n",
      "Iteration 74, loss = 948732.74747986\n",
      "Iteration 75, loss = 935859.39773870\n",
      "Iteration 76, loss = 923161.14557877\n",
      "Iteration 77, loss = 910635.60842110\n",
      "Iteration 78, loss = 898281.13301457\n",
      "Iteration 79, loss = 886094.25026295\n",
      "Iteration 80, loss = 874073.67304777\n",
      "Iteration 81, loss = 862217.31382064\n",
      "Iteration 82, loss = 850521.00120524\n",
      "Iteration 83, loss = 838985.19964748\n",
      "Iteration 84, loss = 827605.94407962\n",
      "Iteration 85, loss = 816382.35783211\n",
      "Iteration 86, loss = 805310.61759628\n",
      "Iteration 87, loss = 794389.59824344\n",
      "Iteration 88, loss = 783617.78151103\n",
      "Iteration 89, loss = 772992.32265212\n",
      "Iteration 90, loss = 762512.18556946\n",
      "Iteration 91, loss = 752174.00170182\n",
      "Iteration 92, loss = 741976.56513652\n",
      "Iteration 93, loss = 731918.72364961\n",
      "Iteration 94, loss = 721997.02385374\n",
      "Iteration 95, loss = 712211.16091357\n",
      "Iteration 96, loss = 702557.70112999\n",
      "Iteration 97, loss = 693036.47011208\n",
      "Iteration 98, loss = 683644.34686068\n",
      "Iteration 99, loss = 674379.89822256\n",
      "Iteration 100, loss = 665242.12424346\n",
      "Iteration 101, loss = 656228.89371795\n",
      "Iteration 102, loss = 647338.17784933\n",
      "Iteration 103, loss = 638568.25800203\n",
      "Iteration 104, loss = 629918.14601560\n",
      "Iteration 105, loss = 621385.93465593\n",
      "Iteration 106, loss = 612969.01795557\n",
      "Iteration 107, loss = 604667.49796517\n",
      "Iteration 108, loss = 596478.45325244\n",
      "Iteration 109, loss = 588401.25420268\n",
      "Iteration 110, loss = 580434.62339089\n",
      "Iteration 111, loss = 572575.49883402\n",
      "Iteration 112, loss = 564823.58881692\n",
      "Iteration 113, loss = 557177.17221389\n",
      "Iteration 114, loss = 549635.05797037\n",
      "Iteration 115, loss = 542195.68375837\n",
      "Iteration 116, loss = 534857.38326316\n",
      "Iteration 117, loss = 527619.49006358\n",
      "Iteration 118, loss = 520479.82503297\n",
      "Iteration 119, loss = 513437.73948352\n",
      "Iteration 120, loss = 506490.30449317\n",
      "Iteration 121, loss = 499639.08825508\n",
      "Iteration 122, loss = 492880.19450104\n",
      "Iteration 123, loss = 486213.23563607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 124, loss = 479637.96930696\n",
      "Iteration 125, loss = 473151.25050928\n",
      "Iteration 126, loss = 466753.64284820\n",
      "Iteration 127, loss = 460442.27626783\n",
      "Iteration 128, loss = 454217.07891033\n",
      "Iteration 129, loss = 448077.31940702\n",
      "Iteration 130, loss = 442020.29524373\n",
      "Iteration 131, loss = 436046.24476781\n",
      "Iteration 132, loss = 430153.42361171\n",
      "Iteration 133, loss = 424340.63096025\n",
      "Iteration 134, loss = 418608.20969323\n",
      "Iteration 135, loss = 412952.16802002\n",
      "Iteration 136, loss = 407373.98926936\n",
      "Iteration 137, loss = 401871.50142005\n",
      "Iteration 138, loss = 396443.68647908\n",
      "Iteration 139, loss = 391090.26979426\n",
      "Iteration 140, loss = 385809.47579612\n",
      "Iteration 141, loss = 380601.07237265\n",
      "Iteration 142, loss = 375462.60838095\n",
      "Iteration 143, loss = 370394.92509565\n",
      "Iteration 144, loss = 365396.15250153\n",
      "Iteration 145, loss = 360465.45630771\n",
      "Iteration 146, loss = 355602.01237775\n",
      "Iteration 147, loss = 350804.04628581\n",
      "Iteration 148, loss = 346071.98824010\n",
      "Iteration 149, loss = 341404.31858101\n",
      "Iteration 150, loss = 336800.08627401\n",
      "Iteration 151, loss = 332258.86525630\n",
      "Iteration 152, loss = 327778.08643702\n",
      "Iteration 153, loss = 323360.22380155\n",
      "Iteration 154, loss = 319001.81995740\n",
      "Iteration 155, loss = 314702.91951414\n",
      "Iteration 156, loss = 310461.70628308\n",
      "Iteration 157, loss = 306278.89871687\n",
      "Iteration 158, loss = 302153.30527350\n",
      "Iteration 159, loss = 298083.78204492\n",
      "Iteration 160, loss = 294069.50542491\n",
      "Iteration 161, loss = 290109.58015425\n",
      "Iteration 162, loss = 286204.04386790\n",
      "Iteration 163, loss = 282351.27263735\n",
      "Iteration 164, loss = 278551.08039911\n",
      "Iteration 165, loss = 274803.11095359\n",
      "Iteration 166, loss = 271104.97933868\n",
      "Iteration 167, loss = 267458.39568511\n",
      "Iteration 168, loss = 263861.21881331\n",
      "Iteration 169, loss = 260312.62649631\n",
      "Iteration 170, loss = 256812.67166368\n",
      "Iteration 171, loss = 253360.28015551\n",
      "Iteration 172, loss = 249954.79649479\n",
      "Iteration 173, loss = 246596.06182892\n",
      "Iteration 174, loss = 243282.53826174\n",
      "Iteration 175, loss = 240014.52726242\n",
      "Iteration 176, loss = 236790.65877796\n",
      "Iteration 177, loss = 233611.47275347\n",
      "Iteration 178, loss = 230474.72943041\n",
      "Iteration 179, loss = 227380.56732097\n",
      "Iteration 180, loss = 224329.61018163\n",
      "Iteration 181, loss = 221319.25524131\n",
      "Iteration 182, loss = 218349.63010801\n",
      "Iteration 183, loss = 215421.58923934\n",
      "Iteration 184, loss = 212532.37300735\n",
      "Iteration 185, loss = 209682.98318553\n",
      "Iteration 186, loss = 206872.61870511\n",
      "Iteration 187, loss = 204099.70745861\n",
      "Iteration 188, loss = 201365.28461451\n",
      "Iteration 189, loss = 198668.01089465\n",
      "Iteration 190, loss = 196006.91935140\n",
      "Iteration 191, loss = 193382.48853406\n",
      "Iteration 192, loss = 190794.77862595\n",
      "Iteration 193, loss = 188240.65582298\n",
      "Iteration 194, loss = 185721.62030135\n",
      "Iteration 195, loss = 183236.84374630\n",
      "Iteration 196, loss = 180787.95720373\n",
      "Iteration 197, loss = 178369.29968205\n",
      "Iteration 198, loss = 175985.19746174\n",
      "Iteration 199, loss = 173633.19773596\n",
      "Iteration 200, loss = 171313.30595084\n",
      "Iteration 1, loss = 641720.11546188\n",
      "Iteration 2, loss = 665152.35617728\n",
      "Iteration 3, loss = 656139.90110193\n",
      "Iteration 4, loss = 647249.83411605\n",
      "Iteration 5, loss = 638481.08585029\n",
      "Iteration 6, loss = 629832.21013985\n",
      "Iteration 7, loss = 621300.26387326\n",
      "Iteration 8, loss = 612884.98405677\n",
      "Iteration 9, loss = 604583.90234045\n",
      "Iteration 10, loss = 596396.23061860\n",
      "Iteration 11, loss = 588320.22205287\n",
      "Iteration 12, loss = 580353.31536920\n",
      "Iteration 13, loss = 572495.43982339\n",
      "Iteration 14, loss = 564744.47996329\n",
      "Iteration 15, loss = 557099.23645937\n",
      "Iteration 16, loss = 549558.01151250\n",
      "Iteration 17, loss = 542119.47961921\n",
      "Iteration 18, loss = 534781.67583216\n",
      "Iteration 19, loss = 527544.81452977\n",
      "Iteration 20, loss = 520406.08832672\n",
      "Iteration 21, loss = 513363.82069094\n",
      "Iteration 22, loss = 506418.08144770\n",
      "Iteration 23, loss = 499567.32784459\n",
      "Iteration 24, loss = 492808.86405659\n",
      "Iteration 25, loss = 486143.19046844\n",
      "Iteration 26, loss = 479568.04466919\n",
      "Iteration 27, loss = 473082.37777171\n",
      "Iteration 28, loss = 466685.03901207\n",
      "Iteration 29, loss = 460374.93752436\n",
      "Iteration 30, loss = 454150.30263450\n",
      "Iteration 31, loss = 448010.97980791\n",
      "Iteration 32, loss = 441955.09752051\n",
      "Iteration 33, loss = 435981.57875811\n",
      "Iteration 34, loss = 430089.06583590\n",
      "Iteration 35, loss = 424277.23987239\n",
      "Iteration 36, loss = 418544.74656886\n",
      "Iteration 37, loss = 412890.21389984\n",
      "Iteration 38, loss = 407312.19687287\n",
      "Iteration 39, loss = 401810.48029542\n",
      "Iteration 40, loss = 396383.20312179\n",
      "Iteration 41, loss = 391030.42572048\n",
      "Iteration 42, loss = 385750.66631230\n",
      "Iteration 43, loss = 380542.37371051\n",
      "Iteration 44, loss = 375404.81984824\n",
      "Iteration 45, loss = 370337.25188064\n",
      "Iteration 46, loss = 365339.29579363\n",
      "Iteration 47, loss = 360408.75673659\n",
      "Iteration 48, loss = 355545.31310426\n",
      "Iteration 49, loss = 350748.40024914\n",
      "Iteration 50, loss = 346016.52182303\n",
      "Iteration 51, loss = 341349.88206619\n",
      "Iteration 52, loss = 336745.99523446\n",
      "Iteration 53, loss = 332206.01778494\n",
      "Iteration 54, loss = 327725.63373807\n",
      "Iteration 55, loss = 323307.87288424\n",
      "Iteration 56, loss = 318949.56873757\n",
      "Iteration 57, loss = 314651.08271747\n",
      "Iteration 58, loss = 310411.17197063\n",
      "Iteration 59, loss = 306228.92687813\n",
      "Iteration 60, loss = 302103.20024064\n",
      "Iteration 61, loss = 298034.06433968\n",
      "Iteration 62, loss = 294019.79964473\n",
      "Iteration 63, loss = 290060.99211972\n",
      "Iteration 64, loss = 286155.30507760\n",
      "Iteration 65, loss = 282303.11961763\n",
      "Iteration 66, loss = 278503.48971382\n",
      "Iteration 67, loss = 274755.25705649\n",
      "Iteration 68, loss = 271059.05550270\n",
      "Iteration 69, loss = 267412.31816364\n",
      "Iteration 70, loss = 263814.86346013\n",
      "Iteration 71, loss = 260267.60852340\n",
      "Iteration 72, loss = 256767.55133558\n",
      "Iteration 73, loss = 253315.37062286\n",
      "Iteration 74, loss = 249910.68410121\n",
      "Iteration 75, loss = 246552.27912093\n",
      "Iteration 76, loss = 243239.06279962\n",
      "Iteration 77, loss = 239971.11530837\n",
      "Iteration 78, loss = 236747.63274181\n",
      "Iteration 79, loss = 233568.33585923\n",
      "Iteration 80, loss = 230431.67160195\n",
      "Iteration 81, loss = 227339.14006541\n",
      "Iteration 82, loss = 224287.35099347\n",
      "Iteration 83, loss = 221277.81449815\n",
      "Iteration 84, loss = 218309.02427791\n",
      "Iteration 85, loss = 215380.76409447\n",
      "Iteration 86, loss = 212492.39684295\n",
      "Iteration 87, loss = 209643.00600979\n",
      "Iteration 88, loss = 206832.15754234\n",
      "Iteration 89, loss = 204060.74446204\n",
      "Iteration 90, loss = 201326.01365711\n",
      "Iteration 91, loss = 198629.26335195\n",
      "Iteration 92, loss = 195968.26342784\n",
      "Iteration 93, loss = 193344.38527008\n",
      "Iteration 94, loss = 190756.01985862\n",
      "Iteration 95, loss = 188203.06229531\n",
      "Iteration 96, loss = 185684.26434355\n",
      "Iteration 97, loss = 183200.47556480\n",
      "Iteration 98, loss = 180750.08549936\n",
      "Iteration 99, loss = 178332.30123560\n",
      "Iteration 100, loss = 175949.05444457\n",
      "Iteration 101, loss = 173597.22304758\n",
      "Iteration 102, loss = 171277.74739935\n",
      "Iteration 103, loss = 168990.09920084\n",
      "Iteration 104, loss = 166733.12973137\n",
      "Iteration 105, loss = 164506.83502997\n",
      "Iteration 106, loss = 162311.06641892\n",
      "Iteration 107, loss = 160145.07232400\n",
      "Iteration 108, loss = 158008.78374100\n",
      "Iteration 109, loss = 155901.57214988\n",
      "Iteration 110, loss = 153823.26157161\n",
      "Iteration 111, loss = 151772.50269947\n",
      "Iteration 112, loss = 149749.77568022\n",
      "Iteration 113, loss = 147755.10073910\n",
      "Iteration 114, loss = 145787.46573343\n",
      "Iteration 115, loss = 143846.21855941\n",
      "Iteration 116, loss = 141931.81220421\n",
      "Iteration 117, loss = 140043.29628489\n",
      "Iteration 118, loss = 138180.99941290\n",
      "Iteration 119, loss = 136343.37109956\n",
      "Iteration 120, loss = 134530.53069570\n",
      "Iteration 121, loss = 132743.92126168\n",
      "Iteration 122, loss = 130980.27593507\n",
      "Iteration 123, loss = 129240.83404035\n",
      "Iteration 124, loss = 127525.51535048\n",
      "Iteration 125, loss = 125833.06222287\n",
      "Iteration 126, loss = 124163.52091616\n",
      "Iteration 127, loss = 122517.18588462\n",
      "Iteration 128, loss = 120893.24661858\n",
      "Iteration 129, loss = 119291.27410215\n",
      "Iteration 130, loss = 117710.95560430\n",
      "Iteration 131, loss = 116152.29537266\n",
      "Iteration 132, loss = 114614.93049300\n",
      "Iteration 133, loss = 113098.05970429\n",
      "Iteration 134, loss = 111603.35748465\n",
      "Iteration 135, loss = 110127.49765249\n",
      "Iteration 136, loss = 108671.83439399\n",
      "Iteration 137, loss = 107236.52567242\n",
      "Iteration 138, loss = 105820.11268663\n",
      "Iteration 139, loss = 104423.50088253\n",
      "Iteration 140, loss = 103045.85482969\n",
      "Iteration 141, loss = 101686.66858740\n",
      "Iteration 142, loss = 100346.85813380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 143, loss = 99024.16667169\n",
      "Iteration 144, loss = 97719.97299795\n",
      "Iteration 145, loss = 96433.72020377\n",
      "Iteration 146, loss = 95165.31581602\n",
      "Iteration 147, loss = 93913.06257462\n",
      "Iteration 148, loss = 92678.22148225\n",
      "Iteration 149, loss = 91460.43270075\n",
      "Iteration 150, loss = 90259.39254608\n",
      "Iteration 151, loss = 89074.99162826\n",
      "Iteration 152, loss = 87905.49766215\n",
      "Iteration 153, loss = 86753.16496520\n",
      "Iteration 154, loss = 85615.97465468\n",
      "Iteration 155, loss = 84494.31704078\n",
      "Iteration 156, loss = 83387.57859257\n",
      "Iteration 157, loss = 82296.34200429\n",
      "Iteration 158, loss = 81219.84643893\n",
      "Iteration 159, loss = 80158.53785681\n",
      "Iteration 160, loss = 79111.33843827\n",
      "Iteration 161, loss = 78078.00913448\n",
      "Iteration 162, loss = 77059.14689546\n",
      "Iteration 163, loss = 76054.03157747\n",
      "Iteration 164, loss = 75062.45966282\n",
      "Iteration 165, loss = 74084.90931644\n",
      "Iteration 166, loss = 73119.58486956\n",
      "Iteration 167, loss = 72168.32417171\n",
      "Iteration 168, loss = 71230.07478716\n",
      "Iteration 169, loss = 70304.07529115\n",
      "Iteration 170, loss = 69391.18924596\n",
      "Iteration 171, loss = 68490.18556389\n",
      "Iteration 172, loss = 67602.05667732\n",
      "Iteration 173, loss = 66725.54151817\n",
      "Iteration 174, loss = 65861.27155706\n",
      "Iteration 175, loss = 65008.23640419\n",
      "Iteration 176, loss = 64167.33667773\n",
      "Iteration 177, loss = 63338.28466617\n",
      "Iteration 178, loss = 62519.38996891\n",
      "Iteration 179, loss = 61712.27149032\n",
      "Iteration 180, loss = 60915.99790655\n",
      "Iteration 181, loss = 60130.99258797\n",
      "Iteration 182, loss = 59356.15579983\n",
      "Iteration 183, loss = 58592.38836405\n",
      "Iteration 184, loss = 57838.53115811\n",
      "Iteration 185, loss = 57095.22979358\n",
      "Iteration 186, loss = 56362.16071142\n",
      "Iteration 187, loss = 55638.03482908\n",
      "Iteration 188, loss = 54924.75483416\n",
      "Iteration 189, loss = 54221.32313661\n",
      "Iteration 190, loss = 53526.90263455\n",
      "Iteration 191, loss = 52842.43357934\n",
      "Iteration 192, loss = 52167.57491030\n",
      "Iteration 193, loss = 51500.79581362\n",
      "Iteration 194, loss = 50843.73138112\n",
      "Iteration 195, loss = 50195.09676421\n",
      "Iteration 196, loss = 49557.36916392\n",
      "Iteration 197, loss = 48925.51364736\n",
      "Iteration 198, loss = 48303.31140632\n",
      "Iteration 199, loss = 47689.72894714\n",
      "Iteration 200, loss = 47084.88227748\n",
      "Iteration 1, loss = 586227.93740085\n",
      "Iteration 2, loss = 607829.25394329\n",
      "Iteration 3, loss = 599596.61721887\n",
      "Iteration 4, loss = 591476.72217971\n",
      "Iteration 5, loss = 583467.46732788\n",
      "Iteration 6, loss = 575567.86095187\n",
      "Iteration 7, loss = 567774.64500836\n",
      "Iteration 8, loss = 560087.80584782\n",
      "Iteration 9, loss = 552506.18918879\n",
      "Iteration 10, loss = 545027.39113551\n",
      "Iteration 11, loss = 537651.03647537\n",
      "Iteration 12, loss = 530373.63374020\n",
      "Iteration 13, loss = 523196.43280097\n",
      "Iteration 14, loss = 516117.07067762\n",
      "Iteration 15, loss = 509133.55952493\n",
      "Iteration 16, loss = 502246.18136729\n",
      "Iteration 17, loss = 495451.48011422\n",
      "Iteration 18, loss = 488749.47512247\n",
      "Iteration 19, loss = 482139.56183826\n",
      "Iteration 20, loss = 475618.48790039\n",
      "Iteration 21, loss = 469186.50065315\n",
      "Iteration 22, loss = 462842.20436367\n",
      "Iteration 23, loss = 456584.33644126\n",
      "Iteration 24, loss = 450411.40238624\n",
      "Iteration 25, loss = 444323.12134512\n",
      "Iteration 26, loss = 438317.73344298\n",
      "Iteration 27, loss = 432393.96885259\n",
      "Iteration 28, loss = 426550.67499430\n",
      "Iteration 29, loss = 420787.02451519\n",
      "Iteration 30, loss = 415101.36084645\n",
      "Iteration 31, loss = 409493.82977267\n",
      "Iteration 32, loss = 403962.16818442\n",
      "Iteration 33, loss = 398506.32397855\n",
      "Iteration 34, loss = 393124.62709984\n",
      "Iteration 35, loss = 387815.46654175\n",
      "Iteration 36, loss = 382579.52288167\n",
      "Iteration 37, loss = 377414.75749854\n",
      "Iteration 38, loss = 372320.07168534\n",
      "Iteration 39, loss = 367294.38666621\n",
      "Iteration 40, loss = 362337.41943952\n",
      "Iteration 41, loss = 357448.41673374\n",
      "Iteration 42, loss = 352625.62043224\n",
      "Iteration 43, loss = 347868.10764705\n",
      "Iteration 44, loss = 343175.96765440\n",
      "Iteration 45, loss = 338547.28721633\n",
      "Iteration 46, loss = 333982.36856122\n",
      "Iteration 47, loss = 329478.68586512\n",
      "Iteration 48, loss = 325036.28015967\n",
      "Iteration 49, loss = 320655.35519128\n",
      "Iteration 50, loss = 316332.69948261\n",
      "Iteration 51, loss = 312070.46016043\n",
      "Iteration 52, loss = 307865.55709954\n",
      "Iteration 53, loss = 303718.65246479\n",
      "Iteration 54, loss = 299626.08363098\n",
      "Iteration 55, loss = 295590.99780554\n",
      "Iteration 56, loss = 291610.31924039\n",
      "Iteration 57, loss = 287684.12902594\n",
      "Iteration 58, loss = 283811.25763315\n",
      "Iteration 59, loss = 279991.12740389\n",
      "Iteration 60, loss = 276223.15963060\n",
      "Iteration 61, loss = 272506.30345776\n",
      "Iteration 62, loss = 268839.75555767\n",
      "Iteration 63, loss = 265223.57945748\n",
      "Iteration 64, loss = 261656.44969075\n",
      "Iteration 65, loss = 258137.60379768\n",
      "Iteration 66, loss = 254667.20883359\n",
      "Iteration 67, loss = 251243.74630411\n",
      "Iteration 68, loss = 247867.36465392\n",
      "Iteration 69, loss = 244536.21421865\n",
      "Iteration 70, loss = 241250.56132876\n",
      "Iteration 71, loss = 238010.33729153\n",
      "Iteration 72, loss = 234813.81470958\n",
      "Iteration 73, loss = 231660.70068521\n",
      "Iteration 74, loss = 228550.57713455\n",
      "Iteration 75, loss = 225483.13405009\n",
      "Iteration 76, loss = 222457.16796328\n",
      "Iteration 77, loss = 219472.00395313\n",
      "Iteration 78, loss = 216527.93908650\n",
      "Iteration 79, loss = 213623.78639797\n",
      "Iteration 80, loss = 210759.29295537\n",
      "Iteration 81, loss = 207934.11155922\n",
      "Iteration 82, loss = 205146.41700780\n",
      "Iteration 83, loss = 202397.49976929\n",
      "Iteration 84, loss = 199686.16151741\n",
      "Iteration 85, loss = 197011.24198260\n",
      "Iteration 86, loss = 194373.87071550\n",
      "Iteration 87, loss = 191770.29226703\n",
      "Iteration 88, loss = 189203.15666084\n",
      "Iteration 89, loss = 186671.49211593\n",
      "Iteration 90, loss = 184174.45939802\n",
      "Iteration 91, loss = 181710.61192130\n",
      "Iteration 92, loss = 179280.75592423\n",
      "Iteration 93, loss = 176883.75349217\n",
      "Iteration 94, loss = 174519.55567414\n",
      "Iteration 95, loss = 172187.38153258\n",
      "Iteration 96, loss = 169887.10290491\n",
      "Iteration 97, loss = 167618.16442784\n",
      "Iteration 98, loss = 165380.01309831\n",
      "Iteration 99, loss = 163171.65669475\n",
      "Iteration 100, loss = 160994.52216144\n",
      "Iteration 101, loss = 158846.46586737\n",
      "Iteration 102, loss = 156728.09049097\n",
      "Iteration 103, loss = 154637.66346054\n",
      "Iteration 104, loss = 152576.76329535\n",
      "Iteration 105, loss = 150543.56851141\n",
      "Iteration 106, loss = 148538.25955630\n",
      "Iteration 107, loss = 146559.57133198\n",
      "Iteration 108, loss = 144608.73905188\n",
      "Iteration 109, loss = 142683.38574868\n",
      "Iteration 110, loss = 140785.29917391\n",
      "Iteration 111, loss = 138912.15831327\n",
      "Iteration 112, loss = 137064.75594270\n",
      "Iteration 113, loss = 135242.60985993\n",
      "Iteration 114, loss = 133445.55255155\n",
      "Iteration 115, loss = 131672.48667049\n",
      "Iteration 116, loss = 129923.78179454\n",
      "Iteration 117, loss = 128199.05428639\n",
      "Iteration 118, loss = 126498.04111339\n",
      "Iteration 119, loss = 124819.39075082\n",
      "Iteration 120, loss = 123163.63845851\n",
      "Iteration 121, loss = 121531.73526357\n",
      "Iteration 122, loss = 119920.70298748\n",
      "Iteration 123, loss = 118331.90501119\n",
      "Iteration 124, loss = 116764.98049706\n",
      "Iteration 125, loss = 115219.18019587\n",
      "Iteration 126, loss = 113694.17303649\n",
      "Iteration 127, loss = 112190.72741185\n",
      "Iteration 128, loss = 110707.52464904\n",
      "Iteration 129, loss = 109244.30464812\n",
      "Iteration 130, loss = 107800.93542654\n",
      "Iteration 131, loss = 106377.18009449\n",
      "Iteration 132, loss = 104972.62467960\n",
      "Iteration 133, loss = 103587.43713644\n",
      "Iteration 134, loss = 102221.56274278\n",
      "Iteration 135, loss = 100874.26439321\n",
      "Iteration 136, loss = 99544.47727272\n",
      "Iteration 137, loss = 98233.34470918\n",
      "Iteration 138, loss = 96940.23965062\n",
      "Iteration 139, loss = 95664.52732033\n",
      "Iteration 140, loss = 94405.79178564\n",
      "Iteration 141, loss = 93164.83025296\n",
      "Iteration 142, loss = 91940.68138090\n",
      "Iteration 143, loss = 90732.85417378\n",
      "Iteration 144, loss = 89540.93231678\n",
      "Iteration 145, loss = 88366.30905258\n",
      "Iteration 146, loss = 87207.20841251\n",
      "Iteration 147, loss = 86063.87270031\n",
      "Iteration 148, loss = 84936.37297119\n",
      "Iteration 149, loss = 83824.18123506\n",
      "Iteration 150, loss = 82726.82706041\n",
      "Iteration 151, loss = 81644.93233419\n",
      "Iteration 152, loss = 80576.47599485\n",
      "Iteration 153, loss = 79524.23981373\n",
      "Iteration 154, loss = 78485.30495117\n",
      "Iteration 155, loss = 77460.62896531\n",
      "Iteration 156, loss = 76449.50991604\n",
      "Iteration 157, loss = 75453.35060714\n",
      "Iteration 158, loss = 74470.34480335\n",
      "Iteration 159, loss = 73500.57948233\n",
      "Iteration 160, loss = 72544.08026328\n",
      "Iteration 161, loss = 71600.77766441\n",
      "Iteration 162, loss = 70669.99999683\n",
      "Iteration 163, loss = 69751.61014994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 164, loss = 68846.14229419\n",
      "Iteration 165, loss = 67953.04393124\n",
      "Iteration 166, loss = 67071.55152098\n",
      "Iteration 167, loss = 66202.31683174\n",
      "Iteration 168, loss = 65345.78173967\n",
      "Iteration 169, loss = 64499.93624662\n",
      "Iteration 170, loss = 63665.64999783\n",
      "Iteration 171, loss = 62843.53986336\n",
      "Iteration 172, loss = 62031.57506136\n",
      "Iteration 173, loss = 61231.21129737\n",
      "Iteration 174, loss = 60441.97753077\n",
      "Iteration 175, loss = 59662.48641414\n",
      "Iteration 176, loss = 58895.02463787\n",
      "Iteration 177, loss = 58137.04121279\n",
      "Iteration 178, loss = 57389.12090579\n",
      "Iteration 179, loss = 56652.25122425\n",
      "Iteration 180, loss = 55924.49227138\n",
      "Iteration 181, loss = 55207.54566334\n",
      "Iteration 182, loss = 54499.92676923\n",
      "Iteration 183, loss = 53802.31771975\n",
      "Iteration 184, loss = 53113.89478939\n",
      "Iteration 185, loss = 52434.91506967\n",
      "Iteration 186, loss = 51765.38169225\n",
      "Iteration 187, loss = 51103.75297410\n",
      "Iteration 188, loss = 50452.70332605\n",
      "Iteration 189, loss = 49809.73405086\n",
      "Iteration 190, loss = 49175.84736517\n",
      "Iteration 191, loss = 48550.15255295\n",
      "Iteration 192, loss = 47933.77274345\n",
      "Iteration 193, loss = 47324.86429430\n",
      "Iteration 194, loss = 46724.53758553\n",
      "Iteration 195, loss = 46132.86294598\n",
      "Iteration 196, loss = 45549.18475725\n",
      "Iteration 197, loss = 44972.82823525\n",
      "Iteration 198, loss = 44404.56154340\n",
      "Iteration 199, loss = 43844.15227875\n",
      "Iteration 200, loss = 43291.35025886\n",
      "Iteration 1, loss = 287717.73837622\n",
      "Iteration 2, loss = 296967.63661319\n",
      "Iteration 3, loss = 292968.14756393\n",
      "Iteration 4, loss = 289023.75849811\n",
      "Iteration 5, loss = 285132.46246007\n",
      "Iteration 6, loss = 281295.66374104\n",
      "Iteration 7, loss = 277509.09341307\n",
      "Iteration 8, loss = 273774.59828820\n",
      "Iteration 9, loss = 270091.94750751\n",
      "Iteration 10, loss = 266458.33839896\n",
      "Iteration 11, loss = 262875.29257898\n",
      "Iteration 12, loss = 259339.70266379\n",
      "Iteration 13, loss = 255853.05195462\n",
      "Iteration 14, loss = 252414.24036753\n",
      "Iteration 15, loss = 249021.32164811\n",
      "Iteration 16, loss = 245675.90050497\n",
      "Iteration 17, loss = 242374.47089194\n",
      "Iteration 18, loss = 239119.02361979\n",
      "Iteration 19, loss = 235907.73143980\n",
      "Iteration 20, loss = 232739.91079540\n",
      "Iteration 21, loss = 229615.21125995\n",
      "Iteration 22, loss = 226533.40741633\n",
      "Iteration 23, loss = 223493.24549927\n",
      "Iteration 24, loss = 220494.82624613\n",
      "Iteration 25, loss = 217536.53788372\n",
      "Iteration 26, loss = 214619.64347495\n",
      "Iteration 27, loss = 211741.97196651\n",
      "Iteration 28, loss = 208903.20061046\n",
      "Iteration 29, loss = 206103.06196586\n",
      "Iteration 30, loss = 203341.18444669\n",
      "Iteration 31, loss = 200617.16260193\n",
      "Iteration 32, loss = 197929.60504438\n",
      "Iteration 33, loss = 195279.42505693\n",
      "Iteration 34, loss = 192665.34611051\n",
      "Iteration 35, loss = 190086.06775218\n",
      "Iteration 36, loss = 187541.81740892\n",
      "Iteration 37, loss = 185032.98561756\n",
      "Iteration 38, loss = 182557.78094947\n",
      "Iteration 39, loss = 180116.67813861\n",
      "Iteration 40, loss = 177708.25775385\n",
      "Iteration 41, loss = 175333.64811727\n",
      "Iteration 42, loss = 172990.55825915\n",
      "Iteration 43, loss = 170679.31027245\n",
      "Iteration 44, loss = 168400.02415225\n",
      "Iteration 45, loss = 166151.41698397\n",
      "Iteration 46, loss = 163933.68710400\n",
      "Iteration 47, loss = 161745.56334651\n",
      "Iteration 48, loss = 159587.82055615\n",
      "Iteration 49, loss = 157459.64664917\n",
      "Iteration 50, loss = 155359.84699524\n",
      "Iteration 51, loss = 153289.39784330\n",
      "Iteration 52, loss = 151246.44775696\n",
      "Iteration 53, loss = 149231.93371796\n",
      "Iteration 54, loss = 147243.76041020\n",
      "Iteration 55, loss = 145283.86437132\n",
      "Iteration 56, loss = 143349.78389915\n",
      "Iteration 57, loss = 141442.61638971\n",
      "Iteration 58, loss = 139560.99081519\n",
      "Iteration 59, loss = 137705.06617498\n",
      "Iteration 60, loss = 135874.69822880\n",
      "Iteration 61, loss = 134069.56412799\n",
      "Iteration 62, loss = 132287.68071354\n",
      "Iteration 63, loss = 130531.08946080\n",
      "Iteration 64, loss = 128798.12855577\n",
      "Iteration 65, loss = 127088.74990328\n",
      "Iteration 66, loss = 125403.10301807\n",
      "Iteration 67, loss = 123739.70245868\n",
      "Iteration 68, loss = 122099.60494057\n",
      "Iteration 69, loss = 120481.37431260\n",
      "Iteration 70, loss = 118884.90863550\n",
      "Iteration 71, loss = 117310.96805622\n",
      "Iteration 72, loss = 115758.19416252\n",
      "Iteration 73, loss = 114226.51157092\n",
      "Iteration 74, loss = 112715.56831562\n",
      "Iteration 75, loss = 111225.82199028\n",
      "Iteration 76, loss = 109755.65421457\n",
      "Iteration 77, loss = 108304.99091922\n",
      "Iteration 78, loss = 106874.60367842\n",
      "Iteration 79, loss = 105464.19582673\n",
      "Iteration 80, loss = 104072.80576861\n",
      "Iteration 81, loss = 102699.94192270\n",
      "Iteration 82, loss = 101345.91986875\n",
      "Iteration 83, loss = 100010.41144950\n",
      "Iteration 84, loss = 98693.39501495\n",
      "Iteration 85, loss = 97393.79077358\n",
      "Iteration 86, loss = 96112.81482713\n",
      "Iteration 87, loss = 94847.73971862\n",
      "Iteration 88, loss = 93600.91766497\n",
      "Iteration 89, loss = 92370.83876135\n",
      "Iteration 90, loss = 91157.51961863\n",
      "Iteration 91, loss = 89960.61676783\n",
      "Iteration 92, loss = 88780.60746420\n",
      "Iteration 93, loss = 87615.94183798\n",
      "Iteration 94, loss = 86467.06098857\n",
      "Iteration 95, loss = 85334.63360377\n",
      "Iteration 96, loss = 84217.38742594\n",
      "Iteration 97, loss = 83114.54676419\n",
      "Iteration 98, loss = 82028.40398756\n",
      "Iteration 99, loss = 80954.95202827\n",
      "Iteration 100, loss = 79897.47332082\n",
      "Iteration 101, loss = 78853.39537087\n",
      "Iteration 102, loss = 77824.36684428\n",
      "Iteration 103, loss = 76809.26437177\n",
      "Iteration 104, loss = 75807.73014497\n",
      "Iteration 105, loss = 74820.17839261\n",
      "Iteration 106, loss = 73845.93827108\n",
      "Iteration 107, loss = 72884.39275692\n",
      "Iteration 108, loss = 71936.68972602\n",
      "Iteration 109, loss = 71001.56834382\n",
      "Iteration 110, loss = 70079.78005156\n",
      "Iteration 111, loss = 69169.40837289\n",
      "Iteration 112, loss = 68272.22885299\n",
      "Iteration 113, loss = 67387.20974721\n",
      "Iteration 114, loss = 66514.19555678\n",
      "Iteration 115, loss = 65652.61962958\n",
      "Iteration 116, loss = 64803.44552214\n",
      "Iteration 117, loss = 63965.24444504\n",
      "Iteration 118, loss = 63138.87311574\n",
      "Iteration 119, loss = 62323.01166537\n",
      "Iteration 120, loss = 61518.87180651\n",
      "Iteration 121, loss = 60726.54228373\n",
      "Iteration 122, loss = 59943.87380164\n",
      "Iteration 123, loss = 59171.72175444\n",
      "Iteration 124, loss = 58410.65130047\n",
      "Iteration 125, loss = 57658.97724922\n",
      "Iteration 126, loss = 56919.11535478\n",
      "Iteration 127, loss = 56188.08809782\n",
      "Iteration 128, loss = 55467.90842552\n",
      "Iteration 129, loss = 54756.80527286\n",
      "Iteration 130, loss = 54056.29453113\n",
      "Iteration 131, loss = 53364.09631586\n",
      "Iteration 132, loss = 52682.13746934\n",
      "Iteration 133, loss = 52009.02141766\n",
      "Iteration 134, loss = 51345.37727206\n",
      "Iteration 135, loss = 50690.65027718\n",
      "Iteration 136, loss = 50044.88574680\n",
      "Iteration 137, loss = 49407.94396560\n",
      "Iteration 138, loss = 48780.06066386\n",
      "Iteration 139, loss = 48160.19235414\n",
      "Iteration 140, loss = 47548.54177330\n",
      "Iteration 141, loss = 46945.63251579\n",
      "Iteration 142, loss = 46350.98173437\n",
      "Iteration 143, loss = 45764.50478454\n",
      "Iteration 144, loss = 45185.26890285\n",
      "Iteration 145, loss = 44614.59627645\n",
      "Iteration 146, loss = 44051.14825744\n",
      "Iteration 147, loss = 43495.91936521\n",
      "Iteration 148, loss = 42948.53843390\n",
      "Iteration 149, loss = 42408.11128149\n",
      "Iteration 150, loss = 41874.87273325\n",
      "Iteration 151, loss = 41349.39959191\n",
      "Iteration 152, loss = 40829.97095817\n",
      "Iteration 153, loss = 40319.01845650\n",
      "Iteration 154, loss = 39814.48824194\n",
      "Iteration 155, loss = 39316.65717410\n",
      "Iteration 156, loss = 38825.75124592\n",
      "Iteration 157, loss = 38341.49342038\n",
      "Iteration 158, loss = 37864.45462209\n",
      "Iteration 159, loss = 37392.93287360\n",
      "Iteration 160, loss = 36928.48727800\n",
      "Iteration 161, loss = 36470.07224006\n",
      "Iteration 162, loss = 36017.74179986\n",
      "Iteration 163, loss = 35571.68806426\n",
      "Iteration 164, loss = 35131.57755590\n",
      "Iteration 165, loss = 34698.15320465\n",
      "Iteration 166, loss = 34269.21420304\n",
      "Iteration 167, loss = 33847.23532114\n",
      "Iteration 168, loss = 33430.82222065\n",
      "Iteration 169, loss = 33020.31966768\n",
      "Iteration 170, loss = 32615.69919538\n",
      "Iteration 171, loss = 32216.13032466\n",
      "Iteration 172, loss = 31820.99890805\n",
      "Iteration 173, loss = 31432.81512970\n",
      "Iteration 174, loss = 31049.28310920\n",
      "Iteration 175, loss = 30670.53822295\n",
      "Iteration 176, loss = 30297.58340852\n",
      "Iteration 177, loss = 29929.32610659\n",
      "Iteration 178, loss = 29566.19902246\n",
      "Iteration 179, loss = 29207.91481620\n",
      "Iteration 180, loss = 28854.27729271\n",
      "Iteration 181, loss = 28506.48718603\n",
      "Iteration 182, loss = 28162.49945665\n",
      "Iteration 183, loss = 27823.72506756\n",
      "Iteration 184, loss = 27489.29067885\n",
      "Iteration 185, loss = 27159.43969794\n",
      "Iteration 186, loss = 26833.97861577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 187, loss = 26512.79566467\n",
      "Iteration 188, loss = 26196.50514228\n",
      "Iteration 189, loss = 25884.05306781\n",
      "Iteration 190, loss = 25576.19397685\n",
      "Iteration 191, loss = 25271.88476975\n",
      "Iteration 192, loss = 24972.52976380\n",
      "Iteration 193, loss = 24677.03097719\n",
      "Iteration 194, loss = 24385.24698369\n",
      "Iteration 195, loss = 24097.98082926\n",
      "Iteration 196, loss = 23813.93895646\n",
      "Iteration 197, loss = 23534.37846183\n",
      "Iteration 198, loss = 23258.23982694\n",
      "Iteration 199, loss = 22985.93666973\n",
      "Iteration 200, loss = 22717.64224097\n",
      "Iteration 1, loss = 354316.59520397\n",
      "Iteration 2, loss = 366092.99342377\n",
      "Iteration 3, loss = 361155.05164670\n",
      "Iteration 4, loss = 356284.08556005\n",
      "Iteration 5, loss = 351479.34802648\n",
      "Iteration 6, loss = 346740.85619657\n",
      "Iteration 7, loss = 342066.03474377\n",
      "Iteration 8, loss = 337455.18769524\n",
      "Iteration 9, loss = 332907.00634308\n",
      "Iteration 10, loss = 328420.01195710\n",
      "Iteration 11, loss = 323994.90704669\n",
      "Iteration 12, loss = 319630.15509844\n",
      "Iteration 13, loss = 315324.52365326\n",
      "Iteration 14, loss = 311077.17204989\n",
      "Iteration 15, loss = 306888.57603554\n",
      "Iteration 16, loss = 302755.45535373\n",
      "Iteration 17, loss = 298679.72789437\n",
      "Iteration 18, loss = 294658.90137545\n",
      "Iteration 19, loss = 290693.41504543\n",
      "Iteration 20, loss = 286781.62190683\n",
      "Iteration 21, loss = 282922.96677903\n",
      "Iteration 22, loss = 279117.19400102\n",
      "Iteration 23, loss = 275362.51422444\n",
      "Iteration 24, loss = 271659.69511711\n",
      "Iteration 25, loss = 268006.91124218\n",
      "Iteration 26, loss = 264403.82905010\n",
      "Iteration 27, loss = 260849.03854502\n",
      "Iteration 28, loss = 257343.33884506\n",
      "Iteration 29, loss = 253886.13422466\n",
      "Iteration 30, loss = 250474.86144650\n",
      "Iteration 31, loss = 247110.84291741\n",
      "Iteration 32, loss = 243791.99707182\n",
      "Iteration 33, loss = 240517.86378900\n",
      "Iteration 34, loss = 237289.26359792\n",
      "Iteration 35, loss = 234104.47683682\n",
      "Iteration 36, loss = 230962.62396313\n",
      "Iteration 37, loss = 227863.56283033\n",
      "Iteration 38, loss = 224806.80084325\n",
      "Iteration 39, loss = 221792.04169465\n",
      "Iteration 40, loss = 218817.63660338\n",
      "Iteration 41, loss = 215883.73262722\n",
      "Iteration 42, loss = 212990.27481652\n",
      "Iteration 43, loss = 210136.32302724\n",
      "Iteration 44, loss = 207319.91659756\n",
      "Iteration 45, loss = 204543.82954151\n",
      "Iteration 46, loss = 201803.90707055\n",
      "Iteration 47, loss = 199101.86931104\n",
      "Iteration 48, loss = 196436.46987942\n",
      "Iteration 49, loss = 193807.41397689\n",
      "Iteration 50, loss = 191214.36825442\n",
      "Iteration 51, loss = 188655.81956174\n",
      "Iteration 52, loss = 186133.44501110\n",
      "Iteration 53, loss = 183644.26596949\n",
      "Iteration 54, loss = 181189.12049605\n",
      "Iteration 55, loss = 178767.96533901\n",
      "Iteration 56, loss = 176378.91493068\n",
      "Iteration 57, loss = 174022.33984234\n",
      "Iteration 58, loss = 171699.22547656\n",
      "Iteration 59, loss = 169406.17857834\n",
      "Iteration 60, loss = 167145.60115978\n",
      "Iteration 61, loss = 164914.72976035\n",
      "Iteration 62, loss = 162715.10188379\n",
      "Iteration 63, loss = 160545.14685705\n",
      "Iteration 64, loss = 158404.24732876\n",
      "Iteration 65, loss = 156292.79782921\n",
      "Iteration 66, loss = 154209.99324591\n",
      "Iteration 67, loss = 152155.67971840\n",
      "Iteration 68, loss = 150129.39841924\n",
      "Iteration 69, loss = 148130.59299555\n",
      "Iteration 70, loss = 146159.06922665\n",
      "Iteration 71, loss = 144213.88011418\n",
      "Iteration 72, loss = 142295.55612906\n",
      "Iteration 73, loss = 140403.16867377\n",
      "Iteration 74, loss = 138536.86927907\n",
      "Iteration 75, loss = 136696.09254219\n",
      "Iteration 76, loss = 134879.84135627\n",
      "Iteration 77, loss = 133088.57311276\n",
      "Iteration 78, loss = 131321.68574741\n",
      "Iteration 79, loss = 129578.69021776\n",
      "Iteration 80, loss = 127859.65277952\n",
      "Iteration 81, loss = 126163.89123440\n",
      "Iteration 82, loss = 124491.23999876\n",
      "Iteration 83, loss = 122841.41425308\n",
      "Iteration 84, loss = 121214.53024211\n",
      "Iteration 85, loss = 119608.78693862\n",
      "Iteration 86, loss = 118025.21788713\n",
      "Iteration 87, loss = 116463.51933775\n",
      "Iteration 88, loss = 114922.51681279\n",
      "Iteration 89, loss = 113402.59539167\n",
      "Iteration 90, loss = 111904.05164837\n",
      "Iteration 91, loss = 110425.37944490\n",
      "Iteration 92, loss = 108967.06692785\n",
      "Iteration 93, loss = 107528.48265388\n",
      "Iteration 94, loss = 106109.03624444\n",
      "Iteration 95, loss = 104709.31843094\n",
      "Iteration 96, loss = 103328.67353561\n",
      "Iteration 97, loss = 101966.56417720\n",
      "Iteration 98, loss = 100623.24099605\n",
      "Iteration 99, loss = 99298.69948504\n",
      "Iteration 100, loss = 97991.13974764\n",
      "Iteration 101, loss = 96702.36159687\n",
      "Iteration 102, loss = 95430.47827861\n",
      "Iteration 103, loss = 94176.59666115\n",
      "Iteration 104, loss = 92939.07319527\n",
      "Iteration 105, loss = 91718.13900199\n",
      "Iteration 106, loss = 90514.14497022\n",
      "Iteration 107, loss = 89326.66131509\n",
      "Iteration 108, loss = 88155.51765443\n",
      "Iteration 109, loss = 87000.09351454\n",
      "Iteration 110, loss = 85860.76432822\n",
      "Iteration 111, loss = 84736.45424679\n",
      "Iteration 112, loss = 83627.65005710\n",
      "Iteration 113, loss = 82533.65986405\n",
      "Iteration 114, loss = 81454.77551708\n",
      "Iteration 115, loss = 80390.81805598\n",
      "Iteration 116, loss = 79340.76703873\n",
      "Iteration 117, loss = 78305.36011274\n",
      "Iteration 118, loss = 77284.00673094\n",
      "Iteration 119, loss = 76276.66842222\n",
      "Iteration 120, loss = 75282.78299179\n",
      "Iteration 121, loss = 74302.59146826\n",
      "Iteration 122, loss = 73335.85342971\n",
      "Iteration 123, loss = 72382.18673445\n",
      "Iteration 124, loss = 71441.34514650\n",
      "Iteration 125, loss = 70513.32204881\n",
      "Iteration 126, loss = 69598.04152920\n",
      "Iteration 127, loss = 68695.38883291\n",
      "Iteration 128, loss = 67804.75777171\n",
      "Iteration 129, loss = 66926.77617917\n",
      "Iteration 130, loss = 66059.73044994\n",
      "Iteration 131, loss = 65205.17160236\n",
      "Iteration 132, loss = 64362.39863292\n",
      "Iteration 133, loss = 63530.50660518\n",
      "Iteration 134, loss = 62710.23655970\n",
      "Iteration 135, loss = 61901.60516609\n",
      "Iteration 136, loss = 61103.82555075\n",
      "Iteration 137, loss = 60316.05691073\n",
      "Iteration 138, loss = 59539.40542438\n",
      "Iteration 139, loss = 58773.78268648\n",
      "Iteration 140, loss = 58017.86566053\n",
      "Iteration 141, loss = 57273.25671102\n",
      "Iteration 142, loss = 56537.58542670\n",
      "Iteration 143, loss = 55812.55644494\n",
      "Iteration 144, loss = 55096.92594422\n",
      "Iteration 145, loss = 54391.97384141\n",
      "Iteration 146, loss = 53695.78136494\n",
      "Iteration 147, loss = 53009.67992973\n",
      "Iteration 148, loss = 52332.31858908\n",
      "Iteration 149, loss = 51664.81950110\n",
      "Iteration 150, loss = 51005.59601985\n",
      "Iteration 151, loss = 50355.81603695\n",
      "Iteration 152, loss = 49714.77383330\n",
      "Iteration 153, loss = 49082.66874591\n",
      "Iteration 154, loss = 48459.04929610\n",
      "Iteration 155, loss = 47843.44611017\n",
      "Iteration 156, loss = 47237.62474087\n",
      "Iteration 157, loss = 46638.85923288\n",
      "Iteration 158, loss = 46048.34712457\n",
      "Iteration 159, loss = 45465.49546045\n",
      "Iteration 160, loss = 44891.64512781\n",
      "Iteration 161, loss = 44325.00801167\n",
      "Iteration 162, loss = 43766.43233553\n",
      "Iteration 163, loss = 43214.58472197\n",
      "Iteration 164, loss = 42671.42903142\n",
      "Iteration 165, loss = 42134.57687821\n",
      "Iteration 166, loss = 41605.87550535\n",
      "Iteration 167, loss = 41083.71648524\n",
      "Iteration 168, loss = 40568.96815232\n",
      "Iteration 169, loss = 40061.08835383\n",
      "Iteration 170, loss = 39560.13056596\n",
      "Iteration 171, loss = 39066.50647799\n",
      "Iteration 172, loss = 38579.76613159\n",
      "Iteration 173, loss = 38098.37925589\n",
      "Iteration 174, loss = 37624.24402069\n",
      "Iteration 175, loss = 37156.73439730\n",
      "Iteration 176, loss = 36695.43101838\n",
      "Iteration 177, loss = 36239.92483202\n",
      "Iteration 178, loss = 35791.65370704\n",
      "Iteration 179, loss = 35348.16077127\n",
      "Iteration 180, loss = 34911.73611949\n",
      "Iteration 181, loss = 34481.04966959\n",
      "Iteration 182, loss = 34056.09123101\n",
      "Iteration 183, loss = 33637.27823522\n",
      "Iteration 184, loss = 33223.57208487\n",
      "Iteration 185, loss = 32815.52170502\n",
      "Iteration 186, loss = 32414.20686888\n",
      "Iteration 187, loss = 32016.87689208\n",
      "Iteration 188, loss = 31625.34650835\n",
      "Iteration 189, loss = 31239.45442632\n",
      "Iteration 190, loss = 30858.59665872\n",
      "Iteration 191, loss = 30483.09854551\n",
      "Iteration 192, loss = 30112.62124206\n",
      "Iteration 193, loss = 29746.97497201\n",
      "Iteration 194, loss = 29387.14768410\n",
      "Iteration 195, loss = 29031.06544834\n",
      "Iteration 196, loss = 28680.00462050\n",
      "Iteration 197, loss = 28334.31610435\n",
      "Iteration 198, loss = 27992.78202761\n",
      "Iteration 199, loss = 27656.08732765\n",
      "Iteration 200, loss = 27325.17015809\n",
      "Iteration 1, loss = 4586.25932254\n",
      "Iteration 2, loss = 1121.60079970\n",
      "Iteration 3, loss = 603.88529636\n",
      "Iteration 4, loss = 418.14524495\n",
      "Iteration 5, loss = 323.84239207\n",
      "Iteration 6, loss = 304.74844256\n",
      "Iteration 7, loss = 299.64475299\n",
      "Iteration 8, loss = 297.82362353\n",
      "Iteration 9, loss = 297.56477181\n",
      "Iteration 10, loss = 297.08227949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 297.15678851\n",
      "Iteration 12, loss = 296.76900531\n",
      "Iteration 13, loss = 296.46451378\n",
      "Iteration 14, loss = 297.18140129\n",
      "Iteration 15, loss = 296.82781720\n",
      "Iteration 16, loss = 296.54512714\n",
      "Iteration 17, loss = 296.57993236\n",
      "Iteration 18, loss = 297.33744147\n",
      "Iteration 19, loss = 296.75532955\n",
      "Iteration 20, loss = 296.82044506\n",
      "Iteration 21, loss = 296.71735361\n",
      "Iteration 22, loss = 297.24791386\n",
      "Iteration 23, loss = 296.53712268\n",
      "Iteration 24, loss = 296.42272361\n",
      "Iteration 25, loss = 296.90915357\n",
      "Iteration 26, loss = 296.69470049\n",
      "Iteration 27, loss = 296.88657736\n",
      "Iteration 28, loss = 297.03277948\n",
      "Iteration 29, loss = 296.15110293\n",
      "Iteration 30, loss = 295.87388617\n",
      "Iteration 31, loss = 296.02330260\n",
      "Iteration 32, loss = 296.23161480\n",
      "Iteration 33, loss = 296.64701875\n",
      "Iteration 34, loss = 296.35484104\n",
      "Iteration 35, loss = 296.74571564\n",
      "Iteration 36, loss = 296.10608353\n",
      "Iteration 37, loss = 296.21484590\n",
      "Iteration 38, loss = 295.74406608\n",
      "Iteration 39, loss = 295.83534101\n",
      "Iteration 40, loss = 296.07630797\n",
      "Iteration 41, loss = 296.31031021\n",
      "Iteration 42, loss = 296.24914875\n",
      "Iteration 43, loss = 295.83089908\n",
      "Iteration 44, loss = 296.53377074\n",
      "Iteration 45, loss = 296.81737426\n",
      "Iteration 46, loss = 296.40888167\n",
      "Iteration 47, loss = 297.09565489\n",
      "Iteration 48, loss = 296.70681789\n",
      "Iteration 49, loss = 296.49015361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4519.77159262\n",
      "Iteration 2, loss = 965.33036691\n",
      "Iteration 3, loss = 637.63979866\n",
      "Iteration 4, loss = 452.05607140\n",
      "Iteration 5, loss = 359.51347608\n",
      "Iteration 6, loss = 326.50479557\n",
      "Iteration 7, loss = 312.71873112\n",
      "Iteration 8, loss = 303.18107919\n",
      "Iteration 9, loss = 301.63566941\n",
      "Iteration 10, loss = 300.66783424\n",
      "Iteration 11, loss = 301.35755981\n",
      "Iteration 12, loss = 300.84337103\n",
      "Iteration 13, loss = 301.75451149\n",
      "Iteration 14, loss = 300.22592305\n",
      "Iteration 15, loss = 300.39002774\n",
      "Iteration 16, loss = 300.87897530\n",
      "Iteration 17, loss = 300.35192227\n",
      "Iteration 18, loss = 300.17701846\n",
      "Iteration 19, loss = 300.08566908\n",
      "Iteration 20, loss = 300.22480388\n",
      "Iteration 21, loss = 300.05916257\n",
      "Iteration 22, loss = 300.34737357\n",
      "Iteration 23, loss = 299.82528237\n",
      "Iteration 24, loss = 299.30192619\n",
      "Iteration 25, loss = 299.92596985\n",
      "Iteration 26, loss = 299.53174382\n",
      "Iteration 27, loss = 300.43157198\n",
      "Iteration 28, loss = 298.98289235\n",
      "Iteration 29, loss = 299.42248829\n",
      "Iteration 30, loss = 298.56607763\n",
      "Iteration 31, loss = 298.92892669\n",
      "Iteration 32, loss = 298.96128871\n",
      "Iteration 33, loss = 298.69862846\n",
      "Iteration 34, loss = 297.95424042\n",
      "Iteration 35, loss = 299.76257676\n",
      "Iteration 36, loss = 298.28227892\n",
      "Iteration 37, loss = 298.33590146\n",
      "Iteration 38, loss = 298.73501337\n",
      "Iteration 39, loss = 298.58407932\n",
      "Iteration 40, loss = 298.66398044\n",
      "Iteration 41, loss = 298.67397458\n",
      "Iteration 42, loss = 298.10321776\n",
      "Iteration 43, loss = 298.36242318\n",
      "Iteration 44, loss = 299.11825758\n",
      "Iteration 45, loss = 298.45538503\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4498.37615455\n",
      "Iteration 2, loss = 1057.04023212\n",
      "Iteration 3, loss = 591.08352013\n",
      "Iteration 4, loss = 410.69379676\n",
      "Iteration 5, loss = 346.31029221\n",
      "Iteration 6, loss = 319.13541805\n",
      "Iteration 7, loss = 307.61530113\n",
      "Iteration 8, loss = 304.37142397\n",
      "Iteration 9, loss = 303.09600232\n",
      "Iteration 10, loss = 302.73413170\n",
      "Iteration 11, loss = 302.93793902\n",
      "Iteration 12, loss = 303.26840492\n",
      "Iteration 13, loss = 303.52088337\n",
      "Iteration 14, loss = 303.18534537\n",
      "Iteration 15, loss = 303.14979466\n",
      "Iteration 16, loss = 302.36392884\n",
      "Iteration 17, loss = 303.21872734\n",
      "Iteration 18, loss = 302.67568346\n",
      "Iteration 19, loss = 302.81365751\n",
      "Iteration 20, loss = 302.71085588\n",
      "Iteration 21, loss = 302.12393239\n",
      "Iteration 22, loss = 302.09248914\n",
      "Iteration 23, loss = 302.20447528\n",
      "Iteration 24, loss = 302.03721046\n",
      "Iteration 25, loss = 302.17945833\n",
      "Iteration 26, loss = 302.67740411\n",
      "Iteration 27, loss = 303.35861406\n",
      "Iteration 28, loss = 302.00048794\n",
      "Iteration 29, loss = 302.45663058\n",
      "Iteration 30, loss = 302.55679587\n",
      "Iteration 31, loss = 302.96514863\n",
      "Iteration 32, loss = 301.97866328\n",
      "Iteration 33, loss = 302.92559363\n",
      "Iteration 34, loss = 301.79019335\n",
      "Iteration 35, loss = 302.79498995\n",
      "Iteration 36, loss = 302.55973415\n",
      "Iteration 37, loss = 301.84253657\n",
      "Iteration 38, loss = 301.89682747\n",
      "Iteration 39, loss = 302.13631769\n",
      "Iteration 40, loss = 302.04769859\n",
      "Iteration 41, loss = 302.80388856\n",
      "Iteration 42, loss = 302.33522485\n",
      "Iteration 43, loss = 303.06910943\n",
      "Iteration 44, loss = 302.87729951\n",
      "Iteration 45, loss = 301.81552182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4530.82049711\n",
      "Iteration 2, loss = 1064.70545809\n",
      "Iteration 3, loss = 592.47795663\n",
      "Iteration 4, loss = 415.76710808\n",
      "Iteration 5, loss = 351.22222814\n",
      "Iteration 6, loss = 322.40868913\n",
      "Iteration 7, loss = 310.24648114\n",
      "Iteration 8, loss = 306.84491015\n",
      "Iteration 9, loss = 305.94304759\n",
      "Iteration 10, loss = 305.88035532\n",
      "Iteration 11, loss = 305.59478971\n",
      "Iteration 12, loss = 305.79594178\n",
      "Iteration 13, loss = 304.98299228\n",
      "Iteration 14, loss = 305.99639796\n",
      "Iteration 15, loss = 305.70333650\n",
      "Iteration 16, loss = 305.47821817\n",
      "Iteration 17, loss = 305.59495995\n",
      "Iteration 18, loss = 305.06112910\n",
      "Iteration 19, loss = 304.79608132\n",
      "Iteration 20, loss = 305.34741766\n",
      "Iteration 21, loss = 304.87609749\n",
      "Iteration 22, loss = 305.05663154\n",
      "Iteration 23, loss = 304.47933085\n",
      "Iteration 24, loss = 304.72890590\n",
      "Iteration 25, loss = 304.51077275\n",
      "Iteration 26, loss = 306.28986117\n",
      "Iteration 27, loss = 305.27383461\n",
      "Iteration 28, loss = 305.25368728\n",
      "Iteration 29, loss = 304.95392800\n",
      "Iteration 30, loss = 304.62388824\n",
      "Iteration 31, loss = 305.70128830\n",
      "Iteration 32, loss = 304.77883255\n",
      "Iteration 33, loss = 305.75754406\n",
      "Iteration 34, loss = 304.56249568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4562.17741701\n",
      "Iteration 2, loss = 1077.16694867\n",
      "Iteration 3, loss = 596.39327330\n",
      "Iteration 4, loss = 417.89609576\n",
      "Iteration 5, loss = 355.83623721\n",
      "Iteration 6, loss = 324.62170654\n",
      "Iteration 7, loss = 310.55507372\n",
      "Iteration 8, loss = 305.57900273\n",
      "Iteration 9, loss = 303.64078622\n",
      "Iteration 10, loss = 304.78578214\n",
      "Iteration 11, loss = 303.17295117\n",
      "Iteration 12, loss = 302.98765656\n",
      "Iteration 13, loss = 303.93917243\n",
      "Iteration 14, loss = 303.75412332\n",
      "Iteration 15, loss = 303.63663528\n",
      "Iteration 16, loss = 304.09433160\n",
      "Iteration 17, loss = 303.80421104\n",
      "Iteration 18, loss = 303.47302511\n",
      "Iteration 19, loss = 304.22351134\n",
      "Iteration 20, loss = 303.16168116\n",
      "Iteration 21, loss = 302.97663566\n",
      "Iteration 22, loss = 303.07453944\n",
      "Iteration 23, loss = 305.44815632\n",
      "Iteration 24, loss = 302.93846650\n",
      "Iteration 25, loss = 303.19288075\n",
      "Iteration 26, loss = 304.22094525\n",
      "Iteration 27, loss = 303.09395310\n",
      "Iteration 28, loss = 303.39271406\n",
      "Iteration 29, loss = 302.66226066\n",
      "Iteration 30, loss = 303.28328145\n",
      "Iteration 31, loss = 303.86386346\n",
      "Iteration 32, loss = 303.25122401\n",
      "Iteration 33, loss = 303.02744848\n",
      "Iteration 34, loss = 302.62259631\n",
      "Iteration 35, loss = 303.05479462\n",
      "Iteration 36, loss = 303.73192849\n",
      "Iteration 37, loss = 303.31242321\n",
      "Iteration 38, loss = 303.16855611\n",
      "Iteration 39, loss = 302.68480630\n",
      "Iteration 40, loss = 303.28588108\n",
      "Iteration 41, loss = 303.88196955\n",
      "Iteration 42, loss = 302.82254206\n",
      "Iteration 43, loss = 303.56819077\n",
      "Iteration 44, loss = 303.08521695\n",
      "Iteration 45, loss = 302.63218393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2434999.63605303\n",
      "Iteration 2, loss = 2540329.88766517\n",
      "Iteration 3, loss = 2505785.55562900\n",
      "Iteration 4, loss = 2471711.19876928\n",
      "Iteration 5, loss = 2438100.92572263\n",
      "Iteration 6, loss = 2404948.63698119\n",
      "Iteration 7, loss = 2372247.17479903\n",
      "Iteration 8, loss = 2339991.51321354\n",
      "Iteration 9, loss = 2308174.75432826\n",
      "Iteration 10, loss = 2276791.23197140\n",
      "Iteration 11, loss = 2245835.37139638\n",
      "Iteration 12, loss = 2215301.26864619\n",
      "Iteration 13, loss = 2185181.44331181\n",
      "Iteration 14, loss = 2155472.75441018\n",
      "Iteration 15, loss = 2126168.66576187\n",
      "Iteration 16, loss = 2097263.18236235\n",
      "Iteration 17, loss = 2068752.00815043\n",
      "Iteration 18, loss = 2040627.85451506\n",
      "Iteration 19, loss = 2012887.61722681\n",
      "Iteration 20, loss = 1985524.82370747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 1958534.27589149\n",
      "Iteration 22, loss = 1931911.14675591\n",
      "Iteration 23, loss = 1905651.34878336\n",
      "Iteration 24, loss = 1879747.62925886\n",
      "Iteration 25, loss = 1854198.57978171\n",
      "Iteration 26, loss = 1828995.85560119\n",
      "Iteration 27, loss = 1804137.77245169\n",
      "Iteration 28, loss = 1779616.20704075\n",
      "Iteration 29, loss = 1755430.27836586\n",
      "Iteration 30, loss = 1731572.88197939\n",
      "Iteration 31, loss = 1708039.94380776\n",
      "Iteration 32, loss = 1684827.86005528\n",
      "Iteration 33, loss = 1661931.40365294\n",
      "Iteration 34, loss = 1639347.42476443\n",
      "Iteration 35, loss = 1617070.74283097\n",
      "Iteration 36, loss = 1595097.26414071\n",
      "Iteration 37, loss = 1573423.00833757\n",
      "Iteration 38, loss = 1552043.49820249\n",
      "Iteration 39, loss = 1530956.16927114\n",
      "Iteration 40, loss = 1510154.67567001\n",
      "Iteration 41, loss = 1489637.23875864\n",
      "Iteration 42, loss = 1469399.33331223\n",
      "Iteration 43, loss = 1449436.20652276\n",
      "Iteration 44, loss = 1429745.06769587\n",
      "Iteration 45, loss = 1410322.07914133\n",
      "Iteration 46, loss = 1391163.92507269\n",
      "Iteration 47, loss = 1372266.55974840\n",
      "Iteration 48, loss = 1353625.67990834\n",
      "Iteration 49, loss = 1335239.15219695\n",
      "Iteration 50, loss = 1317102.89711873\n",
      "Iteration 51, loss = 1299214.35003557\n",
      "Iteration 52, loss = 1281568.73078134\n",
      "Iteration 53, loss = 1264163.52996449\n",
      "Iteration 54, loss = 1246994.87829383\n",
      "Iteration 55, loss = 1230060.55968027\n",
      "Iteration 56, loss = 1213355.88621836\n",
      "Iteration 57, loss = 1196880.07078932\n",
      "Iteration 58, loss = 1180627.72298902\n",
      "Iteration 59, loss = 1164597.35047158\n",
      "Iteration 60, loss = 1148784.05587741\n",
      "Iteration 61, loss = 1133186.78917097\n",
      "Iteration 62, loss = 1117801.37336677\n",
      "Iteration 63, loss = 1102626.09371627\n",
      "Iteration 64, loss = 1087657.43074007\n",
      "Iteration 65, loss = 1072892.16062620\n",
      "Iteration 66, loss = 1058328.45837141\n",
      "Iteration 67, loss = 1043962.45123491\n",
      "Iteration 68, loss = 1029792.38223631\n",
      "Iteration 69, loss = 1015815.24189128\n",
      "Iteration 70, loss = 1002028.45417373\n",
      "Iteration 71, loss = 988429.25703524\n",
      "Iteration 72, loss = 975015.00644146\n",
      "Iteration 73, loss = 961784.06245389\n",
      "Iteration 74, loss = 948732.74747986\n",
      "Iteration 75, loss = 935859.39773870\n",
      "Iteration 76, loss = 923161.14557877\n",
      "Iteration 77, loss = 910635.60842110\n",
      "Iteration 78, loss = 898281.13301457\n",
      "Iteration 79, loss = 886094.25026295\n",
      "Iteration 80, loss = 874073.67304777\n",
      "Iteration 81, loss = 862217.31382064\n",
      "Iteration 82, loss = 850521.00120524\n",
      "Iteration 83, loss = 838985.19964748\n",
      "Iteration 84, loss = 827605.94407962\n",
      "Iteration 85, loss = 816382.35783211\n",
      "Iteration 86, loss = 805310.61759628\n",
      "Iteration 87, loss = 794389.59824344\n",
      "Iteration 88, loss = 783617.78151103\n",
      "Iteration 89, loss = 772992.32265212\n",
      "Iteration 90, loss = 762512.18556946\n",
      "Iteration 91, loss = 752174.00170182\n",
      "Iteration 92, loss = 741976.56513652\n",
      "Iteration 93, loss = 731918.72364961\n",
      "Iteration 94, loss = 721997.02385374\n",
      "Iteration 95, loss = 712211.16091357\n",
      "Iteration 96, loss = 702557.70112999\n",
      "Iteration 97, loss = 693036.47011208\n",
      "Iteration 98, loss = 683644.34686068\n",
      "Iteration 99, loss = 674379.89822256\n",
      "Iteration 100, loss = 665242.12424346\n",
      "Iteration 101, loss = 656228.89371795\n",
      "Iteration 102, loss = 647338.17784933\n",
      "Iteration 103, loss = 638568.25800203\n",
      "Iteration 104, loss = 629918.14601560\n",
      "Iteration 105, loss = 621385.93465593\n",
      "Iteration 106, loss = 612969.01795557\n",
      "Iteration 107, loss = 604667.49796517\n",
      "Iteration 108, loss = 596478.45325244\n",
      "Iteration 109, loss = 588401.25420268\n",
      "Iteration 110, loss = 580434.62339089\n",
      "Iteration 111, loss = 572575.49883402\n",
      "Iteration 112, loss = 564823.58881692\n",
      "Iteration 113, loss = 557177.17221389\n",
      "Iteration 114, loss = 549635.05797037\n",
      "Iteration 115, loss = 542195.68375837\n",
      "Iteration 116, loss = 534857.38326316\n",
      "Iteration 117, loss = 527619.49006358\n",
      "Iteration 118, loss = 520479.82503297\n",
      "Iteration 119, loss = 513437.73948352\n",
      "Iteration 120, loss = 506490.30449317\n",
      "Iteration 121, loss = 499639.08825508\n",
      "Iteration 122, loss = 492880.19450104\n",
      "Iteration 123, loss = 486213.23563607\n",
      "Iteration 124, loss = 479637.96930696\n",
      "Iteration 125, loss = 473151.25050928\n",
      "Iteration 126, loss = 466753.64284820\n",
      "Iteration 127, loss = 460442.27626783\n",
      "Iteration 128, loss = 454217.07891033\n",
      "Iteration 129, loss = 448077.31940702\n",
      "Iteration 130, loss = 442020.29524373\n",
      "Iteration 131, loss = 436046.24476781\n",
      "Iteration 132, loss = 430153.42361171\n",
      "Iteration 133, loss = 424340.63096025\n",
      "Iteration 134, loss = 418608.20969323\n",
      "Iteration 135, loss = 412952.16802002\n",
      "Iteration 136, loss = 407373.98926936\n",
      "Iteration 137, loss = 401871.50142005\n",
      "Iteration 138, loss = 396443.68647908\n",
      "Iteration 139, loss = 391090.26979426\n",
      "Iteration 140, loss = 385809.47579612\n",
      "Iteration 141, loss = 380601.07237265\n",
      "Iteration 142, loss = 375462.60838095\n",
      "Iteration 143, loss = 370394.92509565\n",
      "Iteration 144, loss = 365396.15250153\n",
      "Iteration 145, loss = 360465.45630771\n",
      "Iteration 146, loss = 355602.01237775\n",
      "Iteration 147, loss = 350804.04628581\n",
      "Iteration 148, loss = 346071.98824010\n",
      "Iteration 149, loss = 341404.31858101\n",
      "Iteration 150, loss = 336800.08627401\n",
      "Iteration 151, loss = 332258.86525630\n",
      "Iteration 152, loss = 327778.08643702\n",
      "Iteration 153, loss = 323360.22380155\n",
      "Iteration 154, loss = 319001.81995740\n",
      "Iteration 155, loss = 314702.91951414\n",
      "Iteration 156, loss = 310461.70628308\n",
      "Iteration 157, loss = 306278.89871687\n",
      "Iteration 158, loss = 302153.30527350\n",
      "Iteration 159, loss = 298083.78204492\n",
      "Iteration 160, loss = 294069.50542491\n",
      "Iteration 161, loss = 290109.58015425\n",
      "Iteration 162, loss = 286204.04386790\n",
      "Iteration 163, loss = 282351.27263735\n",
      "Iteration 164, loss = 278551.08039911\n",
      "Iteration 165, loss = 274803.11095359\n",
      "Iteration 166, loss = 271104.97933868\n",
      "Iteration 167, loss = 267458.39568511\n",
      "Iteration 168, loss = 263861.21881331\n",
      "Iteration 169, loss = 260312.62649631\n",
      "Iteration 170, loss = 256812.67166368\n",
      "Iteration 171, loss = 253360.28015551\n",
      "Iteration 172, loss = 249954.79649479\n",
      "Iteration 173, loss = 246596.06182892\n",
      "Iteration 174, loss = 243282.53826174\n",
      "Iteration 175, loss = 240014.52726242\n",
      "Iteration 176, loss = 236790.65877796\n",
      "Iteration 177, loss = 233611.47275347\n",
      "Iteration 178, loss = 230474.72943041\n",
      "Iteration 179, loss = 227380.56732097\n",
      "Iteration 180, loss = 224329.61018163\n",
      "Iteration 181, loss = 221319.25524131\n",
      "Iteration 182, loss = 218349.63010801\n",
      "Iteration 183, loss = 215421.58923934\n",
      "Iteration 184, loss = 212532.37300735\n",
      "Iteration 185, loss = 209682.98318553\n",
      "Iteration 186, loss = 206872.61870511\n",
      "Iteration 187, loss = 204099.70745861\n",
      "Iteration 188, loss = 201365.28461451\n",
      "Iteration 189, loss = 198668.01089465\n",
      "Iteration 190, loss = 196006.91935140\n",
      "Iteration 191, loss = 193382.48853406\n",
      "Iteration 192, loss = 190794.77862595\n",
      "Iteration 193, loss = 188240.65582298\n",
      "Iteration 194, loss = 185721.62030135\n",
      "Iteration 195, loss = 183236.84374630\n",
      "Iteration 196, loss = 180787.95720373\n",
      "Iteration 197, loss = 178369.29968205\n",
      "Iteration 198, loss = 175985.19746174\n",
      "Iteration 199, loss = 173633.19773596\n",
      "Iteration 200, loss = 171313.30595084\n",
      "Iteration 1, loss = 641720.11546188\n",
      "Iteration 2, loss = 665152.35617728\n",
      "Iteration 3, loss = 656139.90110193\n",
      "Iteration 4, loss = 647249.83411605\n",
      "Iteration 5, loss = 638481.08585029\n",
      "Iteration 6, loss = 629832.21013985\n",
      "Iteration 7, loss = 621300.26387326\n",
      "Iteration 8, loss = 612884.98405677\n",
      "Iteration 9, loss = 604583.90234045\n",
      "Iteration 10, loss = 596396.23061860\n",
      "Iteration 11, loss = 588320.22205287\n",
      "Iteration 12, loss = 580353.31536920\n",
      "Iteration 13, loss = 572495.43982339\n",
      "Iteration 14, loss = 564744.47996329\n",
      "Iteration 15, loss = 557099.23645937\n",
      "Iteration 16, loss = 549558.01151250\n",
      "Iteration 17, loss = 542119.47961921\n",
      "Iteration 18, loss = 534781.67583216\n",
      "Iteration 19, loss = 527544.81452977\n",
      "Iteration 20, loss = 520406.08832672\n",
      "Iteration 21, loss = 513363.82069094\n",
      "Iteration 22, loss = 506418.08144770\n",
      "Iteration 23, loss = 499567.32784459\n",
      "Iteration 24, loss = 492808.86405659\n",
      "Iteration 25, loss = 486143.19046844\n",
      "Iteration 26, loss = 479568.04466919\n",
      "Iteration 27, loss = 473082.37777171\n",
      "Iteration 28, loss = 466685.03901207\n",
      "Iteration 29, loss = 460374.93752436\n",
      "Iteration 30, loss = 454150.30263450\n",
      "Iteration 31, loss = 448010.97980791\n",
      "Iteration 32, loss = 441955.09752051\n",
      "Iteration 33, loss = 435981.57875811\n",
      "Iteration 34, loss = 430089.06583590\n",
      "Iteration 35, loss = 424277.23987239\n",
      "Iteration 36, loss = 418544.74656886\n",
      "Iteration 37, loss = 412890.21389984\n",
      "Iteration 38, loss = 407312.19687287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 401810.48029542\n",
      "Iteration 40, loss = 396383.20312179\n",
      "Iteration 41, loss = 391030.42572048\n",
      "Iteration 42, loss = 385750.66631230\n",
      "Iteration 43, loss = 380542.37371051\n",
      "Iteration 44, loss = 375404.81984824\n",
      "Iteration 45, loss = 370337.25188064\n",
      "Iteration 46, loss = 365339.29579363\n",
      "Iteration 47, loss = 360408.75673659\n",
      "Iteration 48, loss = 355545.31310426\n",
      "Iteration 49, loss = 350748.40024914\n",
      "Iteration 50, loss = 346016.52182303\n",
      "Iteration 51, loss = 341349.88206619\n",
      "Iteration 52, loss = 336745.99523446\n",
      "Iteration 53, loss = 332206.01778494\n",
      "Iteration 54, loss = 327725.63373807\n",
      "Iteration 55, loss = 323307.87288424\n",
      "Iteration 56, loss = 318949.56873757\n",
      "Iteration 57, loss = 314651.08271747\n",
      "Iteration 58, loss = 310411.17197063\n",
      "Iteration 59, loss = 306228.92687813\n",
      "Iteration 60, loss = 302103.20024064\n",
      "Iteration 61, loss = 298034.06433968\n",
      "Iteration 62, loss = 294019.79964473\n",
      "Iteration 63, loss = 290060.99211972\n",
      "Iteration 64, loss = 286155.30507760\n",
      "Iteration 65, loss = 282303.11961763\n",
      "Iteration 66, loss = 278503.48971382\n",
      "Iteration 67, loss = 274755.25705649\n",
      "Iteration 68, loss = 271059.05550270\n",
      "Iteration 69, loss = 267412.31816364\n",
      "Iteration 70, loss = 263814.86346013\n",
      "Iteration 71, loss = 260267.60852340\n",
      "Iteration 72, loss = 256767.55133558\n",
      "Iteration 73, loss = 253315.37062286\n",
      "Iteration 74, loss = 249910.68410121\n",
      "Iteration 75, loss = 246552.27912093\n",
      "Iteration 76, loss = 243239.06279962\n",
      "Iteration 77, loss = 239971.11530837\n",
      "Iteration 78, loss = 236747.63274181\n",
      "Iteration 79, loss = 233568.33585923\n",
      "Iteration 80, loss = 230431.67160195\n",
      "Iteration 81, loss = 227339.14006541\n",
      "Iteration 82, loss = 224287.35099347\n",
      "Iteration 83, loss = 221277.81449815\n",
      "Iteration 84, loss = 218309.02427791\n",
      "Iteration 85, loss = 215380.76409447\n",
      "Iteration 86, loss = 212492.39684295\n",
      "Iteration 87, loss = 209643.00600979\n",
      "Iteration 88, loss = 206832.15754234\n",
      "Iteration 89, loss = 204060.74446204\n",
      "Iteration 90, loss = 201326.01365711\n",
      "Iteration 91, loss = 198629.26335195\n",
      "Iteration 92, loss = 195968.26342784\n",
      "Iteration 93, loss = 193344.38527008\n",
      "Iteration 94, loss = 190756.01985862\n",
      "Iteration 95, loss = 188203.06229531\n",
      "Iteration 96, loss = 185684.26434355\n",
      "Iteration 97, loss = 183200.47556480\n",
      "Iteration 98, loss = 180750.08549936\n",
      "Iteration 99, loss = 178332.30123560\n",
      "Iteration 100, loss = 175949.05444457\n",
      "Iteration 101, loss = 173597.22304758\n",
      "Iteration 102, loss = 171277.74739935\n",
      "Iteration 103, loss = 168990.09920084\n",
      "Iteration 104, loss = 166733.12973137\n",
      "Iteration 105, loss = 164506.83502997\n",
      "Iteration 106, loss = 162311.06641892\n",
      "Iteration 107, loss = 160145.07232400\n",
      "Iteration 108, loss = 158008.78374100\n",
      "Iteration 109, loss = 155901.57214988\n",
      "Iteration 110, loss = 153823.26157161\n",
      "Iteration 111, loss = 151772.50269947\n",
      "Iteration 112, loss = 149749.77568022\n",
      "Iteration 113, loss = 147755.10073910\n",
      "Iteration 114, loss = 145787.46573343\n",
      "Iteration 115, loss = 143846.21855941\n",
      "Iteration 116, loss = 141931.81220421\n",
      "Iteration 117, loss = 140043.29628489\n",
      "Iteration 118, loss = 138180.99941290\n",
      "Iteration 119, loss = 136343.37109956\n",
      "Iteration 120, loss = 134530.53069570\n",
      "Iteration 121, loss = 132743.92126168\n",
      "Iteration 122, loss = 130980.27593507\n",
      "Iteration 123, loss = 129240.83404035\n",
      "Iteration 124, loss = 127525.51535048\n",
      "Iteration 125, loss = 125833.06222287\n",
      "Iteration 126, loss = 124163.52091616\n",
      "Iteration 127, loss = 122517.18588462\n",
      "Iteration 128, loss = 120893.24661858\n",
      "Iteration 129, loss = 119291.27410215\n",
      "Iteration 130, loss = 117710.95560430\n",
      "Iteration 131, loss = 116152.29537266\n",
      "Iteration 132, loss = 114614.93049300\n",
      "Iteration 133, loss = 113098.05970429\n",
      "Iteration 134, loss = 111603.35748465\n",
      "Iteration 135, loss = 110127.49765249\n",
      "Iteration 136, loss = 108671.83439399\n",
      "Iteration 137, loss = 107236.52567242\n",
      "Iteration 138, loss = 105820.11268663\n",
      "Iteration 139, loss = 104423.50088253\n",
      "Iteration 140, loss = 103045.85482969\n",
      "Iteration 141, loss = 101686.66858740\n",
      "Iteration 142, loss = 100346.85813380\n",
      "Iteration 143, loss = 99024.16667169\n",
      "Iteration 144, loss = 97719.97299795\n",
      "Iteration 145, loss = 96433.72020377\n",
      "Iteration 146, loss = 95165.31581602\n",
      "Iteration 147, loss = 93913.06257462\n",
      "Iteration 148, loss = 92678.22148225\n",
      "Iteration 149, loss = 91460.43270075\n",
      "Iteration 150, loss = 90259.39254608\n",
      "Iteration 151, loss = 89074.99162826\n",
      "Iteration 152, loss = 87905.49766215\n",
      "Iteration 153, loss = 86753.16496520\n",
      "Iteration 154, loss = 85615.97465468\n",
      "Iteration 155, loss = 84494.31704078\n",
      "Iteration 156, loss = 83387.57859257\n",
      "Iteration 157, loss = 82296.34200429\n",
      "Iteration 158, loss = 81219.84643893\n",
      "Iteration 159, loss = 80158.53785681\n",
      "Iteration 160, loss = 79111.33843827\n",
      "Iteration 161, loss = 78078.00913448\n",
      "Iteration 162, loss = 77059.14689546\n",
      "Iteration 163, loss = 76054.03157747\n",
      "Iteration 164, loss = 75062.45966282\n",
      "Iteration 165, loss = 74084.90931644\n",
      "Iteration 166, loss = 73119.58486956\n",
      "Iteration 167, loss = 72168.32417171\n",
      "Iteration 168, loss = 71230.07478716\n",
      "Iteration 169, loss = 70304.07529115\n",
      "Iteration 170, loss = 69391.18924596\n",
      "Iteration 171, loss = 68490.18556389\n",
      "Iteration 172, loss = 67602.05667732\n",
      "Iteration 173, loss = 66725.54151817\n",
      "Iteration 174, loss = 65861.27155706\n",
      "Iteration 175, loss = 65008.23640419\n",
      "Iteration 176, loss = 64167.33667773\n",
      "Iteration 177, loss = 63338.28466617\n",
      "Iteration 178, loss = 62519.38996891\n",
      "Iteration 179, loss = 61712.27149032\n",
      "Iteration 180, loss = 60915.99790655\n",
      "Iteration 181, loss = 60130.99258797\n",
      "Iteration 182, loss = 59356.15579983\n",
      "Iteration 183, loss = 58592.38836405\n",
      "Iteration 184, loss = 57838.53115811\n",
      "Iteration 185, loss = 57095.22979358\n",
      "Iteration 186, loss = 56362.16071142\n",
      "Iteration 187, loss = 55638.03482908\n",
      "Iteration 188, loss = 54924.75483416\n",
      "Iteration 189, loss = 54221.32313661\n",
      "Iteration 190, loss = 53526.90263455\n",
      "Iteration 191, loss = 52842.43357934\n",
      "Iteration 192, loss = 52167.57491030\n",
      "Iteration 193, loss = 51500.79581362\n",
      "Iteration 194, loss = 50843.73138112\n",
      "Iteration 195, loss = 50195.09676421\n",
      "Iteration 196, loss = 49557.36916392\n",
      "Iteration 197, loss = 48925.51364736\n",
      "Iteration 198, loss = 48303.31140632\n",
      "Iteration 199, loss = 47689.72894714\n",
      "Iteration 200, loss = 47084.88227748\n",
      "Iteration 1, loss = 586227.93740085\n",
      "Iteration 2, loss = 607829.25394329\n",
      "Iteration 3, loss = 599596.61721887\n",
      "Iteration 4, loss = 591476.72217971\n",
      "Iteration 5, loss = 583467.46732788\n",
      "Iteration 6, loss = 575567.86095187\n",
      "Iteration 7, loss = 567774.64500836\n",
      "Iteration 8, loss = 560087.80584782\n",
      "Iteration 9, loss = 552506.18918879\n",
      "Iteration 10, loss = 545027.39113551\n",
      "Iteration 11, loss = 537651.03647537\n",
      "Iteration 12, loss = 530373.63374020\n",
      "Iteration 13, loss = 523196.43280097\n",
      "Iteration 14, loss = 516117.07067762\n",
      "Iteration 15, loss = 509133.55952493\n",
      "Iteration 16, loss = 502246.18136729\n",
      "Iteration 17, loss = 495451.48011422\n",
      "Iteration 18, loss = 488749.47512247\n",
      "Iteration 19, loss = 482139.56183826\n",
      "Iteration 20, loss = 475618.48790039\n",
      "Iteration 21, loss = 469186.50065315\n",
      "Iteration 22, loss = 462842.20436367\n",
      "Iteration 23, loss = 456584.33644126\n",
      "Iteration 24, loss = 450411.40238624\n",
      "Iteration 25, loss = 444323.12134512\n",
      "Iteration 26, loss = 438317.73344298\n",
      "Iteration 27, loss = 432393.96885259\n",
      "Iteration 28, loss = 426550.67499430\n",
      "Iteration 29, loss = 420787.02451519\n",
      "Iteration 30, loss = 415101.36084645\n",
      "Iteration 31, loss = 409493.82977267\n",
      "Iteration 32, loss = 403962.16818442\n",
      "Iteration 33, loss = 398506.32397855\n",
      "Iteration 34, loss = 393124.62709984\n",
      "Iteration 35, loss = 387815.46654175\n",
      "Iteration 36, loss = 382579.52288167\n",
      "Iteration 37, loss = 377414.75749854\n",
      "Iteration 38, loss = 372320.07168534\n",
      "Iteration 39, loss = 367294.38666621\n",
      "Iteration 40, loss = 362337.41943952\n",
      "Iteration 41, loss = 357448.41673374\n",
      "Iteration 42, loss = 352625.62043224\n",
      "Iteration 43, loss = 347868.10764705\n",
      "Iteration 44, loss = 343175.96765440\n",
      "Iteration 45, loss = 338547.28721633\n",
      "Iteration 46, loss = 333982.36856122\n",
      "Iteration 47, loss = 329478.68586512\n",
      "Iteration 48, loss = 325036.28015967\n",
      "Iteration 49, loss = 320655.35519128\n",
      "Iteration 50, loss = 316332.69948261\n",
      "Iteration 51, loss = 312070.46016043\n",
      "Iteration 52, loss = 307865.55709954\n",
      "Iteration 53, loss = 303718.65246479\n",
      "Iteration 54, loss = 299626.08363098\n",
      "Iteration 55, loss = 295590.99780554\n",
      "Iteration 56, loss = 291610.31924039\n",
      "Iteration 57, loss = 287684.12902594\n",
      "Iteration 58, loss = 283811.25763315\n",
      "Iteration 59, loss = 279991.12740389\n",
      "Iteration 60, loss = 276223.15963060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 272506.30345776\n",
      "Iteration 62, loss = 268839.75555767\n",
      "Iteration 63, loss = 265223.57945748\n",
      "Iteration 64, loss = 261656.44969075\n",
      "Iteration 65, loss = 258137.60379768\n",
      "Iteration 66, loss = 254667.20883359\n",
      "Iteration 67, loss = 251243.74630411\n",
      "Iteration 68, loss = 247867.36465392\n",
      "Iteration 69, loss = 244536.21421865\n",
      "Iteration 70, loss = 241250.56132876\n",
      "Iteration 71, loss = 238010.33729153\n",
      "Iteration 72, loss = 234813.81470958\n",
      "Iteration 73, loss = 231660.70068521\n",
      "Iteration 74, loss = 228550.57713455\n",
      "Iteration 75, loss = 225483.13405009\n",
      "Iteration 76, loss = 222457.16796328\n",
      "Iteration 77, loss = 219472.00395313\n",
      "Iteration 78, loss = 216527.93908650\n",
      "Iteration 79, loss = 213623.78639797\n",
      "Iteration 80, loss = 210759.29295537\n",
      "Iteration 81, loss = 207934.11155922\n",
      "Iteration 82, loss = 205146.41700780\n",
      "Iteration 83, loss = 202397.49976929\n",
      "Iteration 84, loss = 199686.16151741\n",
      "Iteration 85, loss = 197011.24198260\n",
      "Iteration 86, loss = 194373.87071550\n",
      "Iteration 87, loss = 191770.29226703\n",
      "Iteration 88, loss = 189203.15666084\n",
      "Iteration 89, loss = 186671.49211593\n",
      "Iteration 90, loss = 184174.45939802\n",
      "Iteration 91, loss = 181710.61192130\n",
      "Iteration 92, loss = 179280.75592423\n",
      "Iteration 93, loss = 176883.75349217\n",
      "Iteration 94, loss = 174519.55567414\n",
      "Iteration 95, loss = 172187.38153258\n",
      "Iteration 96, loss = 169887.10290491\n",
      "Iteration 97, loss = 167618.16442784\n",
      "Iteration 98, loss = 165380.01309831\n",
      "Iteration 99, loss = 163171.65669475\n",
      "Iteration 100, loss = 160994.52216144\n",
      "Iteration 101, loss = 158846.46586737\n",
      "Iteration 102, loss = 156728.09049097\n",
      "Iteration 103, loss = 154637.66346054\n",
      "Iteration 104, loss = 152576.76329535\n",
      "Iteration 105, loss = 150543.56851141\n",
      "Iteration 106, loss = 148538.25955630\n",
      "Iteration 107, loss = 146559.57133198\n",
      "Iteration 108, loss = 144608.73905188\n",
      "Iteration 109, loss = 142683.38574868\n",
      "Iteration 110, loss = 140785.29917391\n",
      "Iteration 111, loss = 138912.15831327\n",
      "Iteration 112, loss = 137064.75594270\n",
      "Iteration 113, loss = 135242.60985993\n",
      "Iteration 114, loss = 133445.55255155\n",
      "Iteration 115, loss = 131672.48667049\n",
      "Iteration 116, loss = 129923.78179454\n",
      "Iteration 117, loss = 128199.05428639\n",
      "Iteration 118, loss = 126498.04111339\n",
      "Iteration 119, loss = 124819.39075082\n",
      "Iteration 120, loss = 123163.63845851\n",
      "Iteration 121, loss = 121531.73526357\n",
      "Iteration 122, loss = 119920.70298748\n",
      "Iteration 123, loss = 118331.90501119\n",
      "Iteration 124, loss = 116764.98049706\n",
      "Iteration 125, loss = 115219.18019587\n",
      "Iteration 126, loss = 113694.17303649\n",
      "Iteration 127, loss = 112190.72741185\n",
      "Iteration 128, loss = 110707.52464904\n",
      "Iteration 129, loss = 109244.30464812\n",
      "Iteration 130, loss = 107800.93542654\n",
      "Iteration 131, loss = 106377.18009449\n",
      "Iteration 132, loss = 104972.62467960\n",
      "Iteration 133, loss = 103587.43713644\n",
      "Iteration 134, loss = 102221.56274278\n",
      "Iteration 135, loss = 100874.26439321\n",
      "Iteration 136, loss = 99544.47727272\n",
      "Iteration 137, loss = 98233.34470918\n",
      "Iteration 138, loss = 96940.23965062\n",
      "Iteration 139, loss = 95664.52732033\n",
      "Iteration 140, loss = 94405.79178564\n",
      "Iteration 141, loss = 93164.83025296\n",
      "Iteration 142, loss = 91940.68138090\n",
      "Iteration 143, loss = 90732.85417378\n",
      "Iteration 144, loss = 89540.93231678\n",
      "Iteration 145, loss = 88366.30905258\n",
      "Iteration 146, loss = 87207.20841251\n",
      "Iteration 147, loss = 86063.87270031\n",
      "Iteration 148, loss = 84936.37297119\n",
      "Iteration 149, loss = 83824.18123506\n",
      "Iteration 150, loss = 82726.82706041\n",
      "Iteration 151, loss = 81644.93233419\n",
      "Iteration 152, loss = 80576.47599485\n",
      "Iteration 153, loss = 79524.23981373\n",
      "Iteration 154, loss = 78485.30495117\n",
      "Iteration 155, loss = 77460.62896531\n",
      "Iteration 156, loss = 76449.50991604\n",
      "Iteration 157, loss = 75453.35060714\n",
      "Iteration 158, loss = 74470.34480335\n",
      "Iteration 159, loss = 73500.57948233\n",
      "Iteration 160, loss = 72544.08026328\n",
      "Iteration 161, loss = 71600.77766441\n",
      "Iteration 162, loss = 70669.99999683\n",
      "Iteration 163, loss = 69751.61014994\n",
      "Iteration 164, loss = 68846.14229419\n",
      "Iteration 165, loss = 67953.04393124\n",
      "Iteration 166, loss = 67071.55152098\n",
      "Iteration 167, loss = 66202.31683174\n",
      "Iteration 168, loss = 65345.78173967\n",
      "Iteration 169, loss = 64499.93624662\n",
      "Iteration 170, loss = 63665.64999783\n",
      "Iteration 171, loss = 62843.53986336\n",
      "Iteration 172, loss = 62031.57506136\n",
      "Iteration 173, loss = 61231.21129737\n",
      "Iteration 174, loss = 60441.97753077\n",
      "Iteration 175, loss = 59662.48641414\n",
      "Iteration 176, loss = 58895.02463787\n",
      "Iteration 177, loss = 58137.04121279\n",
      "Iteration 178, loss = 57389.12090579\n",
      "Iteration 179, loss = 56652.25122425\n",
      "Iteration 180, loss = 55924.49227138\n",
      "Iteration 181, loss = 55207.54566334\n",
      "Iteration 182, loss = 54499.92676923\n",
      "Iteration 183, loss = 53802.31771975\n",
      "Iteration 184, loss = 53113.89478939\n",
      "Iteration 185, loss = 52434.91506967\n",
      "Iteration 186, loss = 51765.38169225\n",
      "Iteration 187, loss = 51103.75297410\n",
      "Iteration 188, loss = 50452.70332605\n",
      "Iteration 189, loss = 49809.73405086\n",
      "Iteration 190, loss = 49175.84736517\n",
      "Iteration 191, loss = 48550.15255295\n",
      "Iteration 192, loss = 47933.77274345\n",
      "Iteration 193, loss = 47324.86429430\n",
      "Iteration 194, loss = 46724.53758553\n",
      "Iteration 195, loss = 46132.86294598\n",
      "Iteration 196, loss = 45549.18475725\n",
      "Iteration 197, loss = 44972.82823525\n",
      "Iteration 198, loss = 44404.56154340\n",
      "Iteration 199, loss = 43844.15227875\n",
      "Iteration 200, loss = 43291.35025886\n",
      "Iteration 1, loss = 287717.73837622\n",
      "Iteration 2, loss = 296967.63661319\n",
      "Iteration 3, loss = 292968.14756393\n",
      "Iteration 4, loss = 289023.75849811\n",
      "Iteration 5, loss = 285132.46246007\n",
      "Iteration 6, loss = 281295.66374104\n",
      "Iteration 7, loss = 277509.09341307\n",
      "Iteration 8, loss = 273774.59828820\n",
      "Iteration 9, loss = 270091.94750751\n",
      "Iteration 10, loss = 266458.33839896\n",
      "Iteration 11, loss = 262875.29257898\n",
      "Iteration 12, loss = 259339.70266379\n",
      "Iteration 13, loss = 255853.05195462\n",
      "Iteration 14, loss = 252414.24036753\n",
      "Iteration 15, loss = 249021.32164811\n",
      "Iteration 16, loss = 245675.90050497\n",
      "Iteration 17, loss = 242374.47089194\n",
      "Iteration 18, loss = 239119.02361979\n",
      "Iteration 19, loss = 235907.73143980\n",
      "Iteration 20, loss = 232739.91079540\n",
      "Iteration 21, loss = 229615.21125995\n",
      "Iteration 22, loss = 226533.40741633\n",
      "Iteration 23, loss = 223493.24549927\n",
      "Iteration 24, loss = 220494.82624613\n",
      "Iteration 25, loss = 217536.53788372\n",
      "Iteration 26, loss = 214619.64347495\n",
      "Iteration 27, loss = 211741.97196651\n",
      "Iteration 28, loss = 208903.20061046\n",
      "Iteration 29, loss = 206103.06196586\n",
      "Iteration 30, loss = 203341.18444669\n",
      "Iteration 31, loss = 200617.16260193\n",
      "Iteration 32, loss = 197929.60504438\n",
      "Iteration 33, loss = 195279.42505693\n",
      "Iteration 34, loss = 192665.34611051\n",
      "Iteration 35, loss = 190086.06775218\n",
      "Iteration 36, loss = 187541.81740892\n",
      "Iteration 37, loss = 185032.98561756\n",
      "Iteration 38, loss = 182557.78094947\n",
      "Iteration 39, loss = 180116.67813861\n",
      "Iteration 40, loss = 177708.25775385\n",
      "Iteration 41, loss = 175333.64811727\n",
      "Iteration 42, loss = 172990.55825915\n",
      "Iteration 43, loss = 170679.31027245\n",
      "Iteration 44, loss = 168400.02415225\n",
      "Iteration 45, loss = 166151.41698397\n",
      "Iteration 46, loss = 163933.68710400\n",
      "Iteration 47, loss = 161745.56334651\n",
      "Iteration 48, loss = 159587.82055615\n",
      "Iteration 49, loss = 157459.64664917\n",
      "Iteration 50, loss = 155359.84699524\n",
      "Iteration 51, loss = 153289.39784330\n",
      "Iteration 52, loss = 151246.44775696\n",
      "Iteration 53, loss = 149231.93371796\n",
      "Iteration 54, loss = 147243.76041020\n",
      "Iteration 55, loss = 145283.86437132\n",
      "Iteration 56, loss = 143349.78389915\n",
      "Iteration 57, loss = 141442.61638971\n",
      "Iteration 58, loss = 139560.99081519\n",
      "Iteration 59, loss = 137705.06617498\n",
      "Iteration 60, loss = 135874.69822880\n",
      "Iteration 61, loss = 134069.56412799\n",
      "Iteration 62, loss = 132287.68071354\n",
      "Iteration 63, loss = 130531.08946080\n",
      "Iteration 64, loss = 128798.12855577\n",
      "Iteration 65, loss = 127088.74990328\n",
      "Iteration 66, loss = 125403.10301807\n",
      "Iteration 67, loss = 123739.70245868\n",
      "Iteration 68, loss = 122099.60494057\n",
      "Iteration 69, loss = 120481.37431260\n",
      "Iteration 70, loss = 118884.90863550\n",
      "Iteration 71, loss = 117310.96805622\n",
      "Iteration 72, loss = 115758.19416252\n",
      "Iteration 73, loss = 114226.51157092\n",
      "Iteration 74, loss = 112715.56831562\n",
      "Iteration 75, loss = 111225.82199028\n",
      "Iteration 76, loss = 109755.65421457\n",
      "Iteration 77, loss = 108304.99091922\n",
      "Iteration 78, loss = 106874.60367842\n",
      "Iteration 79, loss = 105464.19582673\n",
      "Iteration 80, loss = 104072.80576861\n",
      "Iteration 81, loss = 102699.94192270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 101345.91986875\n",
      "Iteration 83, loss = 100010.41144950\n",
      "Iteration 84, loss = 98693.39501495\n",
      "Iteration 85, loss = 97393.79077358\n",
      "Iteration 86, loss = 96112.81482713\n",
      "Iteration 87, loss = 94847.73971862\n",
      "Iteration 88, loss = 93600.91766497\n",
      "Iteration 89, loss = 92370.83876135\n",
      "Iteration 90, loss = 91157.51961863\n",
      "Iteration 91, loss = 89960.61676783\n",
      "Iteration 92, loss = 88780.60746420\n",
      "Iteration 93, loss = 87615.94183798\n",
      "Iteration 94, loss = 86467.06098857\n",
      "Iteration 95, loss = 85334.63360377\n",
      "Iteration 96, loss = 84217.38742594\n",
      "Iteration 97, loss = 83114.54676419\n",
      "Iteration 98, loss = 82028.40398756\n",
      "Iteration 99, loss = 80954.95202827\n",
      "Iteration 100, loss = 79897.47332082\n",
      "Iteration 101, loss = 78853.39537087\n",
      "Iteration 102, loss = 77824.36684428\n",
      "Iteration 103, loss = 76809.26437177\n",
      "Iteration 104, loss = 75807.73014497\n",
      "Iteration 105, loss = 74820.17839261\n",
      "Iteration 106, loss = 73845.93827108\n",
      "Iteration 107, loss = 72884.39275692\n",
      "Iteration 108, loss = 71936.68972602\n",
      "Iteration 109, loss = 71001.56834382\n",
      "Iteration 110, loss = 70079.78005156\n",
      "Iteration 111, loss = 69169.40837289\n",
      "Iteration 112, loss = 68272.22885299\n",
      "Iteration 113, loss = 67387.20974721\n",
      "Iteration 114, loss = 66514.19555678\n",
      "Iteration 115, loss = 65652.61962958\n",
      "Iteration 116, loss = 64803.44552214\n",
      "Iteration 117, loss = 63965.24444504\n",
      "Iteration 118, loss = 63138.87311574\n",
      "Iteration 119, loss = 62323.01166537\n",
      "Iteration 120, loss = 61518.87180651\n",
      "Iteration 121, loss = 60726.54228373\n",
      "Iteration 122, loss = 59943.87380164\n",
      "Iteration 123, loss = 59171.72175444\n",
      "Iteration 124, loss = 58410.65130047\n",
      "Iteration 125, loss = 57658.97724922\n",
      "Iteration 126, loss = 56919.11535478\n",
      "Iteration 127, loss = 56188.08809782\n",
      "Iteration 128, loss = 55467.90842552\n",
      "Iteration 129, loss = 54756.80527286\n",
      "Iteration 130, loss = 54056.29453113\n",
      "Iteration 131, loss = 53364.09631586\n",
      "Iteration 132, loss = 52682.13746934\n",
      "Iteration 133, loss = 52009.02141766\n",
      "Iteration 134, loss = 51345.37727206\n",
      "Iteration 135, loss = 50690.65027718\n",
      "Iteration 136, loss = 50044.88574680\n",
      "Iteration 137, loss = 49407.94396560\n",
      "Iteration 138, loss = 48780.06066386\n",
      "Iteration 139, loss = 48160.19235414\n",
      "Iteration 140, loss = 47548.54177330\n",
      "Iteration 141, loss = 46945.63251579\n",
      "Iteration 142, loss = 46350.98173437\n",
      "Iteration 143, loss = 45764.50478454\n",
      "Iteration 144, loss = 45185.26890285\n",
      "Iteration 145, loss = 44614.59627645\n",
      "Iteration 146, loss = 44051.14825744\n",
      "Iteration 147, loss = 43495.91936521\n",
      "Iteration 148, loss = 42948.53843390\n",
      "Iteration 149, loss = 42408.11128149\n",
      "Iteration 150, loss = 41874.87273325\n",
      "Iteration 151, loss = 41349.39959191\n",
      "Iteration 152, loss = 40829.97095817\n",
      "Iteration 153, loss = 40319.01845650\n",
      "Iteration 154, loss = 39814.48824194\n",
      "Iteration 155, loss = 39316.65717410\n",
      "Iteration 156, loss = 38825.75124592\n",
      "Iteration 157, loss = 38341.49342038\n",
      "Iteration 158, loss = 37864.45462209\n",
      "Iteration 159, loss = 37392.93287360\n",
      "Iteration 160, loss = 36928.48727800\n",
      "Iteration 161, loss = 36470.07224006\n",
      "Iteration 162, loss = 36017.74179986\n",
      "Iteration 163, loss = 35571.68806426\n",
      "Iteration 164, loss = 35131.57755590\n",
      "Iteration 165, loss = 34698.15320465\n",
      "Iteration 166, loss = 34269.21420304\n",
      "Iteration 167, loss = 33847.23532114\n",
      "Iteration 168, loss = 33430.82222065\n",
      "Iteration 169, loss = 33020.31966768\n",
      "Iteration 170, loss = 32615.69919538\n",
      "Iteration 171, loss = 32216.13032466\n",
      "Iteration 172, loss = 31820.99890805\n",
      "Iteration 173, loss = 31432.81512970\n",
      "Iteration 174, loss = 31049.28310920\n",
      "Iteration 175, loss = 30670.53822295\n",
      "Iteration 176, loss = 30297.58340852\n",
      "Iteration 177, loss = 29929.32610659\n",
      "Iteration 178, loss = 29566.19902246\n",
      "Iteration 179, loss = 29207.91481620\n",
      "Iteration 180, loss = 28854.27729271\n",
      "Iteration 181, loss = 28506.48718603\n",
      "Iteration 182, loss = 28162.49945665\n",
      "Iteration 183, loss = 27823.72506756\n",
      "Iteration 184, loss = 27489.29067885\n",
      "Iteration 185, loss = 27159.43969794\n",
      "Iteration 186, loss = 26833.97861577\n",
      "Iteration 187, loss = 26512.79566467\n",
      "Iteration 188, loss = 26196.50514228\n",
      "Iteration 189, loss = 25884.05306781\n",
      "Iteration 190, loss = 25576.19397685\n",
      "Iteration 191, loss = 25271.88476975\n",
      "Iteration 192, loss = 24972.52976380\n",
      "Iteration 193, loss = 24677.03097719\n",
      "Iteration 194, loss = 24385.24698369\n",
      "Iteration 195, loss = 24097.98082926\n",
      "Iteration 196, loss = 23813.93895646\n",
      "Iteration 197, loss = 23534.37846183\n",
      "Iteration 198, loss = 23258.23982694\n",
      "Iteration 199, loss = 22985.93666973\n",
      "Iteration 200, loss = 22717.64224097\n",
      "Iteration 1, loss = 354316.59520397\n",
      "Iteration 2, loss = 366092.99342377\n",
      "Iteration 3, loss = 361155.05164670\n",
      "Iteration 4, loss = 356284.08556005\n",
      "Iteration 5, loss = 351479.34802648\n",
      "Iteration 6, loss = 346740.85619657\n",
      "Iteration 7, loss = 342066.03474377\n",
      "Iteration 8, loss = 337455.18769524\n",
      "Iteration 9, loss = 332907.00634308\n",
      "Iteration 10, loss = 328420.01195710\n",
      "Iteration 11, loss = 323994.90704669\n",
      "Iteration 12, loss = 319630.15509844\n",
      "Iteration 13, loss = 315324.52365326\n",
      "Iteration 14, loss = 311077.17204989\n",
      "Iteration 15, loss = 306888.57603554\n",
      "Iteration 16, loss = 302755.45535373\n",
      "Iteration 17, loss = 298679.72789437\n",
      "Iteration 18, loss = 294658.90137545\n",
      "Iteration 19, loss = 290693.41504543\n",
      "Iteration 20, loss = 286781.62190683\n",
      "Iteration 21, loss = 282922.96677903\n",
      "Iteration 22, loss = 279117.19400102\n",
      "Iteration 23, loss = 275362.51422444\n",
      "Iteration 24, loss = 271659.69511711\n",
      "Iteration 25, loss = 268006.91124218\n",
      "Iteration 26, loss = 264403.82905010\n",
      "Iteration 27, loss = 260849.03854502\n",
      "Iteration 28, loss = 257343.33884506\n",
      "Iteration 29, loss = 253886.13422466\n",
      "Iteration 30, loss = 250474.86144650\n",
      "Iteration 31, loss = 247110.84291741\n",
      "Iteration 32, loss = 243791.99707182\n",
      "Iteration 33, loss = 240517.86378900\n",
      "Iteration 34, loss = 237289.26359792\n",
      "Iteration 35, loss = 234104.47683682\n",
      "Iteration 36, loss = 230962.62396313\n",
      "Iteration 37, loss = 227863.56283033\n",
      "Iteration 38, loss = 224806.80084325\n",
      "Iteration 39, loss = 221792.04169465\n",
      "Iteration 40, loss = 218817.63660338\n",
      "Iteration 41, loss = 215883.73262722\n",
      "Iteration 42, loss = 212990.27481652\n",
      "Iteration 43, loss = 210136.32302724\n",
      "Iteration 44, loss = 207319.91659756\n",
      "Iteration 45, loss = 204543.82954151\n",
      "Iteration 46, loss = 201803.90707055\n",
      "Iteration 47, loss = 199101.86931104\n",
      "Iteration 48, loss = 196436.46987942\n",
      "Iteration 49, loss = 193807.41397689\n",
      "Iteration 50, loss = 191214.36825442\n",
      "Iteration 51, loss = 188655.81956174\n",
      "Iteration 52, loss = 186133.44501110\n",
      "Iteration 53, loss = 183644.26596949\n",
      "Iteration 54, loss = 181189.12049605\n",
      "Iteration 55, loss = 178767.96533901\n",
      "Iteration 56, loss = 176378.91493068\n",
      "Iteration 57, loss = 174022.33984234\n",
      "Iteration 58, loss = 171699.22547656\n",
      "Iteration 59, loss = 169406.17857834\n",
      "Iteration 60, loss = 167145.60115978\n",
      "Iteration 61, loss = 164914.72976035\n",
      "Iteration 62, loss = 162715.10188379\n",
      "Iteration 63, loss = 160545.14685705\n",
      "Iteration 64, loss = 158404.24732876\n",
      "Iteration 65, loss = 156292.79782921\n",
      "Iteration 66, loss = 154209.99324591\n",
      "Iteration 67, loss = 152155.67971840\n",
      "Iteration 68, loss = 150129.39841924\n",
      "Iteration 69, loss = 148130.59299555\n",
      "Iteration 70, loss = 146159.06922665\n",
      "Iteration 71, loss = 144213.88011418\n",
      "Iteration 72, loss = 142295.55612906\n",
      "Iteration 73, loss = 140403.16867377\n",
      "Iteration 74, loss = 138536.86927907\n",
      "Iteration 75, loss = 136696.09254219\n",
      "Iteration 76, loss = 134879.84135627\n",
      "Iteration 77, loss = 133088.57311276\n",
      "Iteration 78, loss = 131321.68574741\n",
      "Iteration 79, loss = 129578.69021776\n",
      "Iteration 80, loss = 127859.65277952\n",
      "Iteration 81, loss = 126163.89123440\n",
      "Iteration 82, loss = 124491.23999876\n",
      "Iteration 83, loss = 122841.41425308\n",
      "Iteration 84, loss = 121214.53024211\n",
      "Iteration 85, loss = 119608.78693862\n",
      "Iteration 86, loss = 118025.21788713\n",
      "Iteration 87, loss = 116463.51933775\n",
      "Iteration 88, loss = 114922.51681279\n",
      "Iteration 89, loss = 113402.59539167\n",
      "Iteration 90, loss = 111904.05164837\n",
      "Iteration 91, loss = 110425.37944490\n",
      "Iteration 92, loss = 108967.06692785\n",
      "Iteration 93, loss = 107528.48265388\n",
      "Iteration 94, loss = 106109.03624444\n",
      "Iteration 95, loss = 104709.31843094\n",
      "Iteration 96, loss = 103328.67353561\n",
      "Iteration 97, loss = 101966.56417720\n",
      "Iteration 98, loss = 100623.24099605\n",
      "Iteration 99, loss = 99298.69948504\n",
      "Iteration 100, loss = 97991.13974764\n",
      "Iteration 101, loss = 96702.36159687\n",
      "Iteration 102, loss = 95430.47827861\n",
      "Iteration 103, loss = 94176.59666115\n",
      "Iteration 104, loss = 92939.07319527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 105, loss = 91718.13900199\n",
      "Iteration 106, loss = 90514.14497022\n",
      "Iteration 107, loss = 89326.66131509\n",
      "Iteration 108, loss = 88155.51765443\n",
      "Iteration 109, loss = 87000.09351454\n",
      "Iteration 110, loss = 85860.76432822\n",
      "Iteration 111, loss = 84736.45424679\n",
      "Iteration 112, loss = 83627.65005710\n",
      "Iteration 113, loss = 82533.65986405\n",
      "Iteration 114, loss = 81454.77551708\n",
      "Iteration 115, loss = 80390.81805598\n",
      "Iteration 116, loss = 79340.76703873\n",
      "Iteration 117, loss = 78305.36011274\n",
      "Iteration 118, loss = 77284.00673094\n",
      "Iteration 119, loss = 76276.66842222\n",
      "Iteration 120, loss = 75282.78299179\n",
      "Iteration 121, loss = 74302.59146826\n",
      "Iteration 122, loss = 73335.85342971\n",
      "Iteration 123, loss = 72382.18673445\n",
      "Iteration 124, loss = 71441.34514650\n",
      "Iteration 125, loss = 70513.32204881\n",
      "Iteration 126, loss = 69598.04152920\n",
      "Iteration 127, loss = 68695.38883291\n",
      "Iteration 128, loss = 67804.75777171\n",
      "Iteration 129, loss = 66926.77617917\n",
      "Iteration 130, loss = 66059.73044994\n",
      "Iteration 131, loss = 65205.17160236\n",
      "Iteration 132, loss = 64362.39863292\n",
      "Iteration 133, loss = 63530.50660518\n",
      "Iteration 134, loss = 62710.23655970\n",
      "Iteration 135, loss = 61901.60516609\n",
      "Iteration 136, loss = 61103.82555075\n",
      "Iteration 137, loss = 60316.05691073\n",
      "Iteration 138, loss = 59539.40542438\n",
      "Iteration 139, loss = 58773.78268648\n",
      "Iteration 140, loss = 58017.86566053\n",
      "Iteration 141, loss = 57273.25671102\n",
      "Iteration 142, loss = 56537.58542670\n",
      "Iteration 143, loss = 55812.55644494\n",
      "Iteration 144, loss = 55096.92594422\n",
      "Iteration 145, loss = 54391.97384141\n",
      "Iteration 146, loss = 53695.78136494\n",
      "Iteration 147, loss = 53009.67992973\n",
      "Iteration 148, loss = 52332.31858908\n",
      "Iteration 149, loss = 51664.81950110\n",
      "Iteration 150, loss = 51005.59601985\n",
      "Iteration 151, loss = 50355.81603695\n",
      "Iteration 152, loss = 49714.77383330\n",
      "Iteration 153, loss = 49082.66874591\n",
      "Iteration 154, loss = 48459.04929610\n",
      "Iteration 155, loss = 47843.44611017\n",
      "Iteration 156, loss = 47237.62474087\n",
      "Iteration 157, loss = 46638.85923288\n",
      "Iteration 158, loss = 46048.34712457\n",
      "Iteration 159, loss = 45465.49546045\n",
      "Iteration 160, loss = 44891.64512781\n",
      "Iteration 161, loss = 44325.00801167\n",
      "Iteration 162, loss = 43766.43233553\n",
      "Iteration 163, loss = 43214.58472197\n",
      "Iteration 164, loss = 42671.42903142\n",
      "Iteration 165, loss = 42134.57687821\n",
      "Iteration 166, loss = 41605.87550535\n",
      "Iteration 167, loss = 41083.71648524\n",
      "Iteration 168, loss = 40568.96815232\n",
      "Iteration 169, loss = 40061.08835383\n",
      "Iteration 170, loss = 39560.13056596\n",
      "Iteration 171, loss = 39066.50647799\n",
      "Iteration 172, loss = 38579.76613159\n",
      "Iteration 173, loss = 38098.37925589\n",
      "Iteration 174, loss = 37624.24402069\n",
      "Iteration 175, loss = 37156.73439730\n",
      "Iteration 176, loss = 36695.43101838\n",
      "Iteration 177, loss = 36239.92483202\n",
      "Iteration 178, loss = 35791.65370704\n",
      "Iteration 179, loss = 35348.16077127\n",
      "Iteration 180, loss = 34911.73611949\n",
      "Iteration 181, loss = 34481.04966959\n",
      "Iteration 182, loss = 34056.09123101\n",
      "Iteration 183, loss = 33637.27823522\n",
      "Iteration 184, loss = 33223.57208487\n",
      "Iteration 185, loss = 32815.52170502\n",
      "Iteration 186, loss = 32414.20686888\n",
      "Iteration 187, loss = 32016.87689208\n",
      "Iteration 188, loss = 31625.34650835\n",
      "Iteration 189, loss = 31239.45442632\n",
      "Iteration 190, loss = 30858.59665872\n",
      "Iteration 191, loss = 30483.09854551\n",
      "Iteration 192, loss = 30112.62124206\n",
      "Iteration 193, loss = 29746.97497201\n",
      "Iteration 194, loss = 29387.14768410\n",
      "Iteration 195, loss = 29031.06544834\n",
      "Iteration 196, loss = 28680.00462050\n",
      "Iteration 197, loss = 28334.31610435\n",
      "Iteration 198, loss = 27992.78202761\n",
      "Iteration 199, loss = 27656.08732765\n",
      "Iteration 200, loss = 27325.17015809\n",
      "Iteration 1, loss = 4586.25932254\n",
      "Iteration 2, loss = 1121.60079970\n",
      "Iteration 3, loss = 603.88529636\n",
      "Iteration 4, loss = 418.14524495\n",
      "Iteration 5, loss = 323.84239207\n",
      "Iteration 6, loss = 304.74844256\n",
      "Iteration 7, loss = 299.64475299\n",
      "Iteration 8, loss = 297.82362353\n",
      "Iteration 9, loss = 297.56477181\n",
      "Iteration 10, loss = 297.08227949\n",
      "Iteration 11, loss = 297.15678851\n",
      "Iteration 12, loss = 296.76900531\n",
      "Iteration 13, loss = 296.46451378\n",
      "Iteration 14, loss = 297.18140129\n",
      "Iteration 15, loss = 296.82781720\n",
      "Iteration 16, loss = 296.54512714\n",
      "Iteration 17, loss = 296.57993236\n",
      "Iteration 18, loss = 297.33744147\n",
      "Iteration 19, loss = 296.75532955\n",
      "Iteration 20, loss = 296.82044506\n",
      "Iteration 21, loss = 296.71735361\n",
      "Iteration 22, loss = 297.24791386\n",
      "Iteration 23, loss = 296.53712268\n",
      "Iteration 24, loss = 296.42272361\n",
      "Iteration 25, loss = 296.90915357\n",
      "Iteration 26, loss = 296.69470049\n",
      "Iteration 27, loss = 296.88657736\n",
      "Iteration 28, loss = 297.03277948\n",
      "Iteration 29, loss = 296.15110293\n",
      "Iteration 30, loss = 295.87388617\n",
      "Iteration 31, loss = 296.02330260\n",
      "Iteration 32, loss = 296.23161480\n",
      "Iteration 33, loss = 296.64701875\n",
      "Iteration 34, loss = 296.35484104\n",
      "Iteration 35, loss = 296.74571564\n",
      "Iteration 36, loss = 296.10608353\n",
      "Iteration 37, loss = 296.21484590\n",
      "Iteration 38, loss = 295.74406608\n",
      "Iteration 39, loss = 295.83534101\n",
      "Iteration 40, loss = 296.07630797\n",
      "Iteration 41, loss = 296.31031021\n",
      "Iteration 42, loss = 296.24914875\n",
      "Iteration 43, loss = 295.83089908\n",
      "Iteration 44, loss = 296.53377074\n",
      "Iteration 45, loss = 296.81737426\n",
      "Iteration 46, loss = 296.40888167\n",
      "Iteration 47, loss = 297.09565489\n",
      "Iteration 48, loss = 296.70681789\n",
      "Iteration 49, loss = 296.49015361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4519.77159262\n",
      "Iteration 2, loss = 965.33036691\n",
      "Iteration 3, loss = 637.63979866\n",
      "Iteration 4, loss = 452.05607140\n",
      "Iteration 5, loss = 359.51347608\n",
      "Iteration 6, loss = 326.50479557\n",
      "Iteration 7, loss = 312.71873112\n",
      "Iteration 8, loss = 303.18107919\n",
      "Iteration 9, loss = 301.63566941\n",
      "Iteration 10, loss = 300.66783424\n",
      "Iteration 11, loss = 301.35755981\n",
      "Iteration 12, loss = 300.84337103\n",
      "Iteration 13, loss = 301.75451149\n",
      "Iteration 14, loss = 300.22592305\n",
      "Iteration 15, loss = 300.39002774\n",
      "Iteration 16, loss = 300.87897530\n",
      "Iteration 17, loss = 300.35192227\n",
      "Iteration 18, loss = 300.17701846\n",
      "Iteration 19, loss = 300.08566908\n",
      "Iteration 20, loss = 300.22480388\n",
      "Iteration 21, loss = 300.05916257\n",
      "Iteration 22, loss = 300.34737357\n",
      "Iteration 23, loss = 299.82528237\n",
      "Iteration 24, loss = 299.30192619\n",
      "Iteration 25, loss = 299.92596985\n",
      "Iteration 26, loss = 299.53174382\n",
      "Iteration 27, loss = 300.43157198\n",
      "Iteration 28, loss = 298.98289235\n",
      "Iteration 29, loss = 299.42248829\n",
      "Iteration 30, loss = 298.56607763\n",
      "Iteration 31, loss = 298.92892669\n",
      "Iteration 32, loss = 298.96128871\n",
      "Iteration 33, loss = 298.69862846\n",
      "Iteration 34, loss = 297.95424042\n",
      "Iteration 35, loss = 299.76257676\n",
      "Iteration 36, loss = 298.28227892\n",
      "Iteration 37, loss = 298.33590146\n",
      "Iteration 38, loss = 298.73501337\n",
      "Iteration 39, loss = 298.58407932\n",
      "Iteration 40, loss = 298.66398044\n",
      "Iteration 41, loss = 298.67397458\n",
      "Iteration 42, loss = 298.10321776\n",
      "Iteration 43, loss = 298.36242318\n",
      "Iteration 44, loss = 299.11825758\n",
      "Iteration 45, loss = 298.45538503\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4498.37615455\n",
      "Iteration 2, loss = 1057.04023212\n",
      "Iteration 3, loss = 591.08352013\n",
      "Iteration 4, loss = 410.69379676\n",
      "Iteration 5, loss = 346.31029221\n",
      "Iteration 6, loss = 319.13541805\n",
      "Iteration 7, loss = 307.61530113\n",
      "Iteration 8, loss = 304.37142397\n",
      "Iteration 9, loss = 303.09600232\n",
      "Iteration 10, loss = 302.73413170\n",
      "Iteration 11, loss = 302.93793902\n",
      "Iteration 12, loss = 303.26840492\n",
      "Iteration 13, loss = 303.52088337\n",
      "Iteration 14, loss = 303.18534537\n",
      "Iteration 15, loss = 303.14979466\n",
      "Iteration 16, loss = 302.36392884\n",
      "Iteration 17, loss = 303.21872734\n",
      "Iteration 18, loss = 302.67568346\n",
      "Iteration 19, loss = 302.81365751\n",
      "Iteration 20, loss = 302.71085588\n",
      "Iteration 21, loss = 302.12393239\n",
      "Iteration 22, loss = 302.09248914\n",
      "Iteration 23, loss = 302.20447528\n",
      "Iteration 24, loss = 302.03721046\n",
      "Iteration 25, loss = 302.17945833\n",
      "Iteration 26, loss = 302.67740411\n",
      "Iteration 27, loss = 303.35861406\n",
      "Iteration 28, loss = 302.00048794\n",
      "Iteration 29, loss = 302.45663058\n",
      "Iteration 30, loss = 302.55679587\n",
      "Iteration 31, loss = 302.96514863\n",
      "Iteration 32, loss = 301.97866328\n",
      "Iteration 33, loss = 302.92559363\n",
      "Iteration 34, loss = 301.79019335\n",
      "Iteration 35, loss = 302.79498995\n",
      "Iteration 36, loss = 302.55973415\n",
      "Iteration 37, loss = 301.84253657\n",
      "Iteration 38, loss = 301.89682747\n",
      "Iteration 39, loss = 302.13631769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 302.04769859\n",
      "Iteration 41, loss = 302.80388856\n",
      "Iteration 42, loss = 302.33522485\n",
      "Iteration 43, loss = 303.06910943\n",
      "Iteration 44, loss = 302.87729951\n",
      "Iteration 45, loss = 301.81552182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4530.82049711\n",
      "Iteration 2, loss = 1064.70545809\n",
      "Iteration 3, loss = 592.47795663\n",
      "Iteration 4, loss = 415.76710808\n",
      "Iteration 5, loss = 351.22222814\n",
      "Iteration 6, loss = 322.40868913\n",
      "Iteration 7, loss = 310.24648114\n",
      "Iteration 8, loss = 306.84491015\n",
      "Iteration 9, loss = 305.94304759\n",
      "Iteration 10, loss = 305.88035532\n",
      "Iteration 11, loss = 305.59478971\n",
      "Iteration 12, loss = 305.79594178\n",
      "Iteration 13, loss = 304.98299228\n",
      "Iteration 14, loss = 305.99639796\n",
      "Iteration 15, loss = 305.70333650\n",
      "Iteration 16, loss = 305.47821817\n",
      "Iteration 17, loss = 305.59495995\n",
      "Iteration 18, loss = 305.06112910\n",
      "Iteration 19, loss = 304.79608132\n",
      "Iteration 20, loss = 305.34741766\n",
      "Iteration 21, loss = 304.87609749\n",
      "Iteration 22, loss = 305.05663154\n",
      "Iteration 23, loss = 304.47933085\n",
      "Iteration 24, loss = 304.72890590\n",
      "Iteration 25, loss = 304.51077275\n",
      "Iteration 26, loss = 306.28986117\n",
      "Iteration 27, loss = 305.27383461\n",
      "Iteration 28, loss = 305.25368728\n",
      "Iteration 29, loss = 304.95392800\n",
      "Iteration 30, loss = 304.62388824\n",
      "Iteration 31, loss = 305.70128830\n",
      "Iteration 32, loss = 304.77883255\n",
      "Iteration 33, loss = 305.75754406\n",
      "Iteration 34, loss = 304.56249568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4562.17741701\n",
      "Iteration 2, loss = 1077.16694867\n",
      "Iteration 3, loss = 596.39327330\n",
      "Iteration 4, loss = 417.89609576\n",
      "Iteration 5, loss = 355.83623721\n",
      "Iteration 6, loss = 324.62170654\n",
      "Iteration 7, loss = 310.55507372\n",
      "Iteration 8, loss = 305.57900273\n",
      "Iteration 9, loss = 303.64078622\n",
      "Iteration 10, loss = 304.78578214\n",
      "Iteration 11, loss = 303.17295117\n",
      "Iteration 12, loss = 302.98765656\n",
      "Iteration 13, loss = 303.93917243\n",
      "Iteration 14, loss = 303.75412332\n",
      "Iteration 15, loss = 303.63663528\n",
      "Iteration 16, loss = 304.09433160\n",
      "Iteration 17, loss = 303.80421104\n",
      "Iteration 18, loss = 303.47302511\n",
      "Iteration 19, loss = 304.22351134\n",
      "Iteration 20, loss = 303.16168116\n",
      "Iteration 21, loss = 302.97663566\n",
      "Iteration 22, loss = 303.07453944\n",
      "Iteration 23, loss = 305.44815632\n",
      "Iteration 24, loss = 302.93846650\n",
      "Iteration 25, loss = 303.19288075\n",
      "Iteration 26, loss = 304.22094525\n",
      "Iteration 27, loss = 303.09395310\n",
      "Iteration 28, loss = 303.39271406\n",
      "Iteration 29, loss = 302.66226066\n",
      "Iteration 30, loss = 303.28328145\n",
      "Iteration 31, loss = 303.86386346\n",
      "Iteration 32, loss = 303.25122401\n",
      "Iteration 33, loss = 303.02744848\n",
      "Iteration 34, loss = 302.62259631\n",
      "Iteration 35, loss = 303.05479462\n",
      "Iteration 36, loss = 303.73192849\n",
      "Iteration 37, loss = 303.31242321\n",
      "Iteration 38, loss = 303.16855611\n",
      "Iteration 39, loss = 302.68480630\n",
      "Iteration 40, loss = 303.28588108\n",
      "Iteration 41, loss = 303.88196955\n",
      "Iteration 42, loss = 302.82254206\n",
      "Iteration 43, loss = 303.56819077\n",
      "Iteration 44, loss = 303.08521695\n",
      "Iteration 45, loss = 302.63218393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4006.59637029\n",
      "Iteration 2, loss = 860.43656507\n",
      "Iteration 3, loss = 486.04551596\n",
      "Iteration 4, loss = 339.92391739\n",
      "Iteration 5, loss = 305.36413055\n",
      "Iteration 6, loss = 299.42789727\n",
      "Iteration 7, loss = 298.20516643\n",
      "Iteration 8, loss = 297.96941227\n",
      "Iteration 9, loss = 297.91337974\n",
      "Iteration 10, loss = 297.59551007\n",
      "Iteration 11, loss = 297.37304829\n",
      "Iteration 12, loss = 297.38068119\n",
      "Iteration 13, loss = 298.46472509\n",
      "Iteration 14, loss = 297.10705680\n",
      "Iteration 15, loss = 297.23089755\n",
      "Iteration 16, loss = 298.16711855\n",
      "Iteration 17, loss = 297.57874967\n",
      "Iteration 18, loss = 296.69131892\n",
      "Iteration 19, loss = 297.51983176\n",
      "Iteration 20, loss = 297.75159210\n",
      "Iteration 21, loss = 297.10198697\n",
      "Iteration 22, loss = 297.34201210\n",
      "Iteration 23, loss = 297.10608501\n",
      "Iteration 24, loss = 297.51420854\n",
      "Iteration 25, loss = 297.40357574\n",
      "Iteration 26, loss = 297.07737925\n",
      "Iteration 27, loss = 297.39162700\n",
      "Iteration 28, loss = 297.38863032\n",
      "Iteration 29, loss = 296.95290437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "{'alpha': 0.0001, 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "param_grid = {\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}  \n",
    "grid = GridSearchCV(MLPRegressor(hidden_layer_sizes=(6,5),\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    learning_rate_init=0.01), param_grid, cv = 5) \n",
    "grid.fit(df_training_scaled, tf_training_data)\n",
    "print(grid.best_params_) \n",
    "\n",
    "#more alpha value. experiment with hidden layers.\n",
    "#plot the iteration graphs as well would be good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NEh_vR3gkyZG",
   "metadata": {
    "id": "NEh_vR3gkyZG"
   },
   "source": [
    "What is the mean absolute error achieved on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3660d9c1",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for test data:19.100473\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "df_testing_data_pred=grid.predict(df_testing_scaled)\n",
    "MAE=mean_absolute_error(tf_testing_data,df_testing_data_pred)\n",
    "print(\"MAE for test data:%f\"%(MAE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PUwlWri4kzze",
   "metadata": {
    "id": "PUwlWri4kzze"
   },
   "source": [
    "Show (draw!) the price elasticity for your apartment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42d222",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c469a-9ef3-45b5-a225-ff4109086d16",
   "metadata": {
    "id": "7e9c469a-9ef3-45b5-a225-ff4109086d16"
   },
   "source": [
    "### Profit discrepencies when incorporating price elasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OmfvMMkWk-Dt",
   "metadata": {
    "id": "OmfvMMkWk-Dt"
   },
   "source": [
    "Use the model that best captures price elasticity on the testing set. You can assume that this model accurately captures price elasticity, i.e., is the ground truth. What is the better of the two proposed prices? Is the better price the optimal price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b855ae",
   "metadata": {
    "id": "ly3THPi7PdLJ"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878657a",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827d79d-7f99-4e6c-af6c-9ba7ff7509bc",
   "metadata": {
    "id": "8827d79d-7f99-4e6c-af6c-9ba7ff7509bc"
   },
   "source": [
    "# Question 4: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a971e0-7b6b-414f-a845-5379cd705838",
   "metadata": {
    "id": "38a971e0-7b6b-414f-a845-5379cd705838"
   },
   "source": [
    "Summarize your findings and discuss limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1c314",
   "metadata": {},
   "source": [
    "_Answer:_ \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
